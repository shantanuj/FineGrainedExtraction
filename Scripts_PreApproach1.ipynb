{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The purpose of this notebook\n",
    "1. Form dataset for training (provided training script) \n",
    "1. Auxiliary sets for problem: Frequency counts, dependency parsing, etc\n",
    "2. Some definitions of problem\n",
    "3. Simple baseline BIlstm + CRF for problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### IGNORE/unorganized section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-13-cf17692c7aec>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-cf17692c7aec>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def find_approximate_occurance_based_on_structure(): #computes similarity between two sentences based on word structure and word contribution\n",
    "    '''Example:\n",
    "    s1: It is very overpriced and not very tasty\n",
    "    s2: This lacks the features and is very expensive.\n",
    "    s3: This lacks the features and is very expensive for a model A laptop.\n",
    "    s4: It is very overpriced and not very tasty for a Michelin star restaraunt. \n",
    "    overpriced= expensive\n",
    "    tasty = feaatures\n",
    "    very = very\n",
    "    not = lack\n",
    "    Michelin star = model A \n",
    "    \n",
    "    Domain dependent words--> get_domain_dependent_words()\n",
    "    Domain independent words --> get_independent_words()\n",
    "    \n",
    "    'very expensive' is together/combined word \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_approximate_occurance_based_on_words():\n",
    "    '''Example:\n",
    "    \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_inter_domain_similarity():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Approach:\n",
    "\n",
    "1. Baseline simple method: BiLSTM + CRF.  This is a baseline model. We need a model with high coverage (high false positives) and then trim the selection using the below model. This can either be simple rules, syntactic structures, just any model with high number of false positives. \n",
    "\n",
    "\n",
    "2. Given a sentence S, train an encoder-decoder network to reproduce the sentence. Compute last hidden state representation based on model M (S-> S). After training for a while, for each sentence: for each aspect word: remove BA, IA labelled words, and get representation. 2nd model takes this representation and the original sentence + some noisy cases and outputs a 1 or 0 depending on task. Effectively, 2nd model is learning to use incomplete representation and other info to learn whether it is an aspect/opinion term. \n",
    "\n",
    "\n",
    "To find out word contribution, by training a model based on missing words. So, if a word is removed, get some representation. Embed the ones with aspects missing to a similar space. Meaning if I have a sentence: I love their ice-cream so much. Then remove ice-cream, get a representation of the sentence, and a representation of the normal (with ice-cream sentence). Using these representations, map as similar (1) in a Siamese net. Now take all other (except opinion terms) and use same model to get representation. But the final model outputs the representations as dissimilar.\n",
    "\n",
    "    G(F(I love their so much), F(I love their ice cream so much)) = 1\n",
    "    G(F(love their ice cream so much), F(I love their ice cream so much)) = 0\n",
    "    G(F(I love their ice cream), F(I love their ice cream so much)) = 2\n",
    "    G(F(I love ice cream so much), F(Sentence)) = \n",
    "\n",
    "\n",
    "\n",
    "        Another approach could be if we remove the word then from all the mined words, if we replace the term with it, does the sentence still make sense (is it legible)-> then it is an aspect word. \n",
    "\n",
    "        So given s= I love their ice cream so much, if we remove ice-cream then replace it with say 50 previous aspect words like -> football, passing, nachos, ambience, does it still make sense? A model M is trained to identify whether a sentence still makes sense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### What is an aspect and what is an opinion term?\n",
    "\n",
    "S1: 'Plus it is small and reasonably light so I can take it with me to and from work. -> small, reasonably light\n",
    "\n",
    "S2:'If internet connectivity is important I would recommend going with a dell net book for 50 bucks more, or buy a USB wireless card.' -> recommend, and internet connectivity;USB wireless card\n",
    "\n",
    "S3: \"Rao is a good restaurant, but it's nothing special.\" -> good, nothing special\n",
    "\n",
    "S4: iLife is easily compatible with Microsoft Office so you can send and receive files from a PC.--> easily compatible-->  iLife;Microsoft Office \n",
    "\n",
    "S5: \"people are rude bit again it's new york!\" -> rude; people\n",
    "\n",
    "S6: \"The speed is incredible and I am more than satisfied.\" --> incredible, 'more than satisfied', 'speed'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To do:\\nMake baseline Bi-directional LSTM and top-CRF model with info about POS tags \\n\\nInputs: POS_tags; Dependency relation wrt next word; Word vector (context info); Info about influence of a word \\n\\nQ: To get a better representation what can we do to the architecture itself (rather than feeding in more features)\\nQ: How do we obtain the influence of a word in a sentence (like on a color scale); attention\\nQ: How do we obtain the interdependeny of words -> dependency structure OR POSE OF WORDS\\nQ: Define interdependency with an example\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To do:\n",
    "Make baseline Bi-directional LSTM and top-CRF model with info about POS tags \n",
    "\n",
    "Inputs: POS_tags; Dependency relation wrt next word; Word vector (context info); Info about influence of a word \n",
    "\n",
    "Q: To get a better representation what can we do to the architecture itself (rather than feeding in more features)\n",
    "Q: How do we obtain the influence of a word in a sentence (like on a color scale); attention\n",
    "Q: How do we obtain the interdependeny of words -> dependency structure OR POSE OF WORDS\n",
    "Q: Define interdependency with an example\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_to_int = {}\n",
    "int_to_word = {}\n",
    "word_vectors = [] \n",
    "pos_tagged_sentences = []\n",
    "dependency_structure_sentences = []\n",
    "pos_to_int = {}\n",
    "label_to_int = {}\n",
    "\n",
    "\n",
    "#for each training example\n",
    "#concatenate word vector with pos tag and dependency structure\n",
    "\n",
    "\n",
    "\n",
    "#1) Make word_to_id and id_to_word\n",
    "#2) Tag each sentence as per scheme\n",
    "#3) Use pytorch bilstm crf model initialized with word embeddings as a variable\n",
    "#4) Train model to predict tags and make it batch wise (fine tune it)\n",
    "#5) Explore adaptation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#1) Convert data to format [([training tokens],[corresponding tags])..]\n",
    "start_tag = \"<START>\"\n",
    "end_tag = \"<END>\"\n",
    "tag_to_id = {start_tag:0,end_tag:-1, \"BA\":1, \"IA\":2, \"BO\":3, \"IO\":4, \"OT\":5}\n",
    "id_to_tag = {id_: tag for tag,id_ in tag_to_id.items()}\n",
    "#construct token to id \n",
    "#use a tokenizer that adapts to most domains-> most of them are\n",
    "#for sentence in laptop_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s = \"U.K. is so so awesome, right!\".translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting dataset info (do not rerun-already stored as pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''SCript to store dataset info'''\n",
    "#https://explosion.ai/blog/sense2vec-with-spacy\n",
    "#Does punctuation and case matter in this case? --> currently we do punctuation removal\n",
    "import csv \n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import spacy\n",
    "import string\n",
    "\n",
    "nlp = spacy.load('en') #load spacy model\n",
    "\n",
    "start_tag = \"<START>\"\n",
    "end_tag = \"<END>\"\n",
    "tag_to_id = {start_tag:0,end_tag:-1, \"BA\":1, \"IA\":2, \"BO\":3, \"IO\":4, \"OT\":5}\n",
    "id_to_tag = {id_: tag for tag,id_ in tag_to_id.items()}\n",
    "\n",
    "\n",
    "def obtain_tokens(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "\n",
    "\n",
    "'''The below functions that use Spacy can be made faster using Spacy specific syntax- example using doc, checking for verbs, etc. However, here it is converted to Python string format to make the primary function compatible with other NLP packages'''\n",
    "def get_tokenized_list_for_delim(phrases, tokenizer_func = obtain_tokens, delim =\";\"):\n",
    "    '''\n",
    "    Input: A sequence of phrases delimited as provided by delimiter: Ex A B; C; D E F\n",
    "    Output: A list of tokenized phrases: [[A, B], [C], [D, E, F]]\n",
    "    '''\n",
    "    #split_phrases = phrases.split(delim)\n",
    "    \n",
    "    split_phrases = phrases.split(delim)\n",
    "    tokenized_split_phrases = map(lambda x: tokenizer_func(x), split_phrases)\n",
    "    return tokenized_split_phrases\n",
    "\n",
    "\n",
    "def get_pos_tags(sentence):\n",
    "    #Using spacy\n",
    "    text = nlp(unicode(sentence))\n",
    "    pos_tagged_text = [(str(token), str(token.pos_), str(token.tag_), str(token.dep_)) for token in text]\n",
    "    return pos_tagged_text\n",
    "\n",
    "def get_dependency_tree(sentence):\n",
    "    #Using spacy again\n",
    "    None\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def get_dataset_info(df, save_to_file = False, save_dir ='laptop', include_sense = False, tokenizer_func=obtain_tokens, get_tags = True, remove_punctuation = False, replace_punctuation= True, to_lower = True, get_dependency_structure = False):\n",
    "    training_data = []\n",
    "    training_data_with_other_stuff = []\n",
    "    out_row = []\n",
    "    token_to_freq = {} \n",
    "    #token_sense_to_id = {}\n",
    "    \n",
    "    token_to_id = {}\n",
    "    loop_i = 0 \n",
    "    for sentence, aspect_words, opinion_words in zip(df.Sentence, df.Aspects, df.Opinions):\n",
    "        \n",
    "        \n",
    "        if(remove_punctuation):\n",
    "            sentence = sentence.translate(None, string.punctuation)\n",
    "        \n",
    "        \n",
    "        if(to_lower):\n",
    "            sentence = sentence.lower()\n",
    "            \n",
    "        \n",
    "        tokenized_sentence = tokenizer_func(sentence)\n",
    "        \n",
    "        if(get_dependency_structure):\n",
    "            #Obtain dependency tree\n",
    "            dependency_tree = get_dependency_tree(sentence)\n",
    "        #if(get_parser_tree):\n",
    "            #Obtain parser tree\n",
    "         #   None\n",
    "        \n",
    "        if(get_tags or include_sense):\n",
    "            tagged_text = get_pos_tags(sentence) #NOTE: We do not give the tokenized sentence, only the sentence\n",
    "        \n",
    "        '''Before we replace punctuation with specific tags we need to perform all functions(POS, etc)'''\n",
    "        if(replace_punctuation):\n",
    "            \n",
    "        \n",
    "        \n",
    "        aspect_list = get_tokenized_list_for_delim(aspect_words, tokenizer_func, ';')\n",
    "        opinion_list = get_tokenized_list_for_delim(opinion_words, tokenizer_func, ';')\n",
    "        #print(aspect_list)\n",
    "        seq_absa_tagged = [] \n",
    "        aspect_ptr = 0\n",
    "        opinion_ptr = 0\n",
    "        skip = 0\n",
    "        \n",
    "        #labels = []\n",
    "        for i_loop, token in enumerate(tokenized_sentence):\n",
    "            temp_token = token \n",
    "            if(include_sense):\n",
    "                '''THIS is specific to spacy format'''\n",
    "            \n",
    "                #Only limit to nouns, verbs and adjectives\n",
    "                \n",
    "                if(tagged_text[i_loop][1] in ['NOUN','VERB','ADJ']):\n",
    "                    token+= '|'+ tagged_text[i_loop][1]\n",
    "                    #print(token)\n",
    "                    \n",
    "            if token not in token_to_id:\n",
    "                token_to_id[token] = len(token_to_id)\n",
    "                token_to_freq[token] = 0 \n",
    "            else:\n",
    "                token_to_freq[token] += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "            if(skip>0): #if we encounter incomplete aspect/opinion matches previously\n",
    "                skip -= 1\n",
    "             \n",
    "            else:    \n",
    "                label = [tag_to_id[\"OT\"]] #assume it is OT, store as list in case of multiple aspect/opinion terms\n",
    "        \n",
    "        \n",
    "                token = temp_token\n",
    "                \n",
    "                if(aspect_ptr < len(aspect_list) and token == aspect_list[aspect_ptr][0]): #Incomplete match: battery--> battery life\n",
    "                    label = [tag_to_id[\"BA\"]]\n",
    "                    skip = len(aspect_list[aspect_ptr]) - 1 #words to skip ahead since they have been already covered\n",
    "                    if(skip>0):\n",
    "                        label += [tag_to_id[\"IA\"] for i in aspect_list[aspect_ptr][1:]] \n",
    "                    aspect_ptr += 1 \n",
    "        \n",
    "                elif(opinion_ptr< len(opinion_list) and token == opinion_list[opinion_ptr][0]): \n",
    "                    label = [tag_to_id[\"BO\"]]\n",
    "                    skip = len(opinion_list[opinion_ptr]) - 1\n",
    "                    if(skip>0):\n",
    "                        label += [tag_to_id[\"IO\"] for i in opinion_list[opinion_ptr][1:]]\n",
    "                    opinion_ptr += 1 \n",
    "        \n",
    "            \n",
    "                seq_absa_tagged+=label\n",
    "                \n",
    "        if(get_tags and get_dependency_structure):\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged, tagged_text, dependency_tree))\n",
    "            out_row = [\"Sentence\", \"Sequence\", \"Tags\",\"Dependency Tree\"]\n",
    "        \n",
    "        elif(get_tags):\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged, tagged_text))\n",
    "            out_row = [\"Sentence\", \"Sequence\", \"Tags\"]\n",
    "        \n",
    "        elif(get_dependency_structure):\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged, dependency_tree))\n",
    "            out_row = [\"Sentence\", \"Sequence\", \"Dependency Tree\"]\n",
    "        else:\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged))\n",
    "            out_row = [\"Sentence\", \"Sequence\"]\n",
    "            \n",
    "        training_data.append((tokenized_sentence, seq_absa_tagged))\n",
    "        #Write data to csv and save vocab as pickle\n",
    "        \n",
    "        if(loop_i%300==0):\n",
    "            print(\"At sentence: {}\".format(-loop_i))\n",
    "            print(training_data_with_other_stuff[-loop_i])\n",
    "        loop_i-=1\n",
    "    \n",
    "    \n",
    "    if(save_to_file):\n",
    "        if(include_sense):\n",
    "            save_dir+='_WITH_SENSE_'\n",
    "        with open(\"Final_data/{}_absa_seq_labelled.csv\".format(save_dir),'wb') as fout:\n",
    "            csv_out = csv.writer(fout)\n",
    "            csv_out.writerow([\"Sentence\",\"Sequence\"])\n",
    "            for row in training_data:\n",
    "                csv_out.writerow(row)\n",
    "    \n",
    "        with open(\"Final_data/{}_additional_info_seq_labelled.csv\".format(save_dir),'wb') as fout:\n",
    "            csv_out = csv.writer(fout)\n",
    "            csv_out.writerow(out_row)\n",
    "            for row in training_data_with_other_stuff:\n",
    "                csv_out.writerow(row) \n",
    "            \n",
    "        with open(\"Final_data/{}_normal_training_list.pickle\".format(save_dir),'wb') as pickle_o:\n",
    "            pickle.dump(training_data, pickle_o)\n",
    "        \n",
    "        with open(\"Final_data/{}_additional_training_list.pickle\".format(save_dir),'wb') as pickle_o:\n",
    "            pickle.dump(training_data_with_other_stuff, pickle_o)\n",
    "        \n",
    "        with open(\"Final_data/{}_vocab.pickle\".format(save_dir),'wb') as pickle_o: \n",
    "            pickle.dump(token_to_id, pickle_o)\n",
    "        \n",
    "        with open(\"Final_data/{}_tag2id.pickle\".format(save_dir), 'wb') as pickle_o:\n",
    "            pickle.dump(tag_to_id, pickle_o)\n",
    "    \n",
    "    return training_data_with_other_stuff, token_to_id\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rest_data = pd.read_csv(\"Final_data/Semeval_14_ver1/Combined_restaurant.csv\")\n",
    "laptop_data = pd.read_csv(\"Final_data/Semeval_14_ver1/Combined_laptop.csv\")\n",
    "\n",
    "#laptop_training_data, token_to_id = get_dataset_info(laptop_data, True, 'laptop_lower', obtain_tokens, True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At sentence: 0\n",
      "(['i', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'battery', 'life'], [5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 3, 1, 2], [('i', 'PRON', 'PRP', 'nsubj'), ('charge', 'VERB', 'VBP', 'ROOT'), ('it', 'PRON', 'PRP', 'dobj'), ('at', 'ADP', 'IN', 'prep'), ('night', 'NOUN', 'NN', 'pobj'), ('and', 'CCONJ', 'CC', 'cc'), ('skip', 'VERB', 'VB', 'conj'), ('taking', 'VERB', 'VBG', 'xcomp'), ('the', 'DET', 'DT', 'det'), ('cord', 'NOUN', 'NN', 'dobj'), ('with', 'ADP', 'IN', 'prep'), ('me', 'PRON', 'PRP', 'pobj'), ('because', 'ADP', 'IN', 'prep'), ('of', 'ADP', 'IN', 'pcomp'), ('the', 'DET', 'DT', 'det'), ('good', 'ADJ', 'JJ', 'amod'), ('battery', 'NOUN', 'NN', 'compound'), ('life', 'NOUN', 'NN', 'pobj')])\n",
      "At sentence: 300\n",
      "(['only', 'thing', 'i', 'would', 'want', 'to', 'add', 'to', 'this', 'is', 'an', 'internal', 'bluray', 'readwrite', 'drive'], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 2, 2], [('only', 'ADJ', 'JJ', 'advmod'), ('thing', 'NOUN', 'NN', 'nsubj'), ('i', 'PRON', 'PRP', 'nsubj'), ('would', 'VERB', 'MD', 'aux'), ('want', 'VERB', 'VB', 'relcl'), ('to', 'PART', 'TO', 'aux'), ('add', 'VERB', 'VB', 'xcomp'), ('to', 'ADP', 'IN', 'prep'), ('this', 'DET', 'DT', 'pobj'), ('is', 'VERB', 'VBZ', 'ROOT'), ('an', 'DET', 'DT', 'det'), ('internal', 'ADJ', 'JJ', 'amod'), ('bluray', 'NOUN', 'NN', 'amod'), ('readwrite', 'NOUN', 'NN', 'compound'), ('drive', 'VERB', 'VB', 'attr')])\n",
      "At sentence: 600\n",
      "(['it', 'is', 'loaded', 'with', 'programs', 'that', 'is', 'of', 'no', 'good', 'for', 'the', 'average', 'user', 'that', 'makes', 'it', 'run', 'way', 'to', 'slow'], [5, 5, 5, 5, 1, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 3], [('it', 'PRON', 'PRP', 'nsubjpass'), ('is', 'VERB', 'VBZ', 'auxpass'), ('loaded', 'VERB', 'VBN', 'ROOT'), ('with', 'ADP', 'IN', 'prep'), ('programs', 'NOUN', 'NNS', 'pobj'), ('that', 'ADJ', 'WDT', 'nsubj'), ('is', 'VERB', 'VBZ', 'relcl'), ('of', 'ADP', 'IN', 'prep'), ('no', 'DET', 'DT', 'det'), ('good', 'NOUN', 'NN', 'pobj'), ('for', 'ADP', 'IN', 'prep'), ('the', 'DET', 'DT', 'det'), ('average', 'ADJ', 'JJ', 'amod'), ('user', 'NOUN', 'NN', 'pobj'), ('that', 'ADJ', 'WDT', 'nsubj'), ('makes', 'VERB', 'VBZ', 'relcl'), ('it', 'PRON', 'PRP', 'nsubj'), ('run', 'VERB', 'VB', 'ccomp'), ('way', 'NOUN', 'NN', 'npadvmod'), ('to', 'PART', 'TO', 'aux'), ('slow', 'VERB', 'VB', 'advcl')])\n",
      "At sentence: 900\n",
      "(['images', 'are', 'crisp', 'and', 'clean'], [5, 5, 3, 5, 3], [('images', 'NOUN', 'NNS', 'nsubj'), ('are', 'VERB', 'VBP', 'ROOT'), ('crisp', 'ADJ', 'JJ', 'acomp'), ('and', 'CCONJ', 'CC', 'cc'), ('clean', 'ADJ', 'JJ', 'conj')])\n",
      "At sentence: 1200\n",
      "(['overall', 'i', 'feel', 'this', 'netbook', 'was', 'poor', 'quality', 'had', 'poor', 'performance', 'although', 'it', 'did', 'have', 'great', 'battery', 'life', 'when', 'it', 'did', 'work'], [5, 5, 5, 5, 5, 5, 3, 1, 5, 3, 1, 5, 5, 5, 5, 3, 1, 2, 5, 5, 5, 5], [('overall', 'ADV', 'RB', 'advmod'), ('i', 'PRON', 'PRP', 'nsubj'), ('feel', 'VERB', 'VBP', 'ROOT'), ('this', 'DET', 'DT', 'det'), ('netbook', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ccomp'), ('poor', 'ADJ', 'JJ', 'amod'), ('quality', 'NOUN', 'NN', 'nsubj'), ('had', 'VERB', 'VBD', 'ccomp'), ('poor', 'ADJ', 'JJ', 'amod'), ('performance', 'NOUN', 'NN', 'dobj'), ('although', 'ADP', 'IN', 'mark'), ('it', 'PRON', 'PRP', 'nsubj'), ('did', 'VERB', 'VBD', 'aux'), ('have', 'VERB', 'VB', 'advcl'), ('great', 'ADJ', 'JJ', 'amod'), ('battery', 'NOUN', 'NN', 'compound'), ('life', 'NOUN', 'NN', 'dobj'), ('when', 'ADV', 'WRB', 'advmod'), ('it', 'PRON', 'PRP', 'nsubj'), ('did', 'VERB', 'VBD', 'advcl'), ('work', 'NOUN', 'NN', 'conj')])\n",
      "At sentence: 1500\n",
      "(['in', '5', 'months', 'the', 'connect', 'quality', 'got', 'worse', 'and', 'worse'], [5, 5, 5, 5, 1, 2, 5, 3, 5, 5], [('in', 'ADP', 'IN', 'prep'), ('5', 'NUM', 'CD', 'nummod'), ('months', 'NOUN', 'NNS', 'pobj'), ('the', 'DET', 'DT', 'det'), ('connect', 'NOUN', 'NN', 'compound'), ('quality', 'NOUN', 'NN', 'nsubj'), ('got', 'VERB', 'VBD', 'ROOT'), ('worse', 'ADJ', 'JJR', 'acomp'), ('and', 'CCONJ', 'CC', 'cc'), ('worse', 'ADJ', 'JJR', 'conj')])\n",
      "At sentence: 1800\n",
      "(['i', 'pondered', 'very', 'long', 'over', 'this', 'decision'], [5, 5, 5, 5, 5, 5, 5], [('i', 'PRON', 'PRP', 'nsubj'), ('pondered', 'VERB', 'VBD', 'ROOT'), ('very', 'ADV', 'RB', 'advmod'), ('long', 'ADV', 'RB', 'advmod'), ('over', 'ADP', 'IN', 'prep'), ('this', 'DET', 'DT', 'det'), ('decision', 'NOUN', 'NN', 'pobj')])\n",
      "At sentence: 2100\n",
      "(['the', 'speakers', 'on', 'it', 'are', 'useless', 'too'], [5, 1, 5, 5, 5, 3, 5], [('the', 'DET', 'DT', 'det'), ('speakers', 'NOUN', 'NNS', 'nsubj'), ('on', 'ADP', 'IN', 'prep'), ('it', 'PRON', 'PRP', 'pobj'), ('are', 'VERB', 'VBP', 'ROOT'), ('useless', 'ADJ', 'JJ', 'acomp'), ('too', 'ADV', 'RB', 'advmod')])\n",
      "At sentence: 2400\n",
      "(['but', 'buy', 'this', 'model', 'and', 'just', 'purchase', '4gb', 'of', 'ram', '2x2gb', 'for', '92', 'or', '1x4gb', 'for', '99', 'and', 'save', 'yourself', '100', 'than', 'the', 'other', 'model', 'with', '8gb', 'of', 'ram'], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [('but', 'CCONJ', 'CC', 'cc'), ('buy', 'VERB', 'VB', 'ROOT'), ('this', 'DET', 'DT', 'det'), ('model', 'NOUN', 'NN', 'dobj'), ('and', 'CCONJ', 'CC', 'cc'), ('just', 'ADV', 'RB', 'advmod'), ('purchase', 'VERB', 'VB', 'conj'), ('4', 'NUM', 'CD', 'nummod'), ('gb', 'NOUN', 'NN', 'dobj'), ('of', 'ADP', 'IN', 'prep'), ('ram', 'NOUN', 'NN', 'compound'), ('2x2', 'NUM', 'CD', 'compound'), ('gb', 'ADP', 'IN', 'pobj'), ('for', 'ADP', 'IN', 'prep'), ('92', 'NUM', 'CD', 'nummod'), ('or', 'CCONJ', 'CC', 'cc'), ('1x4', 'NUM', 'CD', 'conj'), ('gb', 'NOUN', 'NN', 'pobj'), ('for', 'ADP', 'IN', 'prep'), ('99', 'NUM', 'CD', 'pobj'), ('and', 'CCONJ', 'CC', 'cc'), ('save', 'VERB', 'VB', 'conj'), ('yourself', 'PRON', 'PRP', 'dobj'), ('100', 'NUM', 'CD', 'dobj'), ('than', 'ADP', 'IN', 'prep'), ('the', 'DET', 'DT', 'det'), ('other', 'ADJ', 'JJ', 'amod'), ('model', 'NOUN', 'NN', 'pobj'), ('with', 'ADP', 'IN', 'prep'), ('8', 'NUM', 'CD', 'nummod'), ('gb', 'NOUN', 'NN', 'pobj'), ('of', 'ADP', 'IN', 'prep'), ('ram', 'NOUN', 'NN', 'pobj')])\n",
      "At sentence: 2700\n",
      "(['i', 'love', 'my', 'macbook', 'pro'], [5, 3, 5, 5, 5], [('i', 'PRON', 'PRP', 'nsubj'), ('love', 'VERB', 'VBP', 'ROOT'), ('my', 'ADJ', 'PRP$', 'poss'), ('macbook', 'NOUN', 'NN', 'dobj'), ('pro', 'ADJ', 'JJ', 'dep')])\n",
      "At sentence: 3000\n",
      "(['i', 'had', 'to', 'adjust', 'my', 'mousepad', 'sensitivity', 'because', 'it', 'is', 'very', 'sensitive'], [5, 5, 5, 5, 5, 1, 2, 5, 5, 5, 5, 3], [('i', 'PRON', 'PRP', 'nsubj'), ('had', 'VERB', 'VBD', 'ROOT'), ('to', 'PART', 'TO', 'aux'), ('adjust', 'VERB', 'VB', 'xcomp'), ('my', 'ADJ', 'PRP$', 'poss'), ('mousepad', 'NOUN', 'NN', 'amod'), ('sensitivity', 'NOUN', 'NN', 'dobj'), ('because', 'ADP', 'IN', 'mark'), ('it', 'PRON', 'PRP', 'nsubj'), ('is', 'VERB', 'VBZ', 'advcl'), ('very', 'ADV', 'RB', 'advmod'), ('sensitive', 'ADJ', 'JJ', 'acomp')])\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'Final_data/laptop/_WITH_SENSE__absa_seq_labelled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a63a0d67608f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlaptop_tr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaptop_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'laptop/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobtain_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-fbe506147287>\u001b[0m in \u001b[0;36mget_dataset_info\u001b[0;34m(df, save_to_file, save_dir, include_sense, tokenizer_func, get_tags, remove_punctuation, to_lower, get_dependency_structure)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_sense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0msave_dir\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m'_WITH_SENSE_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final_data/{}_absa_seq_labelled.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mcsv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mcsv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sentence\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sequence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Final_data/laptop/_WITH_SENSE__absa_seq_labelled.csv'"
     ]
    }
   ],
   "source": [
    "laptop_tr_data, _ = get_dataset_info(laptop_data, True, 'laptop/',True, obtain_tokens, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Frequency count of tags vs category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<START>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8b3d72c6f918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<START>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '<START>'"
     ]
    }
   ],
   "source": [
    "'''Script to get frequencies of POS tags for opinion and aspect words'''\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.data import load\n",
    "#tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "examples_of_aspect_as_ADJ = []\n",
    "examples_of_opinion_as_NOUN = []\n",
    "\n",
    "\n",
    "def get_freq_pos_opinion_and_aspect(df, limit =-1):\n",
    "    '''Input is a dataframe'''\n",
    "    #bopinion_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #baspect_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #iopinion_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #iaspect_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    bopinion_tags = {} \n",
    "    baspect_tags = {}\n",
    "    iopinion_tags = {} \n",
    "    iaspect_tags = {}\n",
    "    '''\n",
    "    if(limit==-1):\n",
    "        limit = len(df)\n",
    "        print_it = False\n",
    "    else:\n",
    "        limit = limit #for debugging\n",
    "        '''\n",
    "    for sentence, opinion, aspect in zip(df.Sentence, df.Opinions, df.Aspects):\n",
    "        '''\n",
    "        if(limit<=0):\n",
    "            break\n",
    "        limit-=1\n",
    "        '''\n",
    "        '''NLTK\n",
    "        #text = word_tokenize(sentence) #<-NLTK\n",
    "        #pos_tagged_text = pos_tag(text) #<-NLTK\n",
    "        '''\n",
    "        text = nlp(unicode(sentence))\n",
    "        pos_tagged_text = [(str(token), str(token.pos_)) for token in text]\n",
    "        \n",
    "        \n",
    "        for seqs in opinion.split(';'): #find all opinion terms-> can be done together with aspect terms but cleaner this way\n",
    "            for i, opinion_term in enumerate(seqs.split()):\n",
    "                for token_pos_tuple in pos_tagged_text:\n",
    "                    token, pos_tag = token_pos_tuple[0], token_pos_tuple[1]\n",
    "                    if(opinion_term == token):\n",
    "                        if(i>1):#IO\n",
    "                            if(pos_tag not in iopinion_tags):\n",
    "                                iopinion_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                iopinion_tags[pos_tag] += 1\n",
    "                        else:#BO\n",
    "                            if(pos_tag not in bopinion_tags):\n",
    "                                bopinion_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                bopinion_tags[pos_tag] += 1\n",
    "                        '''    \n",
    "                        if(limit> 0 and print_it):\n",
    "                            print(sentence, token_pos_tuple)\n",
    "                        '''\n",
    "                        if(pos_tag == \"NOUN\"):\n",
    "                            examples_of_opinion_as_NOUN.append((sentence, token_pos_tuple))\n",
    "                        \n",
    "        for seqs in aspect.split(';'): #find all aspect terms\n",
    "            for i, aspect_term in enumerate(seqs.split()):\n",
    "                for token_pos_tuple in pos_tagged_text:\n",
    "                    token, pos_tag = token_pos_tuple[0], token_pos_tuple[1]\n",
    "                    if(aspect_term == token):\n",
    "                        if(i>1):#IA\n",
    "                            if(pos_tag not in iaspect_tags):\n",
    "                                iaspect_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                iaspect_tags[pos_tag] += 1\n",
    "                        else:#BA\n",
    "                            if(pos_tag not in baspect_tags):\n",
    "                                baspect_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                baspect_tags[pos_tag] += 1\n",
    "                       \n",
    "                        if(pos_tag == 'ADJ'):\n",
    "                            examples_of_aspect_as_ADJ.append((sentence, token_pos_tuple))\n",
    "    return bopinion_tags, iopinion_tags, baspect_tags, iaspect_tags\n",
    "\n",
    "#laptop_df = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_laptop.csv\")\n",
    "#laptop_freq_dicts = get_freq_pos_opinion_and_aspect(laptop_df)\n",
    "\n",
    "#rest_df = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_restaurant.csv\")\n",
    "#rest_freq_dicts = get_freq_pos_opinion_and_aspect(rest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading dataset info and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Load relevant data for training\n",
    "def get_dataset_info(file_name = \"laptop\", training_data_additional = False, get_inverse = False, lower = True):\n",
    "    \n",
    "    if(lower):\n",
    "        opt_string = \"lower\" #hardcoded\n",
    "    else:\n",
    "        opt_string = \"final\"\n",
    "        \n",
    "        \n",
    "    if(training_data_additional):\n",
    "        training_data = pickle.load(open(\"Final_data/{}_{}_additional_training_list.pickle\".format(file_name, opt_string)))\n",
    "    else:\n",
    "        training_data = pickle.load(open(\"Final_data/{}_{}_normal_training_list.pickle\".format(file_name, opt_string)))\n",
    "        \n",
    "    tag2id = pickle.load(open(\"Final_data/{}_{}_tag2id.pickle\".format(file_name, opt_string)))\n",
    "    \n",
    "    vocab2id = pickle.load(open(\"Final_data/{}_{}_vocab.pickle\".format(file_name, opt_string)))\n",
    "    \n",
    "    if(\"<START>\" in vocab2id.keys()): #assuming <START> and <END> always co-occur\n",
    "        start_token = vocab2id[\"<START\"]\n",
    "        end_token = vocab2id[\"<END>\"]\n",
    "    else:\n",
    "        start_token = vocab2id[\"<START>\"] = len(vocab2id)\n",
    "        end_token = vocab2id[\"<END>\"] = len(vocab2id)\n",
    "    \n",
    "    if(not get_inverse):\n",
    "        return training_data, tag2id, vocab2id, start_token, end_token\n",
    "    else:\n",
    "        id2vocab = {id_:token for token, id_ in vocab2id.items()}\n",
    "        id2tag = {id_:tag for tag, id_ in tag2id.items()}\n",
    "        \n",
    "        return training_data, tag2id, vocab2id, start_token, end_token, id2vocab, id2tag\n",
    "\n",
    "#Load pretrained embeddings function \n",
    "def load_embeddings(path_to_embeddings, word2id, embedding_dim = 25):\n",
    "    '''\n",
    "    Input: This only takes in w2v format. So convert all other embedding types/vectors to w2v format: \n",
    "    Output: Torch variable with embeddings only belonging to and indexed by word2id\n",
    "    \n",
    "    For glove:\n",
    "    Use python -m gensim.scripts.glove2word2vec -i <GloVe vector file> -o <Word2vec vector file>\n",
    "    as per https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "    \n",
    "    \n",
    "    W2V format is:\n",
    "    <Num vectors> <dimensionality>\n",
    "    <word1> <vector rep>\n",
    "    <word2> <vector rep>\n",
    "    ....and so on\n",
    "    '''\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path_to_embeddings, binary = False)\n",
    "    \n",
    "    \n",
    "    corpus_embeddings = np.zeros((len(word2id), embedding_dim))\n",
    "    #Should words not found in embedding file be random or zeros? \n",
    "    #Currently 0 and then they are trained anyway\n",
    "    '''Should we use a try/catch block here so that words that are not in vocab force an exception?\n",
    "    Or should we choose every word in vocab and see if it is in embedding vocab-> worst case |V|*|embedding_vocab|\n",
    "    Or vice versa, same worst case but early stopping \n",
    "    Or use key retrieval so all words in embedding vocab are checked if they belong to dictionary key\n",
    "    '''\n",
    "    for word in word2id.keys():\n",
    "        if word in word_vectors.vocab:\n",
    "            corpus_embeddings[word2id[word]] = np.array(word_vectors[word])\n",
    "        \n",
    "    return torch.from_numpy(corpus_embeddings).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_tokenized_list(phrases, delim =\";\"):\n",
    "    '''\n",
    "    Input: A sequence of phrases delimited by provided by delimiter: Ex A B; C; D E F\n",
    "    Output: A list of tokenized phrases: [[A, B], [C], [D, E, F]]\n",
    "    '''\n",
    "    #split_phrases = phrases.split(delim)\n",
    "    try:\n",
    "        split_phrases = phrases.split(delim)\n",
    "        spacy_objs = [nlp(unicode(phrase)) for phrase in split_phrases]\n",
    "        tokenized_split_phrases = map(lambda phrases: [tokens for tokens in phrases], spacy_objs)\n",
    "    except:\n",
    "        print phrases\n",
    "    return tokenized_split_phrases\n",
    "\n",
    "#def nltk_tokenized_list(seq, delim=\";\"):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
