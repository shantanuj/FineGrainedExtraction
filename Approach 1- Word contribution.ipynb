{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import torch\n",
    "#import nltk\n",
    "import numpy as np\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_data = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_laptop.csv\")\n",
    "rest_data = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_restaurant.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain features/objects/relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain1:\n",
    "    def __init__(self, parameters):\n",
    "        None\n",
    "    \n",
    "class Domain2:\n",
    "    def __init__(self, parameters):\n",
    "        None\n",
    "    \n",
    "    \n",
    "def find_approximate_occurance_based_on_structure(): #computes similarity between two sentences based on word structure and word contribution\n",
    "    '''Example:\n",
    "    s1: It is very overpriced and not very tasty\n",
    "    s2: This lacks the features and is very expensive.\n",
    "    s3: This lacks the features and is very expensive for a model A laptop.\n",
    "    s4: It is very overpriced and not very tasty for a Michelin star restaraunt. \n",
    "    overpriced= expensive\n",
    "    tasty = feaatures\n",
    "    very = very\n",
    "    not = lack\n",
    "    Michelin star = model A \n",
    "    \n",
    "    Domain dependent words--> get_domain_dependent_words()\n",
    "    Domain independent words --> get_independent_words()\n",
    "    \n",
    "    'very expensive' is together/combined word \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_approximate_occurance_based_on_words():\n",
    "    '''Example:\n",
    "    \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_inter_domain_similarity():\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach:\n",
    "\n",
    "1. Baseline simple method: BiLSTM + CRF.  This is a baseline model. We need a model with high coverage (high false positives) and then trim the selection using the below model. This can either be simple rules, syntactic structures, just any model with high number of false positives. \n",
    "\n",
    "\n",
    "2. Given a sentence S, train an encoder-decoder network to reproduce the sentence. Compute last hidden state representation based on model M (S-> S). After training for a while, for each sentence: for each aspect word: remove BA, IA labelled words, and get representation. 2nd model takes this representation and the original sentence + some noisy cases and outputs a 1 or 0 depending on task. Effectively, 2nd model is learning to use incomplete representation and other info to learn whether it is an aspect/opinion term. \n",
    "\n",
    "\n",
    "To find out word contribution, by training a model based on missing words. So, if a word is removed, get some representation. Embed the ones with aspects missing to a similar space. Meaning if I have a sentence: I love their ice-cream so much. Then remove ice-cream, get a representation of the sentence, and a representation of the normal (with ice-cream sentence). Using these representations, map as similar (1) in a Siamese net. Now take all other (except opinion terms) and use same model to get representation. But the final model outputs the representations as dissimilar.\n",
    "\n",
    "    G(F(I love their so much), F(I love their ice cream so much)) = 1\n",
    "    G(F(love their ice cream so much), F(I love their ice cream so much)) = 0\n",
    "    G(F(I love their ice cream), F(I love their ice cream so much)) = 2\n",
    "    G(F(I love ice cream so much), F(Sentence)) = \n",
    "\n",
    "\n",
    "\n",
    "        Another approach could be if we remove the word then from all the mined words, if we replace the term with it, does the sentence still make sense (is it legible)-> then it is an aspect word. \n",
    "\n",
    "        So given s= I love their ice cream so much, if we remove ice-cream then replace it with say 50 previous aspect words like -> football, passing, nachos, ambience, does it still make sense? A model M is trained to identify whether a sentence still makes sense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an aspect and what is an opinion term?\n",
    "\n",
    "S1: 'Plus it is small and reasonably light so I can take it with me to and from work. -> small, reasonably light\n",
    "\n",
    "S2:'If internet connectivity is important I would recommend going with a dell net book for 50 bucks more, or buy a USB wireless card.' -> recommend, and internet connectivity;USB wireless card\n",
    "\n",
    "S3: \"Rao is a good restaurant, but it's nothing special.\" -> good, nothing special\n",
    "\n",
    "S4: iLife is easily compatible with Microsoft Office so you can send and receive files from a PC.--> easily compatible-->  iLife;Microsoft Office \n",
    "\n",
    "S5: \"people are rude bit again it's new york!\" -> rude; people\n",
    "\n",
    "S6: \"The speed is incredible and I am more than satisfied.\" --> incredible, 'more than satisfied', 'speed'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['My only other complaint is that it gets really hot.',\n",
       "  'I could not even put my entire music collection on this garabage.',\n",
       "  'Back then my entire family was Devoted to the Sony name.',\n",
       "  'Plus it is small and reasonably light so I can take it with me to and from work.',\n",
       "  'I also experience the same with my MacBook Air.'],\n",
       " [\"Just don't do it.\",\n",
       "  'So some of the reviews here are accurate about the crowd and noise.',\n",
       "  'Service was very friendly.',\n",
       "  \"Don't judge this place prima facie, you have to try it to believe it, a home away from home for the literate heart.\",\n",
       "  'I love this cozy around the way Rest.'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_pd_column = lambda df,column:df.column \n",
    "lap_sentences = selected_lap_examples.Sentence.tolist()\n",
    "rest_sentences = selected_rest_examples.Sentence.tolist()\n",
    "zip(lap_sentencesrest_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To do:\\nMake baseline Bi-directional LSTM and top-CRF model with info about POS tags \\n\\nInputs: POS_tags; Dependency relation wrt next word; Word vector (context info); Info about influence of a word \\n\\nQ: To get a better representation what can we do to the architecture itself (rather than feeding in more features)\\nQ: How do we obtain the influence of a word in a sentence (like on a color scale); attention\\nQ: How do we obtain the interdependeny of words -> dependency structure OR POSE OF WORDS\\nQ: Define interdependency with an example\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To do:\n",
    "Make baseline Bi-directional LSTM and top-CRF model with info about POS tags \n",
    "\n",
    "Inputs: POS_tags; Dependency relation wrt next word; Word vector (context info); Info about influence of a word \n",
    "\n",
    "Q: To get a better representation what can we do to the architecture itself (rather than feeding in more features)\n",
    "Q: How do we obtain the influence of a word in a sentence (like on a color scale); attention\n",
    "Q: How do we obtain the interdependeny of words -> dependency structure OR POSE OF WORDS\n",
    "Q: Define interdependency with an example\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import pandas as pd\n",
    "import nltk \n",
    "import spacy\n",
    "import string\n",
    "torch.manual_seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "laptop_data = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_laptop.csv\")\n",
    "#rest_data = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_restaurant.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using missing words \n",
    "\n",
    "1. Remove respective word and train a model to either detect whether sentence is legible or not. 2nd way: can we get a frequency method to fill in the blank with respective HISTORIC/TRAINING 'other' words, 'opinion' words or 'aspect' words and see which make the most sense.\n",
    "\n",
    "2. Remove respective word(s) and first train an autoencoding-decoding network (normal sentences- no missing words). Then train a 2nd model to take normal repreenation and missing representation, and output 'OP' , 'AP' or 'OTH' \n",
    "    1. Would adding noise to the first autoencoding network be beneficial?\n",
    "    2. Should the 1st model be frozen when we begin training the 2nd model?\n",
    "    3. Should a new sentence be autencoded few times and then use pretrained 2nd model from training?\n",
    "    4. Is using a dependency structure as in Wenya's better. THere's something here, a better way to capture missing word importance representation- something like attention in this dependency structure\n",
    "    5. How do we get prospective words to choose words to remove?: <b>Principle: All nouns are equal, but some nouns are more important. What makes these more important? </b>\n",
    "    6. How do we resolve multi aspect/opinion (BA,IA) expressions? \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering the question: Why are some nouns (of the same tag) more relevant- as an aspect/opinion- than others?  Below we see examples of such occurances (same word tag- but one of them is an aspect/opinion and the other an 'other' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, csv\n",
    "#nlp = spacy.load('en') #load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively all this is stored now in pickle files, so do not need to rerun parsing,etc\n",
    "laptop_data = pickle.load(open('laptop_final_additional_training_list.pickle','rb'))\n",
    "tag_to_id = pickle.load(open('laptop_final_tag2id.pickle'))\n",
    "id_to_tag = {id_:tag for tag, id_ in tag_to_id.items()}\n",
    "word_to_id = pickle.load(open('laptop_final_vocab.pickle'))\n",
    "id_to_word = {id_:word for word, id_ in word_to_id.items()}\n",
    "vocab_len = len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I charge it at night and skip taking the cord with me because of the good battery life',\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 3, 1, 2],\n",
       " [('I', 'PRON', 'PRP', 'nsubj'),\n",
       "  ('charge', 'VERB', 'VBP', 'ROOT'),\n",
       "  ('it', 'PRON', 'PRP', 'dobj'),\n",
       "  ('at', 'ADP', 'IN', 'prep'),\n",
       "  ('night', 'NOUN', 'NN', 'pobj'),\n",
       "  ('and', 'CCONJ', 'CC', 'cc'),\n",
       "  ('skip', 'VERB', 'VB', 'conj'),\n",
       "  ('taking', 'VERB', 'VBG', 'xcomp'),\n",
       "  ('the', 'DET', 'DT', 'det'),\n",
       "  ('cord', 'NOUN', 'NN', 'dobj'),\n",
       "  ('with', 'ADP', 'IN', 'prep'),\n",
       "  ('me', 'PRON', 'PRP', 'pobj'),\n",
       "  ('because', 'ADP', 'IN', 'prep'),\n",
       "  ('of', 'ADP', 'IN', 'pcomp'),\n",
       "  ('the', 'DET', 'DT', 'det'),\n",
       "  ('good', 'ADJ', 'JJ', 'amod'),\n",
       "  ('battery', 'NOUN', 'NN', 'compound'),\n",
       "  ('life', 'NOUN', 'NN', 'pobj')])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_more_than_n_tag_occurs = lambda n, tag, tagged_tokens: [word for i in tagged_tokens if i[1]==tag] \n",
    "\n",
    "def get_more_than_n_tag_occurs(tagged_tokens, n= 2 , tag_list= [\"NOUN\",\"PRON\"], tag_index = 1):\n",
    "    filtered_toks = [i for i in tagged_tokens if i[tag_index] in tag_list]\n",
    "    if(len(filtered_toks)>=n):\n",
    "        return filtered_toks\n",
    "    else:\n",
    "        return len(filtered_toks)\n",
    "    \n",
    "' '.join(laptop_data[0][0]), laptop_data[0][1], laptop_data[0][2]\n",
    "#' '.join(laptop_data[0][0]), get_more_than_n_tag_occurs(laptop_data[0][2])\n",
    "#How is night and cord and battery_life different? \n",
    "#From a human level, 'night' is representing when the person ('I') is doing something. \n",
    "#Whereas cord\n",
    "\n",
    "#good is the only adjective here\n",
    "#laptop_data[0][0], get_more_than_n_tag_occurs(laptop_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NOUN case:', 'it is of high quality has a killer GUI is extremely stable is highly expandable is bundled with lots of very good applications is easy to use and is absolutely gorgeous', [('quality', 'NOUN', 'NN', 'pobj'), ('killer', 'NOUN', 'NN', 'compound'), ('GUI', 'PROPN', 'NNP', 'nsubj'), ('lots', 'NOUN', 'NNS', 'pobj'), ('applications', 'NOUN', 'NNS', 'pobj')])\n",
      "('ADJ case:', 'it is of high quality has a killer GUI is extremely stable is highly expandable is bundled with lots of very good applications is easy to use and is absolutely gorgeous', [('high', 'ADJ', 'JJ', 'amod'), ('stable', 'ADJ', 'JJ', 'acomp'), ('expandable', 'ADJ', 'JJ', 'acomp'), ('good', 'ADJ', 'JJ', 'amod'), ('easy', 'ADJ', 'JJ', 'acomp'), ('gorgeous', 'ADJ', 'JJ', 'acomp')])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('it', 'OT'),\n",
       " ('is', 'OT'),\n",
       " ('of', 'OT'),\n",
       " ('high', 'BO'),\n",
       " ('quality', 'BA'),\n",
       " ('has', 'OT'),\n",
       " ('a', 'OT'),\n",
       " ('killer', 'OT'),\n",
       " ('GUI', 'BA'),\n",
       " ('is', 'OT'),\n",
       " ('extremely', 'OT'),\n",
       " ('stable', 'BO'),\n",
       " ('is', 'OT'),\n",
       " ('highly', 'OT'),\n",
       " ('expandable', 'BO'),\n",
       " ('is', 'OT'),\n",
       " ('bundled', 'OT'),\n",
       " ('with', 'OT'),\n",
       " ('lots', 'OT'),\n",
       " ('of', 'OT'),\n",
       " ('very', 'OT'),\n",
       " ('good', 'BO'),\n",
       " ('applications', 'BA'),\n",
       " ('is', 'OT'),\n",
       " ('easy', 'BO'),\n",
       " ('to', 'OT'),\n",
       " ('use', 'BA'),\n",
       " ('and', 'OT'),\n",
       " ('is', 'OT'),\n",
       " ('absolutely', 'OT'),\n",
       " ('gorgeous', 'BO')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"NOUN case:\", ' '.join(laptop_data[5][0]), get_more_than_n_tag_occurs(laptop_data[5][2], tag_list=[\"NOUN\",\"PROPN\"]))\n",
    "print(\"ADJ case:\", ' '.join(laptop_data[5][0]), get_more_than_n_tag_occurs(laptop_data[5][2], tag_list=[\"ADJ\"]))\n",
    "\n",
    "'''\n",
    "Very interesting example\n",
    "quality is actually an Opinion term<- how do we know this, because it's part of an ADJ modification (high quality, good quality, etc,)What if it occured independently?\n",
    "Ex: It's quality is suspect. <- THis is an aspect \n",
    "\n",
    "So many cases of ADJ and NOUN yet only few are chosen. Why: answer this\n",
    "\n",
    "Also, opinions do not have to be restricted to aspects \n",
    "'''\n",
    "\n",
    "list(zip(laptop_data[5][0],map(lambda x: id_to_tag[x], laptop_data[5][1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<END>': -1, '<START>': 0, 'BA': 1, 'BO': 3, 'IA': 2, 'IO': 4, 'OT': 5}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make autoencoder model, train it to reproduce same sentence by computing final hidden representation. \n",
    "#Ans the question is adding noise to this worth it? \n",
    "\n",
    "#Noise reconstruction means we try to get representations through incomplete info to be approximately the same as with complete info.\n",
    "#If we are using a model to learn how the difference between a noisy representation and a compelete representation (missing word vs no missing word), then we want the representation to be different, and not almost the same. \n",
    "\n",
    "#2nd question why does a missing word which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do parser trees change with deletion of items\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
