{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import torch\n",
    "#import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain features/objects/relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain1:\n",
    "    def __init__(self, parameters):\n",
    "        None\n",
    "    \n",
    "class Domain2:\n",
    "    def __init__(self, parameters):\n",
    "        None\n",
    "    \n",
    "    \n",
    "def find_approximate_occurance_based_on_structure(): #computes similarity between two sentences based on word structure and word contribution\n",
    "    '''Example:\n",
    "    s1: It is very overpriced and not very tasty\n",
    "    s2: This lacks the features and is very expensive.\n",
    "    s3: This lacks the features and is very expensive for a model A laptop.\n",
    "    s4: It is very overpriced and not very tasty for a Michelin star restaraunt. \n",
    "    overpriced= expensive\n",
    "    tasty = feaatures\n",
    "    very = very\n",
    "    not = lack\n",
    "    Michelin star = model A \n",
    "    \n",
    "    Domain dependent words--> get_domain_dependent_words()\n",
    "    Domain independent words --> get_independent_words()\n",
    "    \n",
    "    'very expensive' is together/combined word \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_approximate_occurance_based_on_words():\n",
    "    '''Example:\n",
    "    \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_inter_domain_similarity():\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach:\n",
    "\n",
    "1. Baseline simple method: BiLSTM + CRF.  This is a baseline model. We need a model with high coverage (high false positives) and then trim the selection using the below model. This can either be simple rules, syntactic structures, just any model with high number of false positives. \n",
    "\n",
    "\n",
    "2. Given a sentence S, train an encoder-decoder network to reproduce the sentence. Compute last hidden state representation based on model M (S-> S). After training for a while, for each sentence: for each aspect word: remove BA, IA labelled words, and get representation. 2nd model takes this representation and the original sentence + some noisy cases and outputs a 1 or 0 depending on task. Effectively, 2nd model is learning to use incomplete representation and other info to learn whether it is an aspect/opinion term. \n",
    "\n",
    "\n",
    "To find out word contribution, by training a model based on missing words. So, if a word is removed, get some representation. Embed the ones with aspects missing to a similar space. Meaning if I have a sentence: I love their ice-cream so much. Then remove ice-cream, get a representation of the sentence, and a representation of the normal (with ice-cream sentence). Using these representations, map as similar (1) in a Siamese net. Now take all other (except opinion terms) and use same model to get representation. But the final model outputs the representations as dissimilar.\n",
    "\n",
    "    G(F(I love their so much), F(I love their ice cream so much)) = 1\n",
    "    G(F(love their ice cream so much), F(I love their ice cream so much)) = 0\n",
    "    G(F(I love their ice cream), F(I love their ice cream so much)) = 2\n",
    "    G(F(I love ice cream so much), F(Sentence)) = \n",
    "\n",
    "\n",
    "\n",
    "        Another approach could be if we remove the word then from all the mined words, if we replace the term with it, does the sentence still make sense (is it legible)-> then it is an aspect word. \n",
    "\n",
    "        So given s= I love their ice cream so much, if we remove ice-cream then replace it with say 50 previous aspect words like -> football, passing, nachos, ambience, does it still make sense? A model M is trained to identify whether a sentence still makes sense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an aspect and what is an opinion term?\n",
    "\n",
    "S1: 'Plus it is small and reasonably light so I can take it with me to and from work. -> small, reasonably light\n",
    "\n",
    "S2:'If internet connectivity is important I would recommend going with a dell net book for 50 bucks more, or buy a USB wireless card.' -> recommend, and internet connectivity;USB wireless card\n",
    "\n",
    "S3: \"Rao is a good restaurant, but it's nothing special.\" -> good, nothing special\n",
    "\n",
    "S4: iLife is easily compatible with Microsoft Office so you can send and receive files from a PC.--> easily compatible-->  iLife;Microsoft Office \n",
    "\n",
    "S5: \"people are rude bit again it's new york!\" -> rude; people\n",
    "\n",
    "S6: \"The speed is incredible and I am more than satisfied.\" --> incredible, 'more than satisfied', 'speed'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Method 1: Using missing words \n",
    "\n",
    "1. Remove respective word and train a model to either detect whether sentence is legible or not. 2nd way: can we get a frequency method to fill in the blank with respective HISTORIC/TRAINING 'other' words, 'opinion' words or 'aspect' words and see which make the most sense.\n",
    "\n",
    "2. Remove respective word(s) and first train an autoencoding-decoding network (normal sentences- no missing words). Then train a 2nd model to take normal repreenation and missing representation, and output 'OP' , 'AP' or 'OTH' \n",
    "    1. Would adding noise to the first autoencoding network be beneficial? No:Cause representations need to be diff.\n",
    "    2. Should the 1st model be frozen when we begin training the 2nd model?\n",
    "    3. Should a new sentence be autencoded few times and then use pretrained 2nd model from training?\n",
    "    4. Is using a dependency structure as in Wenya's better. THere's something here, a better way to capture missing word importance representation- something like attention in this dependency structure\n",
    "    5. How do we get prospective words to choose words to remove?: <b>Principle: All nouns are equal, but some nouns are more important. What makes these more important? </b>\n",
    "    6. How do we resolve multi aspect/opinion (BA,IA) expressions? \n",
    "    7. Should we use a MISSING token to represent absent words and obtain representation? \n",
    "    8. Which step of the LSTM/Structure should we use (last hidden state vs missing state vs Tree RNN structure state)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering the question: Why are some nouns (of the same tag) more relevant- as an aspect/opinion- than others?  Below we see examples of such occurances (same word tag- but one of them is an aspect/opinion and the other an 'other' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import string\n",
    "torch.manual_seed(10)\n",
    "from nltk import Tree\n",
    "\n",
    "#nlp = spacy.load('en') #load spacy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring POS tags and where multiple nouns/types occur: How are they different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Final_data/laptop_lower_additional_training_list.pickle\") as p1:\n",
    "    laptop_data = pickle.load(p1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i charge it at night and skip taking the cord with me because of the good battery life',\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 3, 1, 2],\n",
       " [('i', 'PRON', 'PRP', 'nsubj'),\n",
       "  ('charge', 'VERB', 'VBP', 'ROOT'),\n",
       "  ('it', 'PRON', 'PRP', 'dobj'),\n",
       "  ('at', 'ADP', 'IN', 'prep'),\n",
       "  ('night', 'NOUN', 'NN', 'pobj'),\n",
       "  ('and', 'CCONJ', 'CC', 'cc'),\n",
       "  ('skip', 'VERB', 'VB', 'conj'),\n",
       "  ('taking', 'VERB', 'VBG', 'xcomp'),\n",
       "  ('the', 'DET', 'DT', 'det'),\n",
       "  ('cord', 'NOUN', 'NN', 'dobj'),\n",
       "  ('with', 'ADP', 'IN', 'prep'),\n",
       "  ('me', 'PRON', 'PRP', 'pobj'),\n",
       "  ('because', 'ADP', 'IN', 'prep'),\n",
       "  ('of', 'ADP', 'IN', 'pcomp'),\n",
       "  ('the', 'DET', 'DT', 'det'),\n",
       "  ('good', 'ADJ', 'JJ', 'amod'),\n",
       "  ('battery', 'NOUN', 'NN', 'compound'),\n",
       "  ('life', 'NOUN', 'NN', 'pobj')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_more_than_n_tag_occurs(tagged_tokens, n= 2 , tag_list= [\"NOUN\",\"PRON\"], tag_index = 1):\n",
    "    filtered_toks = [i for i in tagged_tokens if i[tag_index] in tag_list]\n",
    "    if(len(filtered_toks)>=n):\n",
    "        return filtered_toks\n",
    "    else:\n",
    "        return len(filtered_toks)\n",
    "    \n",
    "\n",
    "#rest_data = pd.read_csv(\"Final_data/Semeval_14_ver1/Combined_restaurant.csv\")\n",
    "\n",
    "' '.join(laptop_data[0][0]), laptop_data[0][1], laptop_data[0][2]\n",
    "\n",
    "\n",
    "\n",
    "#' '.join(laptop_data[0][0]), get_more_than_n_tag_occurs(laptop_data[0][2])\n",
    "#How is night and cord and battery_life different? \n",
    "#From a human level, 'night' is representing when the person ('I') is doing something. \n",
    "#Whereas cord\n",
    "\n",
    "#good is the only adjective here\n",
    "#laptop_data[0][0], get_more_than_n_tag_occurs(laptop_data[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NOUN case:', 'it is of high quality has a killer gui is extremely stable is highly expandable is bundled with lots of very good applications is easy to use and is absolutely gorgeous', [('quality', 'NOUN', 'NN', 'pobj'), ('killer', 'NOUN', 'NN', 'compound'), ('gui', 'NOUN', 'NN', 'nsubj'), ('lots', 'NOUN', 'NNS', 'pobj'), ('applications', 'NOUN', 'NNS', 'pobj')])\n",
      "('ADJ case:', 'it is of high quality has a killer gui is extremely stable is highly expandable is bundled with lots of very good applications is easy to use and is absolutely gorgeous', [('high', 'ADJ', 'JJ', 'amod'), ('stable', 'ADJ', 'JJ', 'acomp'), ('expandable', 'ADJ', 'JJ', 'acomp'), ('good', 'ADJ', 'JJ', 'amod'), ('easy', 'ADJ', 'JJ', 'acomp'), ('gorgeous', 'ADJ', 'JJ', 'acomp')])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'id_to_tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5d13b2e38506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m '''\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaptop_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mid_to_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaptop_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-5d13b2e38506>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m '''\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaptop_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mid_to_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlaptop_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'id_to_tag' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"NOUN case:\", ' '.join(laptop_data[5][0]), get_more_than_n_tag_occurs(laptop_data[5][2], tag_list=[\"NOUN\",\"PROPN\"]))\n",
    "print(\"ADJ case:\", ' '.join(laptop_data[5][0]), get_more_than_n_tag_occurs(laptop_data[5][2], tag_list=[\"ADJ\"]))\n",
    "\n",
    "'''\n",
    "Very interesting example\n",
    "quality is actually an Opinion term<- how do we know this, because it's part of an ADJ modification (high quality, good quality, etc,)What if it occured independently?\n",
    "Ex: It's quality is suspect. <- THis is an aspect \n",
    "\n",
    "So many cases of ADJ and NOUN yet only few are chosen. Why: answer this\n",
    "\n",
    "Also, opinions do not have to be restricted to aspects \n",
    "'''\n",
    "\n",
    "list(zip(laptop_data[5][0],map(lambda x: id_to_tag[x], laptop_data[5][1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make autoencoder model, train it to reproduce same sentence by computing final hidden representation. \n",
    "#Ans the question is adding noise to this worth it? \n",
    "\n",
    "#Noise reconstruction means we try to get representations through incomplete info to be approximately the same as with complete info.\n",
    "#If we are using a model to learn how the difference between a noisy representation and a compelete representation (missing word vs no missing word), then we want the representation to be different, and not almost the same. \n",
    "\n",
    "#2nd question why does a missing word which has the same tag/context not be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## How do parser trees change when important words go missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dependency parser tree: https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy\n",
    "\n",
    "\n",
    "def tok_format(tok):\n",
    "    return \"_\".join([tok.orth_, tok.tag_])\n",
    "\n",
    "\n",
    "\n",
    "def to_nltk_tree(node, token_tagged = False):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        if token_tagged:\n",
    "            return Tree(tok_format(node), [to_nltk_tree(child, True) for child in node.children])\n",
    "        else:\n",
    "            return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        if token_tagged:\n",
    "            return tok_format(node)\n",
    "        else:\n",
    "            return node.orth_\n",
    "\n",
    "def get_dependency_parser_tree(seq):\n",
    "    \"\"\"Input is a list of tokens\"\"\"\n",
    "    doc = nlp(unicode(' '.join(seq)))\n",
    "    return [to_nltk_tree(sent.root, True).pretty_print() for sent in doc.sents]\n",
    "\n",
    "#now by removing some aspects/opinions we observe the dependency trees\n",
    "get_ignore_indices = lambda filter_list, input_tag_seq: [i for i, tag in enumerate(input_tag_seq) if tag in filter_list]\n",
    "#w1 w3 w4\n",
    "def get_filtered_sentence(seq, tag_seq, filter_list= [2]):\n",
    "    ignore_indices = get_ignore_indices(filter_list, tag_seq)\n",
    "    #print(ignore_indices)\n",
    "    return [word for i, word in enumerate(seq) if i not in ignore_indices]\n",
    "\n",
    "#' '.join(get_filtered_sentence(laptop_data[0][0], laptop_data[0][1], [1,2])) #removing BA and IA words (1 and 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I charge it at night and skip taking the cord with me because of the good battery life\n",
      "                             charge_VBP                                                                \n",
      "   ______________________________|____________________________                                          \n",
      "  |     |      |       |                                   skip_VB                                     \n",
      "  |     |      |       |                     _________________|_______________                          \n",
      "  |     |      |       |                taking_VBG                        because_IN                   \n",
      "  |     |      |       |          __________|_________         _______________|_________                \n",
      "  |     |      |     at_IN    cord_NN              with_IN    |                      life_NN           \n",
      "  |     |      |       |         |                    |       |       __________________|________       \n",
      "I_PRP it_PRP and_CC night_NN   the_DT               me_PRP  of_IN  the_DT            good_JJ battery_NN\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ' '.join(laptop_data[0][0])\n",
    "get_dependency_parser_tree(laptop_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'with', 'me', 'because', 'of', 'the', 'good']\n",
      "                    charge_VBP                                                           \n",
      "   _____________________|___________________________________                              \n",
      "  |     |      |        |                                skip_VB                         \n",
      "  |     |      |        |                  _________________|______________               \n",
      "  |     |      |        |             taking_VBG                       because_IN        \n",
      "  |     |      |        |         ________|_________                _______|_________     \n",
      "  |     |      |      at_IN      |               with_IN           |              good_JJ\n",
      "  |     |      |        |        |                  |              |                 |    \n",
      "I_PRP it_PRP and_CC  night_NN  the_DT             me_PRP         of_IN             the_DT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = get_filtered_sentence(laptop_data[0][0], laptop_data[0][1], [1,2]) #removing BA and IA words (1 and 2)\n",
    "print filtered_sentence\n",
    "get_dependency_parser_tree(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'battery', 'life']\n",
      "                             charge_VBP                                                         \n",
      "   ______________________________|____________________________                                   \n",
      "  |     |      |       |                                   skip_VB                              \n",
      "  |     |      |       |                     _________________|________                          \n",
      "  |     |      |       |                taking_VBG                 because_IN                   \n",
      "  |     |      |       |          __________|_________         ________|_________                \n",
      "  |     |      |     at_IN    cord_NN              with_IN    |               life_NN           \n",
      "  |     |      |       |         |                    |       |         _________|________       \n",
      "I_PRP it_PRP and_CC night_NN   the_DT               me_PRP  of_IN    the_DT           battery_NN\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = get_filtered_sentence(laptop_data[0][0], laptop_data[0][1], [3,4]) #removing BO and IO words (1 and 2)\n",
    "print filtered_sentence\n",
    "get_dependency_parser_tree(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              i_PRP                                                                 \n",
      "   _____________________________|__________________________                                          \n",
      "  |      |     |                                        skip_NN                                     \n",
      "  |      |     |        ___________________________________|_______________                          \n",
      "  |      |     |       |             taking_VBG                        because_IN                   \n",
      "  |      |     |       |         ________|_________         _______________|_________                \n",
      "  |      |     |       |     cord_NN            with_IN    |                      life_NN           \n",
      "  |      |     |       |        |                  |       |       __________________|________       \n",
      "it_PRP at_IN and_CC night_NN  the_DT             me_PRP  of_IN  the_DT            good_JJ battery_NN\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = laptop_data[0][0]\n",
    "#filtered_sentence.insert(5,'night')\n",
    "#filtered_sentence.remove('night')\n",
    "get_dependency_parser_tree(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          charge_VBP                                                         \n",
      "   ___________________________|____________________________                                   \n",
      "  |     |      |     |                                  skip_VB                              \n",
      "  |     |      |     |                    _________________|________                          \n",
      "  |     |      |     |               taking_VBG                 because_IN                   \n",
      "  |     |      |     |         __________|_________         ________|_________                \n",
      "  |     |      |     |     cord_NN              with_IN    |               life_NN           \n",
      "  |     |      |     |        |                    |       |         _________|________       \n",
      "I_PRP it_PRP at_IN and_CC   the_DT               me_PRP  of_IN    the_DT           battery_NN\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence.remove('night')\n",
    "get_dependency_parser_tree(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### In doing the experiment with using missing word representations, we can have many variations. \n",
    "1. Do we train the model to reproduce complete sentence? (As stated adding noise seems counterintuitive)\n",
    "2. If we use seq2seq model encoders, we could do either of the three:\n",
    "    1. Remove the word completely and use last hidden representation as the decider for type of missing word\n",
    "    2. Replace the word with an 'UNK' token and compute immediate representation at that step, compared to previous steps. \n",
    "    3. Combine 1 and 2 both \n",
    "    \n",
    "3. Let's say for a given input sentence: w1 w2 w3... wn, how do we:\n",
    "    1. Decide candidates to remove?: Need another model for this?? Or do we use a simple heuristic: all nouns, adjs and verbs\n",
    "    2. What if multiple BA IA word seq is there? Do we remove them together?\n",
    "    3. When we remove, do we use the standard seq2seq architectures, or do we use the Tree RNN type?\n",
    "    4. Given that we have detected w1 to w3 as BA..IA then how do we include this information when W6 has been detected as an opinion word. Do we get representations for all possible words at different time steps, and use their config as well in deciding the final seq labels ?\n",
    "    \n",
    "    \n",
    "#### In doing experiment with missing word FOR using historic label representative data--> do we try each characteristic word (so Hello, hi, hey--> binned into 1 cluster, similarly OP_WORDS that are similar binned into 1 cluster) and use a MODEL to detect which resulting sentence makes most sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple implementation of the first model \n",
    "#Steps\n",
    "\n",
    "#1) Take a simple heuristic rule to take positive examples and negative examples for each category (BA, BA.IA, BO, BO.IO, Ot)\n",
    "# 2nd model is a simple siames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "### Script to get tag-token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Script to get frequencies of POS tags for opinion and aspect words'''\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.data import load\n",
    "#tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "examples_of_aspect_as_ADJ = []\n",
    "examples_of_opinion_as_NOUN = []\n",
    "\n",
    "\n",
    "def get_freq_pos_opinion_and_aspect(df, limit =-1):\n",
    "    '''Input is a dataframe'''\n",
    "    #bopinion_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #baspect_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #iopinion_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #iaspect_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    bopinion_tags = {} \n",
    "    baspect_tags = {}\n",
    "    iopinion_tags = {} \n",
    "    iaspect_tags = {}\n",
    "    '''\n",
    "    if(limit==-1):\n",
    "        limit = len(df)\n",
    "        print_it = False\n",
    "    else:\n",
    "        limit = limit #for debugging\n",
    "        '''\n",
    "    for sentence, opinion, aspect in zip(df.Sentence, df.Opinions, df.Aspects):\n",
    "        '''\n",
    "        if(limit<=0):\n",
    "            break\n",
    "        limit-=1\n",
    "        '''\n",
    "        '''NLTK\n",
    "        #text = word_tokenize(sentence) #<-NLTK\n",
    "        #pos_tagged_text = pos_tag(text) #<-NLTK\n",
    "        '''\n",
    "        text = nlp(unicode(sentence))\n",
    "        pos_tagged_text = [(str(token), str(token.pos_)) for token in text]\n",
    "        \n",
    "        \n",
    "        for seqs in opinion.split(';'): #find all opinion terms-> can be done together with aspect terms but cleaner this way\n",
    "            for i, opinion_term in enumerate(seqs.split()):\n",
    "                for token_pos_tuple in pos_tagged_text:\n",
    "                    token, pos_tag = token_pos_tuple[0], token_pos_tuple[1]\n",
    "                    if(opinion_term == token):\n",
    "                        if(i>1):#IO\n",
    "                            if(pos_tag not in iopinion_tags):\n",
    "                                iopinion_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                iopinion_tags[pos_tag] += 1\n",
    "                        else:#BO\n",
    "                            if(pos_tag not in bopinion_tags):\n",
    "                                bopinion_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                bopinion_tags[pos_tag] += 1\n",
    "                        '''    \n",
    "                        if(limit> 0 and print_it):\n",
    "                            print(sentence, token_pos_tuple)\n",
    "                        '''\n",
    "                        if(pos_tag == \"NOUN\"):\n",
    "                            examples_of_opinion_as_NOUN.append((sentence, token_pos_tuple))\n",
    "                        \n",
    "        for seqs in aspect.split(';'): #find all aspect terms\n",
    "            for i, aspect_term in enumerate(seqs.split()):\n",
    "                for token_pos_tuple in pos_tagged_text:\n",
    "                    token, pos_tag = token_pos_tuple[0], token_pos_tuple[1]\n",
    "                    if(aspect_term == token):\n",
    "                        if(i>1):#IA\n",
    "                            if(pos_tag not in iaspect_tags):\n",
    "                                iaspect_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                iaspect_tags[pos_tag] += 1\n",
    "                        else:#BA\n",
    "                            if(pos_tag not in baspect_tags):\n",
    "                                baspect_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                baspect_tags[pos_tag] += 1\n",
    "                       \n",
    "                        if(pos_tag == 'ADJ'):\n",
    "                            examples_of_aspect_as_ADJ.append((sentence, token_pos_tuple))\n",
    "    return bopinion_tags, iopinion_tags, baspect_tags, iaspect_tags\n",
    "\n",
    "#laptop_df = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_laptop.csv\")\n",
    "#laptop_freq_dicts = get_freq_pos_opinion_and_aspect(laptop_df)\n",
    "\n",
    "#rest_df = pd.read_csv(\"./Final_data/Semeval_14_ver1/Combined_restaurant.csv\")\n",
    "#rest_freq_dicts = get_freq_pos_opinion_and_aspect(rest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(({'ADJ': 1507,\n",
       "   'ADP': 49,\n",
       "   'ADV': 372,\n",
       "   'CCONJ': 3,\n",
       "   'DET': 36,\n",
       "   'INTJ': 1,\n",
       "   'NOUN': 256,\n",
       "   'NUM': 4,\n",
       "   'PART': 26,\n",
       "   'PROPN': 1,\n",
       "   'VERB': 477},\n",
       "  {'ADJ': 28,\n",
       "   'ADP': 12,\n",
       "   'ADV': 23,\n",
       "   'CCONJ': 2,\n",
       "   'DET': 13,\n",
       "   'NOUN': 18,\n",
       "   'PART': 6,\n",
       "   'PRON': 1,\n",
       "   'VERB': 16},\n",
       "  {'ADJ': 183,\n",
       "   'ADP': 36,\n",
       "   'ADV': 1,\n",
       "   'CCONJ': 5,\n",
       "   'DET': 11,\n",
       "   'NOUN': 2395,\n",
       "   'NUM': 68,\n",
       "   'PART': 26,\n",
       "   'PRON': 4,\n",
       "   'PROPN': 464,\n",
       "   'VERB': 262},\n",
       "  {'ADJ': 7,\n",
       "   'ADP': 15,\n",
       "   'ADV': 1,\n",
       "   'CCONJ': 1,\n",
       "   'DET': 25,\n",
       "   'NOUN': 196,\n",
       "   'NUM': 4,\n",
       "   'PART': 1,\n",
       "   'PROPN': 37,\n",
       "   'VERB': 6}),\n",
       " ({'ADJ': 2665,\n",
       "   'ADP': 39,\n",
       "   'ADV': 195,\n",
       "   'CCONJ': 1,\n",
       "   'DET': 3,\n",
       "   'INTJ': 2,\n",
       "   'NOUN': 137,\n",
       "   'NUM': 2,\n",
       "   'PART': 9,\n",
       "   'PROPN': 2,\n",
       "   'VERB': 418},\n",
       "  {'ADJ': 1, 'DET': 6, 'NOUN': 9},\n",
       "  {'ADJ': 288,\n",
       "   'ADP': 110,\n",
       "   'ADV': 5,\n",
       "   'CCONJ': 20,\n",
       "   'DET': 12,\n",
       "   'NOUN': 3933,\n",
       "   'NUM': 3,\n",
       "   'PART': 3,\n",
       "   'PRON': 1,\n",
       "   'PROPN': 257,\n",
       "   'VERB': 155,\n",
       "   'X': 3},\n",
       "  {'ADJ': 37,\n",
       "   'ADP': 51,\n",
       "   'ADV': 2,\n",
       "   'CCONJ': 38,\n",
       "   'DET': 33,\n",
       "   'NOUN': 346,\n",
       "   'PART': 1,\n",
       "   'PRON': 1,\n",
       "   'PROPN': 44,\n",
       "   'VERB': 24}))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_freq_dicts, rest_freq_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1) Training an autoencoder for sentences. (We then freeze the best working model for later use in missing words)\n",
    "\n",
    "##### All experiments until the next section (missing words) is with regards to training the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Autoencoder for sentences (Encoder-decoder network)'''\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch import optim \n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "use_cuda = torch.cuda.is_available() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class Encoder_BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dim):\n",
    "        super(Encoder_BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, bidirectional = True) #hidden dim//2 cause it's a bilstm (automatically concats across 1st axis)\n",
    "        self.hidden = \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.randn(2, 1, self.hidden_dim//2)), autograd.Variable(torch.randn(2,1, self.hidden_dim//2)))\n",
    "    \n",
    "    def forward(self, input_seq, hidden):\n",
    "        inputs =\n",
    "'''\n",
    "\n",
    "class Encoder_GRU(nn.Module):\n",
    "    def __init__(self, input_domain_size, hidden_dims, embedding_dims, with_pretrained_embeddings = False,  pretrained_embedding_matrix = None, freeze_embeddings = False, input_domain_word2id = None):\n",
    "        super(Encoder_GRU, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.gru = nn.GRU(embedding_dims, hidden_dims)\n",
    "        \n",
    "        if(not with_pretrained_embeddings):\n",
    "            self.embedding = nn.Embedding(input_domain_size, embedding_dims)\n",
    "        else:\n",
    "            #embeddings = load_embeddings(path_to_embeddings, input_domain_word2id, embedding_dims) #this is a numpy array\n",
    "          \n",
    "            assert input_domain_size == pretrained_embedding_matrix.size()[0]#should match\n",
    "            assert embedding_dims == pretrained_embedding_matrix.size()[1] \n",
    "            self.embedding = nn.Embedding(input_domain_size, embedding_dims)\n",
    "            self.embedding.weight = nn.Parameter(pretrained_embedding_matrix)\n",
    "            if(freeze_embeddings):\n",
    "                self.embedding.requires_grad = False\n",
    "                \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        '''Get the first hidden state'''\n",
    "        #hidden_var =  (autograd.Variable(torch.randn(1, 1, self.hidden_dims)))\n",
    "        hidden_var = autograd.Variable(torch.zeros(1, 1, self.hidden_dims)) #makes more sense to be 0 for all? Why would randomness across different inputs help?\n",
    "        if(use_cuda):\n",
    "            return hidden_var.cuda()\n",
    "        else:\n",
    "            return hidden_var\n",
    "        \n",
    "    def forward(self, input_at_current_time_step, hidden):\n",
    "        '''Expected input is of shape 1*dims'''\n",
    "        embedded_input_seq = self.embedding(input_at_current_time_step).view(1,1,-1) #turn into RNN format [seq_length, minibatch_size, dimensions]\n",
    "        output, hidden = self.gru(embedded_input_seq, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_GRU(nn.Module):\n",
    "    def __init__(self, output_domain_size, hidden_dims, embedding_dims, with_pretrained_embeddings = False,  pretrained_embedding_matrix = None, freeze_embeddings = False, output_domain_word2id = None): \n",
    "        #output size in this case remains same as input_size (since we're reproducing in the same domain)\n",
    "        super(Decoder_GRU, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.gru = nn.GRU(embedding_dims, hidden_dims)\n",
    "        self.out = nn.Linear(hidden_dims, output_domain_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1) #dim is 1 (column), since output of linear is of shape batch=1* output_vocab\n",
    "        \n",
    "        if(not with_pretrained_embeddings):\n",
    "            self.embedding = nn.Embedding(output_domain_size, embedding_dims)\n",
    "        else:\n",
    "            #embeddings = load_embeddings(path_to_embeddings, output_domain_word2id, embedding_dims) #this is a numpy array\n",
    "            assert output_domain_size == pretrained_embedding_matrix.size()[0]#should match\n",
    "            assert embedding_dims == pretrained_embedding_matrix.size()[1] \n",
    "            self.embedding = nn.Embedding(output_domain_size, embedding_dims)\n",
    "            self.embedding.weight = nn.Parameter(pretrained_embedding_matrix)\n",
    "            if(freeze_embeddings):\n",
    "                self.embedding.requires_grad = False\n",
    "                \n",
    "    def init_hidden(self):\n",
    "        #hidden_var =  (autograd.Variable(torch.randn(1, 1, self.hidden_dims)))\n",
    "        hidden_var = autograd.Variable(torch.zeros(1, 1, self.hidden_dims))\n",
    "        if(use_cuda):\n",
    "            return hidden_var.cuda()\n",
    "        else:\n",
    "            return hidden_var\n",
    "        \n",
    "    def forward(self, input_at_time_step, hidden):\n",
    "        embedded_input = self.embedding(input_at_time_step).view(1,1,-1)\n",
    "        relud_embedding = F.relu(embedded_input) #just cause it works for others\n",
    "        output_at_time_step, hidden = self.gru(relud_embedding, hidden) #in the current case where we loop through each time step, the output is the same as the hidden state\n",
    "        softmaxed_prediction = self.softmax(self.out(output_at_time_step[0])) #index output_time_step by row to get shape 1*hidden_dims\n",
    "        return softmaxed_prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Training model for each sequence function. This is a subsidiary of the training function'''\n",
    "def train_model(input_seq_variable, output_seq_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, cost_func, start_token, end_token, max_length = 30, teacher_forcing_ratio = 0.4):\n",
    "    #.....................................................\n",
    "    #This function trains the encoder and decoder architecture jointly to reproduce input and output sequences\n",
    "    #High level overview:\n",
    "    #1) With step by step encoder, get the hidden state at each time step\n",
    "    #2) Use last step hidden rep, any relevant step (ex:missing variable) hidden rep or attention based cumulation to pass as decoder initial hidden step\n",
    "    #3) Now decode using hidden info and other ways to include encoding info (IF REQUIRED). Use softmax and top 1 prediction to get output distribution\n",
    "    #4) Calculate cost (in this case should be negative log likelihood)\n",
    "    #5) Backpropate through loss\n",
    "    #6) Update variables based on optimizer\n",
    "    #......................................................\n",
    "    \n",
    "    #1.1) initialize encoder hidden state\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad() #Reset accumulated gradients from previous batch\n",
    "    loss = 0\n",
    "    \n",
    "    input_length = input_seq_variable.size()[0]\n",
    "    output_length = output_seq_variable.size()[0]\n",
    "    \n",
    "    \n",
    "    '''We do not for this model need encoder_outputs since attention is not used and we do not make use of historic encoder outputs'''\n",
    "    #encoder_outputs = autograd.Variable(torch.zeros(max_length, encoder.hidden_dims)) #store encoder hidden states for later use\n",
    "    #encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    #1.2) Get encoded representations \n",
    "    for encoder_time_step in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_seq_variable[encoder_time_step], encoder_hidden) \n",
    "        #encoder_outputs[encoder_time_step] = encoder_output[0][0] #store encoder hidden state (output in this case since we're looping individually)\n",
    "        \n",
    "        \n",
    "    #2.1) for the decoder, we have to add the start token \n",
    "    decoder_input = autograd.Variable(torch.LongTensor([[start_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    #2.2) Initialize hidden state for decoder\n",
    "    decoder_hidden = encoder_hidden #encoder_outputs[-1]<- this won't be true in case we don't loop so avoid\n",
    "    \n",
    "    use_teacher_forcing = True if random.random()< teacher_forcing_ratio else False\n",
    "    \n",
    "    #3) Run decoder and optimize to reproduce input\n",
    "    \n",
    "    if(use_teacher_forcing):\n",
    "        for decoder_time_step in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) #replace with other variants later\n",
    "            loss+= cost_func(decoder_output, output_seq_variable[decoder_time_step]) #adding loss for this sequence\n",
    "            decoder_input = output_seq_variable[decoder_time_step]\n",
    "    \n",
    "    else:\n",
    "        for decoder_time_step in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            top_prob, top_index = decoder_output.data.topk(1)\n",
    "            next_input_id = top_index[0][0] #use this index as input from vocab\n",
    "            \n",
    "            decoder_input = autograd.Variable(torch.LongTensor([[next_input_id]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "            loss+= cost_func(decoder_output, output_seq_variable[decoder_time_step])\n",
    "            if(next_input_id == end_token):\n",
    "                '''should outputting an EOS when it is not be heavily punished by loss function? \n",
    "                However, that may lead to future longer sentences (as short sentences are punished).\n",
    "                Regardless, how can we implement that'''\n",
    "                break\n",
    "           \n",
    "        \n",
    "    #Optimize variables based on loss\n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0]/output_length #return average loss across output sequence\n",
    "\n",
    "\n",
    "def test_model(input_seq_variable, encoder, decoder, cost_func, start_token, end_token, output_seq_variable =None, no_testing = False, max_length = 30):\n",
    "    #Same same but no weight updates/teacher forcing\n",
    "    \n",
    "    #1) Initialize encoder 1st hidden state and reset loss\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    loss = 0 \n",
    "    \n",
    "    #2) Pass sequence through encoder network\n",
    "    input_length = input_seq_variable.size()[0]\n",
    "    \n",
    "    for encoder_time_step in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_seq_variable[encoder_time_step], encoder_hidden)\n",
    "        \n",
    "    #3) Initialize decoder output\n",
    "\n",
    "    decoder_input = autograd.Variable(torch.LongTensor([[start_token]])) #remember dim is 1*1*-1\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = encoder_hidden \n",
    "    \n",
    "    #4) Repeat decoder output until end_token is reached (for real world application-otherwise for testing stop when eos is reached)\n",
    "    predicted_sequence = []\n",
    "    if(no_testing):\n",
    "        #Reproduce sentence\n",
    "        while(next_input_id != end_token):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            top_prob, top_index = decoder_output.data.topk(1)\n",
    "            next_input_id = top_index[0][0] #use this index as input from vocab\n",
    "            \n",
    "            decoder_input = autograd.Variable(torch.LongTensor([[next_input_id]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            predicted_sequence.append(top_index)\n",
    "        \n",
    "        return predicted_sequence\n",
    "           \n",
    "                \n",
    "    else:\n",
    "        #Testing based on output_length\n",
    "        output_length = output_seq_variable.size()[0]\n",
    "        for decoder_time_step in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            top_prob, top_index = decoder_output.data.topk(1)\n",
    "            next_input_id = top_index[0][0] #use this index as input from vocab\n",
    "\n",
    "            decoder_input = autograd.Variable(torch.LongTensor([[next_input_id]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            predicted_sequence.append(top_index)\n",
    "            \n",
    "            loss+= cost_func(decoder_output, output_seq_variable[decoder_time_step])\n",
    "            if(next_input_id == end_token):\n",
    "                '''should outputting an EOS when it is not be heavily punished by loss function? \n",
    "                However, that may lead to future longer sentences (as short sentences are punished).\n",
    "                Regardless, how can we implement that'''\n",
    "                break\n",
    "                \n",
    "        return loss.data[0]/output_length, predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nm = autograd.Variable(torch.randn(3,4))\\na = nn.Linear(4,5)\\nprint(a(m).size())\\nx = F.softmax(a(m), dim =1)\\nprint(x.data.topk(1))\\nx.data.topk(1)[1][0], x.data.topk(1)[1][0][0]\\n'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "m = autograd.Variable(torch.randn(3,4))\n",
    "a = nn.Linear(4,5)\n",
    "print(a(m).size())\n",
    "x = F.softmax(a(m), dim =1)\n",
    "print(x.data.topk(1))\n",
    "x.data.topk(1)[1][0], x.data.topk(1)[1][0][0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare training files: input--> output (where output is the same)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Load relevant data for training\n",
    "def get_dataset_info(file_name = \"laptop\", training_data_additional = False, get_inverse = False, lower = True):\n",
    "    \n",
    "    if(lower):\n",
    "        opt_string = \"lower\" #hardcoded\n",
    "    else:\n",
    "        opt_string = \"final\"\n",
    "        \n",
    "        \n",
    "    if(training_data_additional):\n",
    "        training_data = pickle.load(open(\"Final_data/{}_{}_additional_training_list.pickle\".format(file_name, opt_string)))\n",
    "    else:\n",
    "        training_data = pickle.load(open(\"Final_data/{}_{}_normal_training_list.pickle\".format(file_name, opt_string)))\n",
    "        \n",
    "    tag2id = pickle.load(open(\"Final_data/{}_{}_tag2id.pickle\".format(file_name, opt_string)))\n",
    "    \n",
    "    vocab2id = pickle.load(open(\"Final_data/{}_{}_vocab.pickle\".format(file_name, opt_string)))\n",
    "    \n",
    "    if(\"<START>\" in vocab2id.keys()): #assuming <START> and <END> always co-occur\n",
    "        start_token = vocab2id[\"<START\"]\n",
    "        end_token = vocab2id[\"<END>\"]\n",
    "    else:\n",
    "        start_token = vocab2id[\"<START>\"] = len(vocab2id)\n",
    "        end_token = vocab2id[\"<END>\"] = len(vocab2id)\n",
    "    \n",
    "    if(not get_inverse):\n",
    "        return training_data, tag2id, vocab2id, start_token, end_token\n",
    "    else:\n",
    "        id2vocab = {id_:token for token, id_ in vocab2id.items()}\n",
    "        id2tag = {id_:tag for tag, id_ in tag2id.items()}\n",
    "        \n",
    "        return training_data, tag2id, vocab2id, start_token, end_token, id2vocab, id2tag\n",
    "\n",
    "#Load pretrained embeddings function \n",
    "def load_embeddings(path_to_embeddings, word2id, embedding_dim = 25):\n",
    "    '''\n",
    "    Input: This only takes in w2v format. So convert all other embedding types/vectors to w2v format: \n",
    "    Output: Torch variable with embeddings only belonging to and indexed by word2id\n",
    "    \n",
    "    For glove:\n",
    "    Use python -m gensim.scripts.glove2word2vec -i <GloVe vector file> -o <Word2vec vector file>\n",
    "    as per https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "    \n",
    "    \n",
    "    W2V format is:\n",
    "    <Num vectors> <dimensionality>\n",
    "    <word1> <vector rep>\n",
    "    <word2> <vector rep>\n",
    "    ....and so on\n",
    "    '''\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path_to_embeddings, binary = False)\n",
    "    \n",
    "    \n",
    "    corpus_embeddings = np.zeros((len(word2id), embedding_dim))\n",
    "    #Should words not found in embedding file be random or zeros? \n",
    "    #Currently 0 and then they are trained anyway\n",
    "    '''Should we use a try/catch block here so that words that are not in vocab force an exception?\n",
    "    Or should we choose every word in vocab and see if it is in embedding vocab-> worst case |V|*|embedding_vocab|\n",
    "    Or vice versa, same worst case but early stopping \n",
    "    Or use key retrieval so all words in embedding vocab are checked if they belong to dictionary key\n",
    "    '''\n",
    "    for word in word2id.keys():\n",
    "        if word in word_vectors.vocab:\n",
    "            corpus_embeddings[word2id[word]] = np.array(word_vectors[word])\n",
    "        \n",
    "    return torch.from_numpy(corpus_embeddings).float()\n",
    "\n",
    "\n",
    "'''Training the simple encoder-decoder'''\n",
    "def form_indexed_sequence_variable(sequence, word_to_id, end_token):\n",
    "    #Input sequence is a list of word tokens\n",
    "    #Output sequence is a list of id tokens based on provided word to id mapping\n",
    "    input_seq = [word_to_id[token] for token in sequence] + [end_token]\n",
    "    input_seq_var = autograd.Variable(torch.LongTensor(input_seq)).view(-1,1) #make a 2 rank tensor\n",
    "    if(use_cuda):\n",
    "        return input_seq_var.cuda()\n",
    "    else:\n",
    "        return input_seq_var\n",
    "    \n",
    "def generate_training_sequence(training_dataset, word_to_id, end_token):\n",
    "    '''Generator function to yield training example \n",
    "    Not very useful in this case but for large datasets it can save memory if we pass the dataset by reference.\n",
    "    Use hpy (guppy for py2) to explore memory usage in the heap\n",
    "    '''\n",
    "    random.shuffle(training_dataset) \n",
    "    for training_example in training_dataset:\n",
    "        yield(form_indexed_sequence_variable(training_example[0], word_to_id, end_token), form_indexed_sequence_variable(training_example[1], word_to_id, end_token))\n",
    "    \n",
    "    \n",
    "def generate_testing_sequence(testing_dataset, word_to_id, end_token, testing_size = 50):\n",
    "    '''Generator function to yield testing example \n",
    "    Not very useful in this case but for large datasets it can save memory if we pass the dataset by reference.\n",
    "    Use hpy (guppy for py2) to explore memory usage in the heap\n",
    "    '''\n",
    "    random.shuffle(testing_dataset) \n",
    "    for training_example in testing_dataset[:testing_size]:\n",
    "        yield(form_indexed_sequence_variable(training_example[0], word_to_id, end_token), form_indexed_sequence_variable(training_example[1], word_to_id, end_token))\n",
    "        \n",
    "def generate_sentence_from_id(seq, id_to_word):\n",
    "    return map(lambda x: id_to_word[x], seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training script'''\n",
    "\n",
    "'''Training function\n",
    "Note: We use stochastic optimization (variables are updated after each iteration in a given epoch)\n",
    "'''    \n",
    "\n",
    "def trainIters(encoder, decoder, n_epochs, enc_learning_rate, dec_learning_rate, enc_optimizer, dec_optimizer, loss_function, start_token, end_token,  training_data, testing_data, vocab2id, testing_size = 50, save_when_lowest_loss = False):\n",
    "    global min_testing_loss \n",
    "    encoder_optimizer = enc_optimizer(encoder.parameters(), lr = enc_learning_rate)\n",
    "    decoder_optimizer = dec_optimizer(decoder.parameters(), lr = dec_learning_rate)\n",
    "    loss_function = loss_function\n",
    "    #\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"At epoch: {}\".format(epoch))\n",
    "        t1 = time.time()\n",
    "        training_gen = generate_training_sequence(training_data, vocab2id, end_token)\n",
    "        loss = 0\n",
    "        iters = 0\n",
    "        for input_seq, target_seq in training_gen:\n",
    "            loss += train_model(input_seq, target_seq, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_function, start_token, end_token)\n",
    "            if(iters%500==0):\n",
    "                tm = time.time()\n",
    "                print(\"At iteration {}, time taken: {}\".format(iters, tm-t1))\n",
    "            iters+=1\n",
    "        print(\"Training Loss for epoch: {}\".format(loss/iters))\n",
    "        t2 = time.time()\n",
    "        print(\"time for epoch: {}\".format(t2-t1))\n",
    "        \n",
    "        te_loss = 0 \n",
    "        te_iters = 0\n",
    "        testing_gen = generate_testing_sequence(testing_data, vocab2id, end_token, testing_size)\n",
    "        for input_seq, target_seq in testing_gen:\n",
    "            te_loss_temp, predicted_seq = test_model(input_seq, encoder, decoder, loss_function, start_token, end_token, target_seq)\n",
    "            te_loss += te_loss_temp\n",
    "            if(te_iters%100==0):\n",
    "                tm = time.time()\n",
    "                print(\"At iteration {}, time taken: {}\".format(te_iters, tm-t1))\n",
    "            te_iters+=1\n",
    "        testing_loss = te_loss/te_iters\n",
    "        print(\"Testing Loss for epoch: {}\".format(testing_loss))\n",
    "        t2 = time.time()\n",
    "        \n",
    "        if(save_when_lowest_loss):\n",
    "            if(testing_loss < min_testing_loss):\n",
    "                torch.save(encoder.state_dict(), \"Trained_models/Autoencoder_trained_models/glove50dims/best_enc_state.pth\")\n",
    "                torch.save(decoder.state_dict(), \"Trained_models/Autoencoder_trained_models/glove50dims/best_dec_state.pth\")\n",
    "                torch.save(encoder, \"Trained_models/Autoencoder_trained_models/glove50dims/best_enc.pth\")\n",
    "                torch.save(decoder, \"Trained_models/Autoencoder_trained_models/glove50dims/best_dec.pth\")\n",
    "                pickle.dump(testing_loss, open(\"Trained_models/Autoencoder_trained_models/glove50dims/min_te_loss.p\",'wb'))\n",
    "                min_testing_loss = testing_loss\n",
    "                print(\"Saved model for best testing loss\")\n",
    "       # print(\"time for epoch: {}\".format(t2-t1))\n",
    "        \n",
    "        \n",
    "#form_indexed_sequence_variable(laptop_data[0][0], laptop_vocab2id, end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_data, tag2id, laptop_vocab2id, start_token, end_token = get_dataset_info()\n",
    "vocab_len = len(laptop_vocab2id)\n",
    "num_total_examples = len(laptop_data)\n",
    "\n",
    "\n",
    "\n",
    "'''The below is for training an autoencoder model'''\n",
    "training_first_index = 0\n",
    "training_last_index = int(num_total_examples*0.7)\n",
    "testing_first_index = training_last_index + 1\n",
    "\n",
    "tokenized_sentences_list = map(lambda x: x[0], laptop_data)\n",
    "dataset_for_autoencoding = [(tokenized_sentence, tokenized_sentence) for tokenized_sentence in tokenized_sentences_list]\n",
    "training_data = dataset_for_autoencoding[:training_last_index+1] \n",
    "testing_data = dataset_for_autoencoding[testing_first_index:] \n",
    "\n",
    "assert len(training_data) + len(testing_data)  == len(dataset_for_autoencoding) #Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with lowest test loss of 3.15490326473\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "'''Model and hyperparameters'''\n",
    "\n",
    "already_exists = True\n",
    "pretrained_embeddings_needed = True\n",
    "\n",
    "if(pretrained_embeddings_needed):\n",
    "    pretrained_embedding_matrix_encoder = load_embeddings(\"./Final_data/embeddings/glove/glove50dimw2v_format.txt\", laptop_vocab2id, 50)\n",
    "    pretrained_embedding_matrix_decoder = pretrained_embedding_matrix_encoder #Only for this case, since output domain is same\n",
    " #model already exists  \n",
    "\n",
    "encoder_hidden_dims = 50\n",
    "encoder_embedding_dims = 50\n",
    "decoder_hidden_dims = 50\n",
    "decoder_embedding_dims = 50\n",
    "\n",
    "if(already_exists == True):\n",
    "    \n",
    "    encoder_path = \"Trained_models/Autoencoder_trained_models/glove50dims/encoder1_2state.pth\"\n",
    "    encoder1 = Encoder_GRU(vocab_len, encoder_hidden_dims, encoder_embedding_dims, True, pretrained_embedding_matrix_encoder)\n",
    "    encoder1.load_state_dict(torch.load(encoder_path))\n",
    "    \n",
    "    decoder_path = \"Trained_models/Autoencoder_trained_models/glove50dims/decoder1_2state.pth\"\n",
    "    decoder1 = Decoder_GRU(vocab_len, decoder_hidden_dims, decoder_embedding_dims, True, pretrained_embedding_matrix_decoder)\n",
    "    decoder1.load_state_dict(torch.load(decoder_path))\n",
    "    \n",
    "    min_testing_loss = pickle.load(open(\"Trained_models/Autoencoder_trained_models/glove50dims/min_te_loss.p\",'rb'))\n",
    "    print(\"Loaded model with lowest test loss of {}\".format(min_testing_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "else:\n",
    "    \n",
    "    encoder1 = Encoder_GRU(vocab_len, encoder_hidden_dims, encoder_embedding_dims, True, pretrained_embedding_matrix_encoder)\n",
    "    \n",
    "    \n",
    "    decoder1 = Decoder_GRU(vocab_len, decoder_hidden_dims, decoder_embedding_dims, True, pretrained_embedding_matrix_decoder)\n",
    "    min_testing_loss = 1e3\n",
    "\n",
    "if(use_cuda):\n",
    "    encoder1 = encoder1.cuda()\n",
    "    decoder1 = decoder1.cuda()\n",
    "    \n",
    "enc_learning_rate = 0.01\n",
    "enc_optimizer = optim.SGD\n",
    "dec_learning_rate = 0.01\n",
    "dec_optimizer = optim.SGD\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 0\n",
      "At iteration 0, time taken: 0.0375430583954\n",
      "At iteration 500, time taken: 17.9576630592\n",
      "At iteration 1000, time taken: 34.0831320286\n",
      "At iteration 1500, time taken: 51.5019700527\n",
      "At iteration 2000, time taken: 70.5592730045\n",
      "Training Loss for epoch: 2.97441833938\n",
      "time for epoch: 75.095277071\n",
      "At iteration 0, time taken: 75.10455513\n",
      "At iteration 100, time taken: 76.083204031\n",
      "At iteration 200, time taken: 77.0557579994\n",
      "At iteration 300, time taken: 77.9593000412\n",
      "At iteration 400, time taken: 78.906648159\n",
      "At iteration 500, time taken: 79.8624432087\n",
      "At iteration 600, time taken: 80.9124979973\n",
      "At iteration 700, time taken: 81.7745850086\n",
      "At iteration 800, time taken: 82.6545710564\n",
      "At iteration 900, time taken: 83.6630151272\n",
      "Testing Loss for epoch: 4.03362233307\n",
      "At epoch: 1\n",
      "At iteration 0, time taken: 0.0176389217377\n",
      "At iteration 500, time taken: 21.3936100006\n",
      "At iteration 1000, time taken: 47.0536820889\n",
      "At iteration 1500, time taken: 62.7475450039\n",
      "At iteration 2000, time taken: 82.5864760876\n",
      "Training Loss for epoch: 2.88897775737\n",
      "time for epoch: 87.9853060246\n",
      "At iteration 0, time taken: 88.0032360554\n",
      "At iteration 100, time taken: 89.4736671448\n",
      "At iteration 200, time taken: 91.1600949764\n",
      "At iteration 300, time taken: 92.638436079\n",
      "At iteration 400, time taken: 93.973582983\n",
      "At iteration 500, time taken: 95.0616381168\n",
      "At iteration 600, time taken: 96.1726341248\n",
      "At iteration 700, time taken: 97.1822099686\n",
      "At iteration 800, time taken: 98.1485440731\n",
      "At iteration 900, time taken: 99.3616359234\n",
      "Testing Loss for epoch: 4.72425291808\n",
      "At epoch: 2\n",
      "At iteration 0, time taken: 0.0344839096069\n",
      "At iteration 500, time taken: 31.3470170498\n",
      "At iteration 1000, time taken: 50.9076039791\n",
      "At iteration 1500, time taken: 69.4915599823\n",
      "At iteration 2000, time taken: 88.6314890385\n",
      "Training Loss for epoch: 2.89910322037\n",
      "time for epoch: 97.2585380077\n",
      "At iteration 0, time taken: 97.2716019154\n",
      "At iteration 100, time taken: 98.6540958881\n",
      "At iteration 200, time taken: 100.144710064\n",
      "At iteration 300, time taken: 101.879601955\n",
      "At iteration 400, time taken: 103.770714045\n",
      "At iteration 500, time taken: 105.42667985\n",
      "At iteration 600, time taken: 106.757340908\n",
      "At iteration 700, time taken: 108.070502043\n",
      "At iteration 800, time taken: 109.520289898\n",
      "At iteration 900, time taken: 110.914787054\n",
      "Testing Loss for epoch: 4.6440640992\n",
      "At epoch: 3\n",
      "At iteration 0, time taken: 0.052041053772\n",
      "At iteration 500, time taken: 21.6044909954\n",
      "At iteration 1000, time taken: 38.0374629498\n",
      "At iteration 1500, time taken: 54.6313159466\n",
      "At iteration 2000, time taken: 72.7019078732\n",
      "Training Loss for epoch: 2.89740412871\n",
      "time for epoch: 78.0295438766\n",
      "At iteration 0, time taken: 78.037307024\n",
      "At iteration 100, time taken: 79.1349408627\n",
      "At iteration 200, time taken: 80.2185468674\n",
      "At iteration 300, time taken: 81.1717190742\n",
      "At iteration 400, time taken: 82.2198820114\n",
      "At iteration 500, time taken: 83.3342590332\n",
      "At iteration 600, time taken: 84.2861509323\n",
      "At iteration 700, time taken: 85.3222720623\n",
      "At iteration 800, time taken: 86.3424379826\n",
      "At iteration 900, time taken: 87.5186789036\n",
      "Testing Loss for epoch: 4.070662683\n",
      "At epoch: 4\n",
      "At iteration 0, time taken: 0.0264990329742\n",
      "At iteration 500, time taken: 18.7761518955\n",
      "At iteration 1000, time taken: 37.9238159657\n",
      "At iteration 1500, time taken: 56.6035318375\n",
      "At iteration 2000, time taken: 76.1728420258\n",
      "Training Loss for epoch: 2.92142186346\n",
      "time for epoch: 81.4850418568\n",
      "At iteration 0, time taken: 81.4936218262\n",
      "At iteration 100, time taken: 82.4367368221\n",
      "At iteration 200, time taken: 83.3322119713\n",
      "At iteration 300, time taken: 84.2823219299\n",
      "At iteration 400, time taken: 85.2070510387\n",
      "At iteration 500, time taken: 86.1185250282\n",
      "At iteration 600, time taken: 87.1024708748\n",
      "At iteration 700, time taken: 87.976295948\n",
      "At iteration 800, time taken: 88.9417178631\n",
      "At iteration 900, time taken: 89.8367259502\n",
      "Testing Loss for epoch: 3.73197248809\n",
      "At epoch: 5\n",
      "At iteration 0, time taken: 0.0559468269348\n",
      "At iteration 500, time taken: 16.2576918602\n",
      "At iteration 1000, time taken: 31.9531519413\n",
      "At iteration 1500, time taken: 48.6533927917\n",
      "At iteration 2000, time taken: 65.7202789783\n",
      "Training Loss for epoch: 2.95793564897\n",
      "time for epoch: 70.3699820042\n",
      "At iteration 0, time taken: 70.3822088242\n",
      "At iteration 100, time taken: 71.2802298069\n",
      "At iteration 200, time taken: 72.0877299309\n",
      "At iteration 300, time taken: 73.0098137856\n",
      "At iteration 400, time taken: 73.9140088558\n",
      "At iteration 500, time taken: 74.7525088787\n",
      "At iteration 600, time taken: 75.6515519619\n",
      "At iteration 700, time taken: 76.5378389359\n",
      "At iteration 800, time taken: 77.4429838657\n",
      "At iteration 900, time taken: 78.3948688507\n",
      "Testing Loss for epoch: 3.95219622842\n",
      "At epoch: 6\n",
      "At iteration 0, time taken: 0.0233080387115\n",
      "At iteration 500, time taken: 16.2099380493\n",
      "At iteration 1000, time taken: 32.4084129333\n",
      "At iteration 1500, time taken: 49.4667479992\n",
      "At iteration 2000, time taken: 69.9799730778\n",
      "Training Loss for epoch: 2.89935073245\n",
      "time for epoch: 74.700963974\n",
      "At iteration 0, time taken: 74.7053380013\n",
      "At iteration 100, time taken: 75.5117731094\n",
      "At iteration 200, time taken: 76.2788119316\n",
      "At iteration 300, time taken: 77.0282781124\n",
      "At iteration 400, time taken: 77.9328980446\n",
      "At iteration 500, time taken: 78.7114260197\n",
      "At iteration 600, time taken: 79.5422050953\n",
      "At iteration 700, time taken: 80.3458850384\n",
      "At iteration 800, time taken: 81.1248130798\n",
      "At iteration 900, time taken: 82.0019259453\n",
      "Testing Loss for epoch: 3.28397102808\n",
      "Saved model for best testing loss\n",
      "At epoch: 7\n",
      "At iteration 0, time taken: 0.0351738929749\n",
      "At iteration 500, time taken: 16.6367619038\n",
      "At iteration 1000, time taken: 32.4525039196\n",
      "At iteration 1500, time taken: 47.7980618477\n",
      "At iteration 2000, time taken: 63.9681499004\n",
      "Training Loss for epoch: 2.91334004936\n",
      "time for epoch: 68.358618021\n",
      "At iteration 0, time taken: 68.3669068813\n",
      "At iteration 100, time taken: 69.407766819\n",
      "At iteration 200, time taken: 70.3679249287\n",
      "At iteration 300, time taken: 71.2885458469\n",
      "At iteration 400, time taken: 72.2922768593\n",
      "At iteration 500, time taken: 73.2106399536\n",
      "At iteration 600, time taken: 74.2475068569\n",
      "At iteration 700, time taken: 75.2371599674\n",
      "At iteration 800, time taken: 76.1527528763\n",
      "At iteration 900, time taken: 77.1368489265\n",
      "Testing Loss for epoch: 4.50788169581\n",
      "At epoch: 8\n",
      "At iteration 0, time taken: 0.028697013855\n",
      "At iteration 500, time taken: 15.6661450863\n",
      "At iteration 1000, time taken: 30.9190499783\n",
      "At iteration 1500, time taken: 46.4277141094\n",
      "At iteration 2000, time taken: 63.2772932053\n",
      "Training Loss for epoch: 2.86486307874\n",
      "time for epoch: 67.4196891785\n",
      "At iteration 0, time taken: 67.4238450527\n",
      "At iteration 100, time taken: 68.2498171329\n",
      "At iteration 200, time taken: 69.3044731617\n",
      "At iteration 300, time taken: 70.1802561283\n",
      "At iteration 400, time taken: 71.0641980171\n",
      "At iteration 500, time taken: 71.9823410511\n",
      "At iteration 600, time taken: 72.8621151447\n",
      "At iteration 700, time taken: 73.7637121677\n",
      "At iteration 800, time taken: 74.6076750755\n",
      "At iteration 900, time taken: 75.4608690739\n",
      "Testing Loss for epoch: 3.86370225831\n",
      "At epoch: 9\n",
      "At iteration 0, time taken: 0.0224161148071\n",
      "At iteration 500, time taken: 16.6048769951\n",
      "At iteration 1000, time taken: 32.352230072\n",
      "At iteration 1500, time taken: 48.3313751221\n",
      "At iteration 2000, time taken: 63.6353120804\n",
      "Training Loss for epoch: 2.90116143264\n",
      "time for epoch: 68.3569800854\n",
      "At iteration 0, time taken: 68.3638470173\n",
      "At iteration 100, time taken: 69.3177771568\n",
      "At iteration 200, time taken: 70.1926691532\n",
      "At iteration 300, time taken: 71.2054440975\n",
      "At iteration 400, time taken: 72.2876150608\n",
      "At iteration 500, time taken: 73.2522599697\n",
      "At iteration 600, time taken: 74.1776959896\n",
      "At iteration 700, time taken: 75.178935051\n",
      "At iteration 800, time taken: 76.1548841\n",
      "At iteration 900, time taken: 77.1651690006\n",
      "Testing Loss for epoch: 4.26170590711\n",
      "At epoch: 10\n",
      "At iteration 0, time taken: 0.0168960094452\n",
      "At iteration 500, time taken: 15.7619721889\n",
      "At iteration 1000, time taken: 31.9822590351\n",
      "At iteration 1500, time taken: 47.2397751808\n",
      "At iteration 2000, time taken: 63.2315371037\n",
      "Training Loss for epoch: 2.8347901595\n",
      "time for epoch: 67.7219262123\n",
      "At iteration 0, time taken: 67.7305121422\n",
      "At iteration 100, time taken: 68.5571010113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 200, time taken: 69.5505051613\n",
      "At iteration 300, time taken: 70.5095582008\n",
      "At iteration 400, time taken: 71.4240880013\n",
      "At iteration 500, time taken: 72.3052170277\n",
      "At iteration 600, time taken: 73.4944601059\n",
      "At iteration 700, time taken: 74.4661900997\n",
      "At iteration 800, time taken: 75.3854341507\n",
      "At iteration 900, time taken: 76.4065320492\n",
      "Testing Loss for epoch: 4.23263543267\n",
      "At epoch: 11\n",
      "At iteration 0, time taken: 0.0288090705872\n",
      "At iteration 500, time taken: 15.9946050644\n",
      "At iteration 1000, time taken: 31.9137830734\n",
      "At iteration 1500, time taken: 49.1142990589\n",
      "At iteration 2000, time taken: 64.9301729202\n",
      "Training Loss for epoch: 2.92504424282\n",
      "time for epoch: 69.1401059628\n",
      "At iteration 0, time taken: 69.1517670155\n",
      "At iteration 100, time taken: 69.9394369125\n",
      "At iteration 200, time taken: 70.7138791084\n",
      "At iteration 300, time taken: 71.5077610016\n",
      "At iteration 400, time taken: 72.3225600719\n",
      "At iteration 500, time taken: 73.1056659222\n",
      "At iteration 600, time taken: 73.925647974\n",
      "At iteration 700, time taken: 74.8133289814\n",
      "At iteration 800, time taken: 75.5870249271\n",
      "At iteration 900, time taken: 76.383341074\n",
      "Testing Loss for epoch: 3.32668683285\n",
      "At epoch: 12\n",
      "At iteration 0, time taken: 0.0396981239319\n",
      "At iteration 500, time taken: 16.5423130989\n",
      "At iteration 1000, time taken: 32.6892490387\n",
      "At iteration 1500, time taken: 48.4609451294\n",
      "At iteration 2000, time taken: 64.254940033\n",
      "Training Loss for epoch: 2.88221122674\n",
      "time for epoch: 68.6340949535\n",
      "At iteration 0, time taken: 68.638258934\n",
      "At iteration 100, time taken: 69.5431330204\n",
      "At iteration 200, time taken: 70.3745939732\n",
      "At iteration 300, time taken: 71.2558660507\n",
      "At iteration 400, time taken: 72.1950080395\n",
      "At iteration 500, time taken: 73.0696721077\n",
      "At iteration 600, time taken: 74.0604169369\n",
      "At iteration 700, time taken: 74.9731669426\n",
      "At iteration 800, time taken: 75.9084479809\n",
      "At iteration 900, time taken: 76.7860291004\n",
      "Testing Loss for epoch: 4.02950660446\n",
      "At epoch: 13\n",
      "At iteration 0, time taken: 0.0186049938202\n",
      "At iteration 500, time taken: 49.1179661751\n",
      "At iteration 1000, time taken: 65.3972940445\n",
      "At iteration 1500, time taken: 81.6804270744\n",
      "At iteration 2000, time taken: 98.2246701717\n",
      "Training Loss for epoch: 2.8566511409\n",
      "time for epoch: 102.759683132\n",
      "At iteration 0, time taken: 102.769435167\n",
      "At iteration 100, time taken: 103.679224968\n",
      "At iteration 200, time taken: 104.630007982\n",
      "At iteration 300, time taken: 105.609065056\n",
      "At iteration 400, time taken: 106.579656124\n",
      "At iteration 500, time taken: 107.641767979\n",
      "At iteration 600, time taken: 108.556370974\n",
      "At iteration 700, time taken: 109.441874027\n",
      "At iteration 800, time taken: 110.401045084\n",
      "At iteration 900, time taken: 111.337548971\n",
      "Testing Loss for epoch: 4.36405395426\n",
      "At epoch: 14\n",
      "At iteration 0, time taken: 0.0143809318542\n",
      "At iteration 500, time taken: 16.2964987755\n",
      "At iteration 1000, time taken: 32.4850468636\n",
      "At iteration 1500, time taken: 49.3950238228\n",
      "At iteration 2000, time taken: 65.6745319366\n",
      "Training Loss for epoch: 2.89565030976\n",
      "time for epoch: 69.7038009167\n",
      "At iteration 0, time taken: 69.7121818066\n",
      "At iteration 100, time taken: 70.5437698364\n",
      "At iteration 200, time taken: 71.4833869934\n",
      "At iteration 300, time taken: 72.2828757763\n",
      "At iteration 400, time taken: 73.2816238403\n",
      "At iteration 500, time taken: 74.143217802\n",
      "At iteration 600, time taken: 75.0075919628\n",
      "At iteration 700, time taken: 75.8505618572\n",
      "At iteration 800, time taken: 76.6726708412\n",
      "At iteration 900, time taken: 77.5550088882\n",
      "Testing Loss for epoch: 3.89003099236\n",
      "At epoch: 15\n",
      "At iteration 0, time taken: 0.0309889316559\n",
      "At iteration 500, time taken: 16.3264329433\n",
      "At iteration 1000, time taken: 32.8611469269\n",
      "At iteration 1500, time taken: 48.7797439098\n",
      "At iteration 2000, time taken: 64.9398889542\n",
      "Training Loss for epoch: 2.88839026976\n",
      "time for epoch: 68.9834330082\n",
      "At iteration 0, time taken: 68.9887480736\n",
      "At iteration 100, time taken: 69.810559988\n",
      "At iteration 200, time taken: 70.6125068665\n",
      "At iteration 300, time taken: 71.4068069458\n",
      "At iteration 400, time taken: 72.2158620358\n",
      "At iteration 500, time taken: 73.0634119511\n",
      "At iteration 600, time taken: 73.9291419983\n",
      "At iteration 700, time taken: 74.6629140377\n",
      "At iteration 800, time taken: 75.4720439911\n",
      "At iteration 900, time taken: 76.3149938583\n",
      "Testing Loss for epoch: 3.57863784439\n",
      "At epoch: 16\n",
      "At iteration 0, time taken: 0.039046049118\n",
      "At iteration 500, time taken: 16.0730800629\n",
      "At iteration 1000, time taken: 32.0048179626\n",
      "At iteration 1500, time taken: 47.948071003\n",
      "At iteration 2000, time taken: 64.1129570007\n",
      "Training Loss for epoch: 2.84506414685\n",
      "time for epoch: 68.5897619724\n",
      "At iteration 0, time taken: 68.5985469818\n",
      "At iteration 100, time taken: 69.6422519684\n",
      "At iteration 200, time taken: 70.6625759602\n",
      "At iteration 300, time taken: 71.5765328407\n",
      "At iteration 400, time taken: 72.5852999687\n",
      "At iteration 500, time taken: 73.5504040718\n",
      "At iteration 600, time taken: 74.4740178585\n",
      "At iteration 700, time taken: 75.4338960648\n",
      "At iteration 800, time taken: 76.2835440636\n",
      "At iteration 900, time taken: 77.2314529419\n",
      "Testing Loss for epoch: 4.52967781126\n",
      "At epoch: 17\n",
      "At iteration 0, time taken: 0.0335400104523\n",
      "At iteration 500, time taken: 15.9501998425\n",
      "At iteration 1000, time taken: 31.2533438206\n",
      "At iteration 1500, time taken: 47.4814858437\n",
      "At iteration 2000, time taken: 64.3952038288\n",
      "Training Loss for epoch: 2.87340434602\n",
      "time for epoch: 68.7207238674\n",
      "At iteration 0, time taken: 68.7294669151\n",
      "At iteration 100, time taken: 69.5751359463\n",
      "At iteration 200, time taken: 70.3953108788\n",
      "At iteration 300, time taken: 71.251033783\n",
      "At iteration 400, time taken: 72.0242938995\n",
      "At iteration 500, time taken: 72.8362498283\n",
      "At iteration 600, time taken: 73.6263039112\n",
      "At iteration 700, time taken: 74.4092957973\n",
      "At iteration 800, time taken: 75.3148288727\n",
      "At iteration 900, time taken: 76.1837899685\n",
      "Testing Loss for epoch: 3.62850798894\n",
      "At epoch: 18\n",
      "At iteration 0, time taken: 0.0165240764618\n",
      "At iteration 500, time taken: 16.8004789352\n",
      "At iteration 1000, time taken: 32.8316559792\n",
      "At iteration 1500, time taken: 49.2628560066\n",
      "At iteration 2000, time taken: 64.9524068832\n",
      "Training Loss for epoch: 2.83317797415\n",
      "time for epoch: 69.2969219685\n",
      "At iteration 0, time taken: 69.3047599792\n",
      "At iteration 100, time taken: 70.1621088982\n",
      "At iteration 200, time taken: 71.0248010159\n",
      "At iteration 300, time taken: 71.9244830608\n",
      "At iteration 400, time taken: 72.9075090885\n",
      "At iteration 500, time taken: 73.8086090088\n",
      "At iteration 600, time taken: 74.6427710056\n",
      "At iteration 700, time taken: 75.5656321049\n",
      "At iteration 800, time taken: 76.5561800003\n",
      "At iteration 900, time taken: 77.5385990143\n",
      "Testing Loss for epoch: 4.05101671422\n",
      "At epoch: 19\n",
      "At iteration 0, time taken: 0.0302770137787\n",
      "At iteration 500, time taken: 16.1466040611\n",
      "At iteration 1000, time taken: 32.8283319473\n",
      "At iteration 1500, time taken: 48.5801010132\n",
      "At iteration 2000, time taken: 64.8770060539\n",
      "Training Loss for epoch: 2.83704030333\n",
      "time for epoch: 68.8036940098\n",
      "At iteration 0, time taken: 68.8089940548\n",
      "At iteration 100, time taken: 69.6248290539\n",
      "At iteration 200, time taken: 70.471955061\n",
      "At iteration 300, time taken: 71.2701809406\n",
      "At iteration 400, time taken: 72.1906778812\n",
      "At iteration 500, time taken: 73.1112589836\n",
      "At iteration 600, time taken: 74.023927927\n",
      "At iteration 700, time taken: 74.967674017\n",
      "At iteration 800, time taken: 75.9456899166\n",
      "At iteration 900, time taken: 76.8424539566\n",
      "Testing Loss for epoch: 4.14640377579\n",
      "At epoch: 20\n",
      "At iteration 0, time taken: 0.015625\n",
      "At iteration 500, time taken: 16.7154769897\n",
      "At iteration 1000, time taken: 33.2135760784\n",
      "At iteration 1500, time taken: 49.5679311752\n",
      "At iteration 2000, time taken: 65.4285399914\n",
      "Training Loss for epoch: 2.9049203697\n",
      "time for epoch: 69.6987621784\n",
      "At iteration 0, time taken: 69.7155611515\n",
      "At iteration 100, time taken: 70.5077362061\n",
      "At iteration 200, time taken: 71.26947999\n",
      "At iteration 300, time taken: 72.0152850151\n",
      "At iteration 400, time taken: 72.806442976\n",
      "At iteration 500, time taken: 73.5450050831\n",
      "At iteration 600, time taken: 74.269190073\n",
      "At iteration 700, time taken: 75.0094912052\n",
      "At iteration 800, time taken: 75.8150341511\n",
      "At iteration 900, time taken: 76.5830440521\n",
      "Testing Loss for epoch: 3.15490326473\n",
      "Saved model for best testing loss\n",
      "At epoch: 21\n",
      "At iteration 0, time taken: 0.0205090045929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 500, time taken: 16.0615341663\n",
      "At iteration 1000, time taken: 31.9992961884\n",
      "At iteration 1500, time taken: 47.7398610115\n",
      "At iteration 2000, time taken: 64.7002971172\n",
      "Training Loss for epoch: 2.79825875066\n",
      "time for epoch: 69.187415123\n",
      "At iteration 0, time taken: 69.195374012\n",
      "At iteration 100, time taken: 70.434595108\n",
      "At iteration 200, time taken: 71.4868819714\n",
      "At iteration 300, time taken: 72.5125031471\n",
      "At iteration 400, time taken: 73.4985041618\n",
      "At iteration 500, time taken: 74.554197073\n",
      "At iteration 600, time taken: 75.4909260273\n",
      "At iteration 700, time taken: 76.457321167\n",
      "At iteration 800, time taken: 77.3724889755\n",
      "At iteration 900, time taken: 78.3004620075\n",
      "Testing Loss for epoch: 4.84196761776\n",
      "At epoch: 22\n",
      "At iteration 0, time taken: 0.0322380065918\n",
      "At iteration 500, time taken: 15.8388741016\n",
      "At iteration 1000, time taken: 32.1600050926\n",
      "At iteration 1500, time taken: 47.9942359924\n",
      "At iteration 2000, time taken: 64.5558459759\n",
      "Training Loss for epoch: 2.80621796711\n",
      "time for epoch: 69.0742659569\n",
      "At iteration 0, time taken: 69.0911300182\n",
      "At iteration 100, time taken: 70.0538671017\n",
      "At iteration 200, time taken: 71.0164921284\n",
      "At iteration 300, time taken: 71.8786780834\n",
      "At iteration 400, time taken: 72.8707540035\n",
      "At iteration 500, time taken: 73.888625145\n",
      "At iteration 600, time taken: 74.883092165\n",
      "At iteration 700, time taken: 75.8102440834\n",
      "At iteration 800, time taken: 76.8891611099\n",
      "At iteration 900, time taken: 78.0586380959\n",
      "Testing Loss for epoch: 4.63733472265\n",
      "At epoch: 23\n",
      "At iteration 0, time taken: 0.0260751247406\n",
      "At iteration 500, time taken: 16.9365501404\n",
      "At iteration 1000, time taken: 32.5410320759\n",
      "At iteration 1500, time taken: 48.9499971867\n",
      "At iteration 2000, time taken: 64.8000211716\n",
      "Training Loss for epoch: 2.84387804278\n",
      "time for epoch: 68.9422051907\n",
      "At iteration 0, time taken: 68.949775219\n",
      "At iteration 100, time taken: 69.8574211597\n",
      "At iteration 200, time taken: 70.7052180767\n",
      "At iteration 300, time taken: 71.7586121559\n",
      "At iteration 400, time taken: 72.7536962032\n",
      "At iteration 500, time taken: 73.7475850582\n",
      "At iteration 600, time taken: 74.7189650536\n",
      "At iteration 700, time taken: 75.6531610489\n",
      "At iteration 800, time taken: 76.5456380844\n",
      "At iteration 900, time taken: 77.4305241108\n",
      "Testing Loss for epoch: 4.26196217221\n",
      "At epoch: 24\n",
      "At iteration 0, time taken: 0.0524899959564\n",
      "At iteration 500, time taken: 16.4650700092\n",
      "At iteration 1000, time taken: 32.615445137\n",
      "At iteration 1500, time taken: 48.8519051075\n",
      "At iteration 2000, time taken: 64.5957050323\n",
      "Training Loss for epoch: 2.82510798165\n",
      "time for epoch: 68.8636341095\n",
      "At iteration 0, time taken: 68.8803751469\n",
      "At iteration 100, time taken: 69.7567169666\n",
      "At iteration 200, time taken: 70.5523920059\n",
      "At iteration 300, time taken: 71.3890521526\n",
      "At iteration 400, time taken: 72.243035078\n",
      "At iteration 500, time taken: 73.1347579956\n",
      "At iteration 600, time taken: 74.0045080185\n",
      "At iteration 700, time taken: 74.9550900459\n",
      "At iteration 800, time taken: 75.9094409943\n",
      "At iteration 900, time taken: 76.8450009823\n",
      "Testing Loss for epoch: 3.94903464967\n"
     ]
    }
   ],
   "source": [
    "#trainIters(encoder1, decoder1, 25, enc_learning_rate, dec_learning_rate, enc_optimizer, dec_optimizer, loss_function, start_token, end_token, training_data, testing_data, laptop_vocab2id, len(testing_data), save_when_lowest_loss = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another question should the autoencoder be trained jointly with future objective function? \n",
    "#That would mean that it is encoding across all sentences such that the encoding is useful \n",
    "#Is it better to not use just an encoder which is trained jointly with missing words such that the representations obtained are different for each  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2) Stimulate missing words and HOW TO MAKE AN ARCHITECTURE TO CAPTURE MISSING WORD(S) CONTRIBUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word interactions is the next step-> How do we figure the best interaction network\n",
    "#### Tree tracing of words seemed a distant possibility-> it captures the interaction as well as the contribution--> we want to guess where the most important interactions/contributions occur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dataset_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3526cbefa641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlaptop_expanded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_additional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_dataset_info' is not defined"
     ]
    }
   ],
   "source": [
    "laptop_expanded_data = get_dataset_info(training_data_additional=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'charge',\n",
       "  'it',\n",
       "  'at',\n",
       "  'night',\n",
       "  'and',\n",
       "  'skip',\n",
       "  'taking',\n",
       "  'the',\n",
       "  'cord',\n",
       "  'with',\n",
       "  'me',\n",
       "  'because',\n",
       "  'of',\n",
       "  'the',\n",
       "  'good',\n",
       "  'battery',\n",
       "  'life'],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 3, 1, 2],\n",
       " [('i', 'PRON', 'PRP', 'nsubj'),\n",
       "  ('charge', 'VERB', 'VBP', 'ROOT'),\n",
       "  ('it', 'PRON', 'PRP', 'dobj'),\n",
       "  ('at', 'ADP', 'IN', 'prep'),\n",
       "  ('night', 'NOUN', 'NN', 'pobj'),\n",
       "  ('and', 'CCONJ', 'CC', 'cc'),\n",
       "  ('skip', 'VERB', 'VB', 'conj'),\n",
       "  ('taking', 'VERB', 'VBG', 'xcomp'),\n",
       "  ('the', 'DET', 'DT', 'det'),\n",
       "  ('cord', 'NOUN', 'NN', 'dobj'),\n",
       "  ('with', 'ADP', 'IN', 'prep'),\n",
       "  ('me', 'PRON', 'PRP', 'pobj'),\n",
       "  ('because', 'ADP', 'IN', 'prep'),\n",
       "  ('of', 'ADP', 'IN', 'pcomp'),\n",
       "  ('the', 'DET', 'DT', 'det'),\n",
       "  ('good', 'ADJ', 'JJ', 'amod'),\n",
       "  ('battery', 'NOUN', 'NN', 'compound'),\n",
       "  ('life', 'NOUN', 'NN', 'pobj')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_expanded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-829f6fc736b6>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-829f6fc736b6>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    if\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def filter_sentence(tokenized_sentence_tuple, remove_by_seq_tag = [tag2id['BA']], remove_by_pos_tag = ['NOUN'], random_ignore =0.8):\n",
    "    #Can do batch processing? \n",
    "    #Random ignore is used when words may not \n",
    "    filtered_tokenized_sentence = []\n",
    "    tokenized_sentence = tokenized_sentence_tuple[0]\n",
    "    absa_tags = tokenized_sentence_tuple[1]\n",
    "    pos_tags = tokenized_sentence_tuple[2]\n",
    "    \n",
    "    filtered_sentences = [] #this contains the list of output filtered sentences\n",
    "    sure_skip_indices = []\n",
    "    soso_skip_indices = []\n",
    "    random_ignore = [] # randomly drop some non important words from filtered lists->do later\n",
    "    for index, token in enumerate(tokenized_sentence_tuple):\n",
    "        #for each token check if \n",
    "        if absa_tags[index] in remove_by_seq_tag:\n",
    "            skip_indices.append([index])\n",
    "        elif pos_tags[index] in remove_by_pos_tag:\n",
    "            soso_skip_indices.append([index])\n",
    "    \n",
    "    #\n",
    "    final_indices = sure_skip_indices + random.choice(soso_skip_indices)     # closest_dist(sure_skipsoso_skip_indices)\n",
    "        \n",
    "    #Make sure skip sentences and then just do a permutation of nearby words for other candidates-> choose some randomly\n",
    "    #+ Choose some randomly from sosoindices to remove\n",
    "    #Label whichever is a 1 \n",
    "    \n",
    "\n",
    "laptop_expanded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "choice() takes exactly 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-37e0daccc1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: choice() takes exactly 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "random.choice([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'laptop_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56225d3ef290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlaptop_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'laptop_data' is not defined"
     ]
    }
   ],
   "source": [
    "laptop_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 5),\n",
       " ('is', 5),\n",
       " ('of', 5),\n",
       " ('high', 3),\n",
       " ('quality', 1),\n",
       " ('has', 5),\n",
       " ('a', 5),\n",
       " ('killer', 5),\n",
       " ('gui', 5),\n",
       " ('is', 5),\n",
       " ('extremely', 5),\n",
       " ('stable', 3),\n",
       " ('is', 5),\n",
       " ('highly', 5),\n",
       " ('expandable', 3),\n",
       " ('is', 5),\n",
       " ('bundled', 5),\n",
       " ('with', 5),\n",
       " ('lots', 5),\n",
       " ('of', 5),\n",
       " ('very', 5),\n",
       " ('good', 3),\n",
       " ('applications', 5),\n",
       " ('is', 5),\n",
       " ('easy', 3),\n",
       " ('to', 5),\n",
       " ('use', 5),\n",
       " ('and', 5),\n",
       " ('is', 5),\n",
       " ('absolutely', 5),\n",
       " ('gorgeous', 3)]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Stimulating missing words\n",
    "#The ball was quite round and its feeling on kicking was awesome. \n",
    "#zip(laptop_data[20][0], laptop_data[5][1])#, ' '.join(map(lambda x: str(x),laptop_data[5][1]))\n",
    "laptop_data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the tech guy then said the service center does not do 1to1 exchange and i have to direct my concern to the sales team which is the retail shop which i bought my netbook from',\n",
       " [5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(laptop_data[2][0]), laptop_data[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next step: Generate candidates for removal (missing words)\n",
    "\n",
    "def generate_candidate_missing_words_and_labels():\n",
    "    '''Takes input sequence: removes aspect and opinion words jo'''\n",
    "    '''Given a sentence, get dependency parser and tags\n",
    "    Remove certain words based on rules? or random noise-> \n",
    "    Train jointly with encoder?\n",
    "    \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def generate_noisy_input():\n",
    "    None\n",
    "    \n",
    "    \n",
    "def best_fit_missing_word_rep():\n",
    "    '''Make hash embeddings for all words--> assign to buckets depending on number\n",
    "    Learn buckets for all possible variations\n",
    "    Try to fit each bucket rep into missing phrase and see which fits best 3\n",
    "    '''\n",
    "    None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
