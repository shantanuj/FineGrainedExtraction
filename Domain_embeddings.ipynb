{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Q: Why do we need pretrained embeddings?\n",
    "\n",
    "General embeddings\n",
    "The purpose of embeddings is to be able to represent words with information we deem useful for our task. This is vaguely the definition of a feature for a model. The current trending mechanism to represent word information in deep learning is by being able to encode the usage of a word through its neighbours/context. These context specific features can be derived through tuned matrix transformations, ie, a neural network. Thus, from a neural network point of view, for each word we try and obtain a representation/vector that when transformed to the vocab space (softmax layer) results in high activations for mostly co-occuring words. \n",
    "\n",
    "These embeddings can be used in multiple ways for a deep learning text specific task. They can be tuned further based on the task (this results in a lot more parameters), or set as fixed. Give rep to a model and ask it to based on this, capture other info. Good detailed overview  with other techniques: https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795) \n",
    "\n",
    "2nd extension: Word senses\n",
    "Consider the sentences:\n",
    "S1: \"The stock price took a hit during recession\"\n",
    "S2: \"He hit the ball for a six\" \n",
    "The word hit has a different meaning/contribution in the sentence. This varying usage of a word is termed as a word exhibiting different senses, and something the vanilla word embedding will not capture.\n",
    "A simple method to capture different senses is to associate the POS tag with the word when computing the embedding. Hence, hit|NOUN and hit|VERB will have two different embeddings. \n",
    "\n",
    "3rd extension: Domain extension-Domain linkers, etc\n",
    "Now consider the scenario for our problem wherein we train a deep learning model to detect aspect and opinion terms with its input features as our pretrained word embeddings. The resultant trained model, based on its architecture, learns the transformation matrices (which may be used to derive additional variables for computation- ex. attention) to transform the data into a 'latent/middle ground' space.\n",
    "Let's break down the computation and training process of a BiLSTM-CRF model:\n",
    "1. Prepare the sequence of inputs- and corresponding embeddings.\n",
    "2. Run the computation steps to obtain the intermediate features for each word- composite of transformations of two hidden bilstm states (https://arxiv.org/abs/1511.00215) \n",
    "3. Compute the log likelihoods based on crf feature weights to output sequence labels.\n",
    "\n",
    "I suppose that the network tries to do the following:\n",
    "Given \n",
    "\n",
    "The question now is if we had two domains where labels are available for the first one, but a limited number of labels are available for the second one (we can take the case of being able to ask which sentence really needs a label-which would help our model the most, etc)\n",
    "\n",
    "One simple approach would be to say that since the data is tuned generally- we should have similar feature representations for similar words across domains. This can be viewed as sharing a common latent space- which can either be done by finding similar word contributions amongst words in sentences of different domains- basically words that perform similar roles should have similar embeddings. \n",
    "An easier approach is to say that words that are linked by the same word across domains should have the same embedding- since they're the same feature. \n",
    "\n",
    "-->Another problem in domain adaptation for general sentiment analysisis (not just extraction) is that words can connotate different sentiments. easy-> good for a test, perhaps bad for describing a footballer\n",
    "difficult-> good for describing defence, bad for describing a situation \n",
    "(https://nlp.stanford.edu/projects/socialsent/) \n",
    "\n",
    "--> This is again why a reasoning structure is needed-> soft when used in football can tell about a soft shot(-), soft tackle(-), soft touch(-), feather like control(+). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f16aaba15b8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Input layer and vocab one hot encoded inputs\n",
    "#Embedding layer and its resultant transformation into vocab size<- another parameter\n",
    "#Noisy inputs, etc \n",
    "import csv \n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import spacy\n",
    "import string\n",
    "\n",
    "nlp = spacy.load('en') #load spacy model\n",
    "\n",
    "    \n",
    "def obtain_tokens(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "start_tag = \"<START>\"\n",
    "end_tag = \"<END>\"\n",
    "tag_to_id = {start_tag:0,end_tag:-1, \"BA\":1, \"IA\":2, \"BO\":3, \"IO\":4, \"OT\":5}\n",
    "id_to_tag = {id_: tag for tag,id_ in tag_to_id.items()}\n",
    "\n",
    "'''The below functions that use Spacy can be made faster using Spacy specific syntax- example using doc, checking for verbs, etc. However, here it is converted to Python string format to make the primary function compatible with other NLP packages'''\n",
    "def get_tokenized_list_for_delim(phrases, tokenizer_func = obtain_tokens, delim =\";\"):\n",
    "    '''\n",
    "    Input: A sequence of phrases delimited as provided by delimiter: Ex A B; C; D E F\n",
    "    Output: A list of tokenized phrases: [[A, B], [C], [D, E, F]]\n",
    "    '''\n",
    "    #split_phrases = phrases.split(delim)\n",
    "    \n",
    "    split_phrases = phrases.split(delim)\n",
    "    tokenized_split_phrases = map(lambda x: tokenizer_func(x), split_phrases)\n",
    "    return tokenized_split_phrases\n",
    "\n",
    "\n",
    "punctuation_dict = {'.':' <PERIOD> ',\n",
    "                    ',':' <COMMA> ', \n",
    "                    '\"':' <QUOTATION_MARK> ', \n",
    "                    ';':' <SEMICOLON> ',\n",
    "                    '!': ' <EXCLAMATION_MARK> ',\n",
    "                    '?': ' <QUESTION_MARK> ', \n",
    "                    ':': ' <COLON> '\n",
    "                   ,'-': ' <HYPHEN> ',\n",
    "                    '(': ' <LEFT PARENTHESIS> ',\n",
    "                    ')': ' <RIGHT PARENTHESIS> '\n",
    "                   }\n",
    "\n",
    "\n",
    "def process_text_pos_and_punctuation(text, replace_punctuation):\n",
    "    text = nlp(unicode(text))\n",
    "    tagged_text = []\n",
    "    tokenized_text = []\n",
    "    if(replace_punctuation):\n",
    "        for token in text:\n",
    "            temp_token = token\n",
    "            if(str(token.pos_) == 'PUNCT'):\n",
    "                if(str(token) in punctuation_dict.keys()):\n",
    "                    token = punctuation_dict[str(token)]\n",
    "            tokenized_text.append(str(token))\n",
    "            tagged_text.append((str(token), str(temp_token.pos_), str(temp_token.tag_), str(temp_token.dep_)))\n",
    "    \n",
    "    else:\n",
    "        tagged_text = [(str(token), str(token.pos_), str(token.tag_), str(token.dep_)) for token in text]\n",
    "        tokenized_text = map(lambda x:x[0], tagged_text)\n",
    "    return tokenized_text, tagged_text\n",
    "\n",
    "def store_dataset_info(df, save_to_file = False, save_path ='Final_data/laptop', include_sense = False,  replace_punctuation= True, to_lower = True, get_tags = True, remove_punctuation= False, get_dependency_structure = False, tokenizer_func= obtain_tokens):\n",
    "    training_data = []\n",
    "    training_data_with_other_stuff = []\n",
    "    out_row = []\n",
    "    token_to_freq = {} \n",
    "    #token_sense_to_id = {}\n",
    "    \n",
    "    token_to_id = {}\n",
    "    loop_i = 0 \n",
    "    for sentence, aspect_words, opinion_words in zip(df.Sentence, df.Aspects, df.Opinions):\n",
    "        \n",
    "        \n",
    "        if(remove_punctuation):\n",
    "            sentence = sentence.translate(None, string.punctuation)\n",
    "        \n",
    "        \n",
    "        if(to_lower):\n",
    "            sentence = sentence.lower()\n",
    "            \n",
    "        \n",
    "        tokenized_sentence, tagged_text = process_text_pos_and_punctuation(sentence, replace_punctuation)\n",
    "\n",
    "        if(get_dependency_structure):\n",
    "            #Obtain dependency tree\n",
    "            dependency_tree = get_dependency_tree(sentence)\n",
    "        #if(get_parser_tree):\n",
    "            #Obtain parser tree\n",
    "         #   None\n",
    "        \n",
    "        \n",
    "        aspect_list = get_tokenized_list_for_delim(aspect_words, tokenizer_func, ';')\n",
    "        opinion_list = get_tokenized_list_for_delim(opinion_words, tokenizer_func, ';')\n",
    "        #print(aspect_list)\n",
    "        seq_absa_tagged = [] \n",
    "        aspect_ptr = 0\n",
    "        opinion_ptr = 0\n",
    "        skip = 0\n",
    "        \n",
    "        #labels = []\n",
    "        for i_loop, token in enumerate(tokenized_sentence):\n",
    "            temp_token = token \n",
    "            \n",
    "            #1) If sense is included then we change token to token|POS\n",
    "            if(include_sense):\n",
    "                '''THIS is specific to spacy format'''\n",
    "            \n",
    "                #Only limit to nouns, verbs and adjectives\n",
    "                \n",
    "                if(tagged_text[i_loop][1] in ['NOUN','VERB','ADJ']):\n",
    "                    token+= '|'+ tagged_text[i_loop][1]\n",
    "                    tokenized_sentence[i_loop] = token\n",
    "                    #print(token)\n",
    "                    \n",
    "            if token not in token_to_id:\n",
    "                token_to_id[token] = len(token_to_id)\n",
    "                token_to_freq[token] = 0 \n",
    "            else:\n",
    "                token_to_freq[token] += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "            if(skip>0): #if we encounter incomplete aspect/opinion matches previously\n",
    "                skip -= 1\n",
    "             \n",
    "            else:    \n",
    "                label = [tag_to_id[\"OT\"]] #assume it is OT, store as list in case of multiple aspect/opinion terms\n",
    "        \n",
    "        \n",
    "                token = temp_token\n",
    "                \n",
    "                if(aspect_ptr < len(aspect_list) and token == aspect_list[aspect_ptr][0]): #Incomplete match: battery--> battery life\n",
    "                    label = [tag_to_id[\"BA\"]]\n",
    "                    skip = len(aspect_list[aspect_ptr]) - 1 #words to skip ahead since they have been already covered\n",
    "                    if(skip>0):\n",
    "                        label += [tag_to_id[\"IA\"] for i in aspect_list[aspect_ptr][1:]] \n",
    "                    aspect_ptr += 1 \n",
    "        \n",
    "                elif(opinion_ptr< len(opinion_list) and token == opinion_list[opinion_ptr][0]): \n",
    "                    label = [tag_to_id[\"BO\"]]\n",
    "                    skip = len(opinion_list[opinion_ptr]) - 1\n",
    "                    if(skip>0):\n",
    "                        label += [tag_to_id[\"IO\"] for i in opinion_list[opinion_ptr][1:]]\n",
    "                    opinion_ptr += 1 \n",
    "        \n",
    "            \n",
    "                seq_absa_tagged+=label\n",
    "                \n",
    "        if(get_dependency_structure):\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged, tagged_text, dependency_tree))\n",
    "            out_row = [\"Sentence\", \"Sequence\", \"Tags\",\"Dependency Tree\"]\n",
    "        \n",
    "        elif(get_tags):\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged, tagged_text))\n",
    "            out_row = [\"Sentence\", \"Sequence\", \"Tags\"]\n",
    "        \n",
    "        elif(get_dependency_structure):\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged, dependency_tree))\n",
    "            out_row = [\"Sentence\", \"Sequence\", \"Dependency Tree\"]\n",
    "        else:\n",
    "            training_data_with_other_stuff.append((tokenized_sentence, seq_absa_tagged))\n",
    "            out_row = [\"Sentence\", \"Sequence\"]\n",
    "            \n",
    "        training_data.append((tokenized_sentence, seq_absa_tagged))\n",
    "        #Write data to csv and save vocab as pickle\n",
    "        \n",
    "        if(loop_i%300==0):\n",
    "            print(\"At sentence: {}\".format(-loop_i))\n",
    "            print(training_data_with_other_stuff[-loop_i])\n",
    "        loop_i-=1\n",
    "    \n",
    "    \n",
    "    if(save_to_file):\n",
    "        opt_info = \"Normal_\"\n",
    "        if(include_sense):\n",
    "            opt_info='WITH_SENSE_'\n",
    "        \n",
    "        processed_normal_csv_path = \"{}/{}_absa_seq_labelled.csv\".format(save_path, opt_info)\n",
    "        with open(processed_normal_csv_path,'wb') as fout:\n",
    "            csv_out = csv.writer(fout)\n",
    "            csv_out.writerow([\"Sentence\",\"Sequence\"])\n",
    "            for row in training_data:\n",
    "                csv_out.writerow(row)\n",
    "                \n",
    "        processed_additional_info_csv_path = \"{}/{}_additional_info_seq_labelled.csv\".format(save_path, opt_info)\n",
    "        with open(processed_additional_info_csv_path, 'wb') as fout:\n",
    "            csv_out = csv.writer(fout)\n",
    "            csv_out.writerow(out_row)\n",
    "            for row in training_data_with_other_stuff:\n",
    "                csv_out.writerow(row) \n",
    "        \n",
    "        processed_norm_training_data_pickle = \"{}/{}_normal_training_list.pickle\".format(save_path, opt_info)\n",
    "        with open(processed_norm_training_data_pickle,'wb') as pickle_o:\n",
    "            pickle.dump(training_data, pickle_o)\n",
    "        \n",
    "        processed_add_training_data_pickle = \"{}/{}_additional_training_list.pickle\".format(save_path, opt_info)\n",
    "        with open(processed_add_training_data_pickle,'wb') as pickle_o:\n",
    "            pickle.dump(training_data_with_other_stuff, pickle_o)\n",
    "        \n",
    "        processed_vocab_pickle = \"{}/{}_vocab.pickle\".format(save_path, opt_info)\n",
    "        with open(processed_vocab_pickle,'wb') as pickle_o: \n",
    "            pickle.dump(token_to_id, pickle_o)\n",
    "        \n",
    "        token_to_freq_pickle = \"{}/{}_tokenfreq.pickle\".format(save_path, opt_info)\n",
    "        with open(token_to_freq_pickle,'wb') as pickle_o: \n",
    "            pickle.dump(token_to_freq, pickle_o)\n",
    "        \n",
    "        tag_to_id_pickle = \"{}/{}_tag2id.pickle\".format(save_path, opt_info)\n",
    "        with open(tag_to_id_pickle, 'wb') as pickle_o:\n",
    "            pickle.dump(tag_to_id, pickle_o)\n",
    "    \n",
    "    return save_path, processed_normal_csv_path, processed_vocab_pickle \n",
    "\n",
    "class Domain:\n",
    "    def __init__(self, name, primary_dir, data_path, raw_csv_file=None, already_processed = True):\n",
    "        \"\"\"\n",
    "        primary_dir refers to the directory where the domain info is stored\n",
    "        Ideally data_dir will be uniform throughout all domains.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.primary_dir = primary_dir\n",
    "        self.raw_csv_file = raw_csv_file\n",
    "        self.data_path = data_path #this is the processed data is stored\n",
    "        self.vocab_path = None\n",
    "        self.embedding_dir = None\n",
    "        if(not already_processed):\n",
    "            self.tr_data_path, self.vocab_path = self.run_data_processing(True) #with sense, lower\n",
    "            _, _ = self.run_data_processing(False)\n",
    "            \n",
    "    def run_data_processing(self, with_sense= False):\n",
    "        pd_file = pd.read_csv(self.raw_csv_file) #convert to pandas\n",
    "        print(\"Processing raw data first with and then without sense\")\n",
    "        _, tr_data_path, vocab_path = store_dataset_info(pd_file, True, self.data_path, with_sense)\n",
    "        return tr_data_path, vocab_path\n",
    "        \n",
    "    def get_domain_independent_features(self):\n",
    "        None\n",
    "        \n",
    "    def get_feature_label_mutual_info(self):\n",
    "        None\n",
    "        \n",
    "    def features(self):\n",
    "        None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing raw data first with and then without sense\n",
      "At sentence: 0\n",
      "(['but', 'the', 'staff|NOUN', 'was|VERB', 'so', 'horrible|ADJ', 'to', 'us', ' <PERIOD> '], [5, 5, 1, 5, 5, 3, 5, 5, 5], [('but', 'CCONJ', 'CC', 'cc'), ('the', 'DET', 'DT', 'det'), ('staff', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ROOT'), ('so', 'ADV', 'RB', 'advmod'), ('horrible', 'ADJ', 'JJ', 'acomp'), ('to', 'ADP', 'IN', 'prep'), ('us', 'PRON', 'PRP', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 300\n",
      "(['this', 'place|NOUN', 'is|VERB', 'great|ADJ', ' <PERIOD> '], [5, 5, 5, 3, 5], [('this', 'DET', 'DT', 'det'), ('place', 'NOUN', 'NN', 'nsubj'), ('is', 'VERB', 'VBZ', 'ROOT'), ('great', 'ADJ', 'JJ', 'acomp'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 600\n",
      "(['the', 'food|NOUN', 'is|VERB', 'o.k', ' <PERIOD> ', ' <COMMA> ', 'but', 'not', 'any', 'better|ADJ', 'than', 'what|NOUN', 'you', 'get|VERB', 'at', 'a', 'good|ADJ', 'neighborhood|NOUN', 'restaurant|NOUN', ' <PERIOD> '], [5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [('the', 'DET', 'DT', 'det'), ('food', 'NOUN', 'NN', 'nsubj'), ('is', 'VERB', 'VBZ', 'ROOT'), ('o.k', 'PUNCT', ':', 'attr'), (' <PERIOD> ', 'PUNCT', '.', 'punct'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('but', 'CCONJ', 'CC', 'cc'), ('not', 'ADV', 'RB', 'neg'), ('any', 'DET', 'DT', 'advmod'), ('better', 'ADJ', 'JJR', 'ROOT'), ('than', 'ADP', 'IN', 'prep'), ('what', 'NOUN', 'WP', 'dobj'), ('you', 'PRON', 'PRP', 'nsubj'), ('get', 'VERB', 'VBP', 'pcomp'), ('at', 'ADP', 'IN', 'prep'), ('a', 'DET', 'DT', 'det'), ('good', 'ADJ', 'JJ', 'amod'), ('neighborhood', 'NOUN', 'NN', 'compound'), ('restaurant', 'NOUN', 'NN', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 900\n",
      "(['we', 'wo|VERB', \"n't\", 'go|VERB', 'to', 'this', 'place|NOUN', 'again', 'for', 'a', 'good|ADJ', 'meal|NOUN', ' <PERIOD> '], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 1, 5], [('we', 'PRON', 'PRP', 'nsubj'), ('wo', 'VERB', 'MD', 'aux'), (\"n't\", 'ADV', 'RB', 'neg'), ('go', 'VERB', 'VB', 'ROOT'), ('to', 'ADP', 'IN', 'prep'), ('this', 'DET', 'DT', 'det'), ('place', 'NOUN', 'NN', 'pobj'), ('again', 'ADV', 'RB', 'advmod'), ('for', 'ADP', 'IN', 'prep'), ('a', 'DET', 'DT', 'det'), ('good', 'ADJ', 'JJ', 'amod'), ('meal', 'NOUN', 'NN', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 1200\n",
      "(['it', \"'s|VERB\", 'a', 'steal|NOUN', ' <EXCLAMATION_MARK> '], [5, 5, 5, 5, 5], [('it', 'PRON', 'PRP', 'nsubj'), (\"'s\", 'VERB', 'VBZ', 'ROOT'), ('a', 'DET', 'DT', 'det'), ('steal', 'NOUN', 'NN', 'attr'), (' <EXCLAMATION_MARK> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 1500\n",
      "(['my|ADJ', 'steak|NOUN', 'au', 'poivre|NOUN', 'was|VERB', 'one', 'of', 'the', 'worst|ADJ', 'i', \"'ve|VERB\", 'had|VERB', ' <PERIOD> '], [5, 1, 2, 2, 5, 5, 5, 5, 3, 5, 5, 5, 5], [('my', 'ADJ', 'PRP$', 'poss'), ('steak', 'NOUN', 'NN', 'nsubj'), ('au', 'DET', 'DT', 'prep'), ('poivre', 'NOUN', 'NN', 'pobj'), ('was', 'VERB', 'VBD', 'ROOT'), ('one', 'NUM', 'CD', 'attr'), ('of', 'ADP', 'IN', 'prep'), ('the', 'DET', 'DT', 'det'), ('worst', 'ADJ', 'JJS', 'pobj'), ('i', 'PRON', 'PRP', 'nsubj'), (\"'ve\", 'VERB', 'VB', 'aux'), ('had', 'VERB', 'VBN', 'relcl'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 1800\n",
      "(['bring|VERB', 'your|ADJ', 'appetites|NOUN', ' <PERIOD> '], [5, 5, 5, 5], [('bring', 'VERB', 'VB', 'ROOT'), ('your', 'ADJ', 'PRP$', 'poss'), ('appetites', 'NOUN', 'NNS', 'dobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 2100\n",
      "(['service|NOUN', 'is|VERB', 'friendly|ADJ', ' <COMMA> ', 'prices|NOUN', 'are|VERB', 'good|ADJ', ' <HYPHEN> ', 'delivery|NOUN', 'time|NOUN', 'was|VERB', 'a', 'little|ADJ', 'slow|ADJ', ' <COMMA> ', 'but', 'for', 'the', 'way|NOUN', 'this', 'pizza|NOUN', 'tastes|VERB', ' <COMMA> ', 'i', \"'m|VERB\", 'willing|ADJ', 'to', 'overlook|VERB', 'it', ' <PERIOD> '], [5, 5, 3, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [('service', 'NOUN', 'NN', 'nsubj'), ('is', 'VERB', 'VBZ', 'ccomp'), ('friendly', 'ADJ', 'JJ', 'acomp'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('prices', 'NOUN', 'NNS', 'nsubj'), ('are', 'VERB', 'VBP', 'ROOT'), ('good', 'ADJ', 'JJ', 'amod'), (' <HYPHEN> ', 'PUNCT', 'HYPH', 'punct'), ('delivery', 'NOUN', 'NN', 'compound'), ('time', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ccomp'), ('a', 'DET', 'DT', 'det'), ('little', 'ADJ', 'JJ', 'advmod'), ('slow', 'ADJ', 'JJ', 'acomp'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('but', 'CCONJ', 'CC', 'cc'), ('for', 'ADP', 'IN', 'prep'), ('the', 'DET', 'DT', 'det'), ('way', 'NOUN', 'NN', 'pobj'), ('this', 'DET', 'DT', 'det'), ('pizza', 'NOUN', 'NN', 'nsubj'), ('tastes', 'VERB', 'VBZ', 'relcl'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('i', 'PRON', 'PRP', 'nsubj'), (\"'m\", 'VERB', 'VBP', 'conj'), ('willing', 'ADJ', 'JJ', 'acomp'), ('to', 'PART', 'TO', 'aux'), ('overlook', 'VERB', 'VB', 'xcomp'), ('it', 'PRON', 'PRP', 'dobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 2400\n",
      "(['the', 'hot|ADJ', 'and', 'sour|ADJ', 'soup|NOUN', 'was|VERB', 'unbearably', 'hot|ADJ', 'and', 'tasted|VERB', 'of', 'only|ADJ', 'pepper|NOUN', 'and', 'nothing|NOUN', 'else', ' <PERIOD> '], [5, 5, 5, 5, 1, 5, 3, 4, 5, 5, 5, 5, 1, 5, 5, 5, 5], [('the', 'DET', 'DT', 'det'), ('hot', 'ADJ', 'JJ', 'amod'), ('and', 'CCONJ', 'CC', 'cc'), ('sour', 'ADJ', 'JJ', 'conj'), ('soup', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ROOT'), ('unbearably', 'ADV', 'RB', 'advmod'), ('hot', 'ADJ', 'JJ', 'acomp'), ('and', 'CCONJ', 'CC', 'cc'), ('tasted', 'VERB', 'VBN', 'conj'), ('of', 'ADP', 'IN', 'prep'), ('only', 'ADJ', 'JJ', 'amod'), ('pepper', 'NOUN', 'NN', 'pobj'), ('and', 'CCONJ', 'CC', 'cc'), ('nothing', 'NOUN', 'NN', 'conj'), ('else', 'ADV', 'RB', 'advmod'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 2700\n",
      "(['service|NOUN', 'was|VERB', 'slow|ADJ', ' <COMMA> ', 'but', 'the', 'people|NOUN', 'were|VERB', 'friendly|ADJ', ' <PERIOD> '], [5, 5, 3, 5, 5, 5, 5, 5, 3, 5], [('service', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ROOT'), ('slow', 'ADJ', 'JJ', 'acomp'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('but', 'CCONJ', 'CC', 'cc'), ('the', 'DET', 'DT', 'det'), ('people', 'NOUN', 'NNS', 'nsubj'), ('were', 'VERB', 'VBD', 'conj'), ('friendly', 'ADJ', 'JJ', 'acomp'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 3000\n",
      "(['if', 'you', 'like|VERB', 'the', 'food|NOUN', 'and', 'the', 'value|NOUN', 'you', 'get|VERB', 'from', 'some', 'of', 'chinatown|ADJ', 'restaurants|NOUN', ' <COMMA> ', 'this', 'is|VERB', 'not', 'the', 'place|NOUN', 'for', 'you', ' <PERIOD> '], [5, 5, 3, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [('if', 'ADP', 'IN', 'mark'), ('you', 'PRON', 'PRP', 'nsubj'), ('like', 'VERB', 'VBP', 'advcl'), ('the', 'DET', 'DT', 'det'), ('food', 'NOUN', 'NN', 'dobj'), ('and', 'CCONJ', 'CC', 'cc'), ('the', 'DET', 'DT', 'det'), ('value', 'NOUN', 'NN', 'conj'), ('you', 'PRON', 'PRP', 'nsubj'), ('get', 'VERB', 'VBP', 'relcl'), ('from', 'ADP', 'IN', 'prep'), ('some', 'DET', 'DT', 'pobj'), ('of', 'ADP', 'IN', 'prep'), ('chinatown', 'ADJ', 'JJ', 'amod'), ('restaurants', 'NOUN', 'NNS', 'pobj'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('this', 'DET', 'DT', 'nsubj'), ('is', 'VERB', 'VBZ', 'ROOT'), ('not', 'ADV', 'RB', 'neg'), ('the', 'DET', 'DT', 'det'), ('place', 'NOUN', 'NN', 'attr'), ('for', 'ADP', 'IN', 'prep'), ('you', 'PRON', 'PRP', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "Processing raw data first with and then without sense\n",
      "At sentence: 0\n",
      "(['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', ' <PERIOD> '], [5, 5, 1, 5, 5, 3, 5, 5, 5], [('but', 'CCONJ', 'CC', 'cc'), ('the', 'DET', 'DT', 'det'), ('staff', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ROOT'), ('so', 'ADV', 'RB', 'advmod'), ('horrible', 'ADJ', 'JJ', 'acomp'), ('to', 'ADP', 'IN', 'prep'), ('us', 'PRON', 'PRP', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 300\n",
      "(['this', 'place', 'is', 'great', ' <PERIOD> '], [5, 5, 5, 3, 5], [('this', 'DET', 'DT', 'det'), ('place', 'NOUN', 'NN', 'nsubj'), ('is', 'VERB', 'VBZ', 'ROOT'), ('great', 'ADJ', 'JJ', 'acomp'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 600\n",
      "(['the', 'food', 'is', 'o.k', ' <PERIOD> ', ' <COMMA> ', 'but', 'not', 'any', 'better', 'than', 'what', 'you', 'get', 'at', 'a', 'good', 'neighborhood', 'restaurant', ' <PERIOD> '], [5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [('the', 'DET', 'DT', 'det'), ('food', 'NOUN', 'NN', 'nsubj'), ('is', 'VERB', 'VBZ', 'ROOT'), ('o.k', 'PUNCT', ':', 'attr'), (' <PERIOD> ', 'PUNCT', '.', 'punct'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('but', 'CCONJ', 'CC', 'cc'), ('not', 'ADV', 'RB', 'neg'), ('any', 'DET', 'DT', 'advmod'), ('better', 'ADJ', 'JJR', 'ROOT'), ('than', 'ADP', 'IN', 'prep'), ('what', 'NOUN', 'WP', 'dobj'), ('you', 'PRON', 'PRP', 'nsubj'), ('get', 'VERB', 'VBP', 'pcomp'), ('at', 'ADP', 'IN', 'prep'), ('a', 'DET', 'DT', 'det'), ('good', 'ADJ', 'JJ', 'amod'), ('neighborhood', 'NOUN', 'NN', 'compound'), ('restaurant', 'NOUN', 'NN', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 900\n",
      "(['we', 'wo', \"n't\", 'go', 'to', 'this', 'place', 'again', 'for', 'a', 'good', 'meal', ' <PERIOD> '], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 1, 5], [('we', 'PRON', 'PRP', 'nsubj'), ('wo', 'VERB', 'MD', 'aux'), (\"n't\", 'ADV', 'RB', 'neg'), ('go', 'VERB', 'VB', 'ROOT'), ('to', 'ADP', 'IN', 'prep'), ('this', 'DET', 'DT', 'det'), ('place', 'NOUN', 'NN', 'pobj'), ('again', 'ADV', 'RB', 'advmod'), ('for', 'ADP', 'IN', 'prep'), ('a', 'DET', 'DT', 'det'), ('good', 'ADJ', 'JJ', 'amod'), ('meal', 'NOUN', 'NN', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 1200\n",
      "(['it', \"'s\", 'a', 'steal', ' <EXCLAMATION_MARK> '], [5, 5, 5, 5, 5], [('it', 'PRON', 'PRP', 'nsubj'), (\"'s\", 'VERB', 'VBZ', 'ROOT'), ('a', 'DET', 'DT', 'det'), ('steal', 'NOUN', 'NN', 'attr'), (' <EXCLAMATION_MARK> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 1500\n",
      "(['my', 'steak', 'au', 'poivre', 'was', 'one', 'of', 'the', 'worst', 'i', \"'ve\", 'had', ' <PERIOD> '], [5, 1, 2, 2, 5, 5, 5, 5, 3, 5, 5, 5, 5], [('my', 'ADJ', 'PRP$', 'poss'), ('steak', 'NOUN', 'NN', 'nsubj'), ('au', 'DET', 'DT', 'prep'), ('poivre', 'NOUN', 'NN', 'pobj'), ('was', 'VERB', 'VBD', 'ROOT'), ('one', 'NUM', 'CD', 'attr'), ('of', 'ADP', 'IN', 'prep'), ('the', 'DET', 'DT', 'det'), ('worst', 'ADJ', 'JJS', 'pobj'), ('i', 'PRON', 'PRP', 'nsubj'), (\"'ve\", 'VERB', 'VB', 'aux'), ('had', 'VERB', 'VBN', 'relcl'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 1800\n",
      "(['bring', 'your', 'appetites', ' <PERIOD> '], [5, 5, 5, 5], [('bring', 'VERB', 'VB', 'ROOT'), ('your', 'ADJ', 'PRP$', 'poss'), ('appetites', 'NOUN', 'NNS', 'dobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 2100\n",
      "(['service', 'is', 'friendly', ' <COMMA> ', 'prices', 'are', 'good', ' <HYPHEN> ', 'delivery', 'time', 'was', 'a', 'little', 'slow', ' <COMMA> ', 'but', 'for', 'the', 'way', 'this', 'pizza', 'tastes', ' <COMMA> ', 'i', \"'m\", 'willing', 'to', 'overlook', 'it', ' <PERIOD> '], [5, 5, 3, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [('service', 'NOUN', 'NN', 'nsubj'), ('is', 'VERB', 'VBZ', 'ccomp'), ('friendly', 'ADJ', 'JJ', 'acomp'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('prices', 'NOUN', 'NNS', 'nsubj'), ('are', 'VERB', 'VBP', 'ROOT'), ('good', 'ADJ', 'JJ', 'amod'), (' <HYPHEN> ', 'PUNCT', 'HYPH', 'punct'), ('delivery', 'NOUN', 'NN', 'compound'), ('time', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ccomp'), ('a', 'DET', 'DT', 'det'), ('little', 'ADJ', 'JJ', 'advmod'), ('slow', 'ADJ', 'JJ', 'acomp'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('but', 'CCONJ', 'CC', 'cc'), ('for', 'ADP', 'IN', 'prep'), ('the', 'DET', 'DT', 'det'), ('way', 'NOUN', 'NN', 'pobj'), ('this', 'DET', 'DT', 'det'), ('pizza', 'NOUN', 'NN', 'nsubj'), ('tastes', 'VERB', 'VBZ', 'relcl'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('i', 'PRON', 'PRP', 'nsubj'), (\"'m\", 'VERB', 'VBP', 'conj'), ('willing', 'ADJ', 'JJ', 'acomp'), ('to', 'PART', 'TO', 'aux'), ('overlook', 'VERB', 'VB', 'xcomp'), ('it', 'PRON', 'PRP', 'dobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 2400\n",
      "(['the', 'hot', 'and', 'sour', 'soup', 'was', 'unbearably', 'hot', 'and', 'tasted', 'of', 'only', 'pepper', 'and', 'nothing', 'else', ' <PERIOD> '], [5, 5, 5, 5, 1, 5, 3, 4, 5, 5, 5, 5, 1, 5, 5, 5, 5], [('the', 'DET', 'DT', 'det'), ('hot', 'ADJ', 'JJ', 'amod'), ('and', 'CCONJ', 'CC', 'cc'), ('sour', 'ADJ', 'JJ', 'conj'), ('soup', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ROOT'), ('unbearably', 'ADV', 'RB', 'advmod'), ('hot', 'ADJ', 'JJ', 'acomp'), ('and', 'CCONJ', 'CC', 'cc'), ('tasted', 'VERB', 'VBN', 'conj'), ('of', 'ADP', 'IN', 'prep'), ('only', 'ADJ', 'JJ', 'amod'), ('pepper', 'NOUN', 'NN', 'pobj'), ('and', 'CCONJ', 'CC', 'cc'), ('nothing', 'NOUN', 'NN', 'conj'), ('else', 'ADV', 'RB', 'advmod'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 2700\n",
      "(['service', 'was', 'slow', ' <COMMA> ', 'but', 'the', 'people', 'were', 'friendly', ' <PERIOD> '], [5, 5, 3, 5, 5, 5, 5, 5, 3, 5], [('service', 'NOUN', 'NN', 'nsubj'), ('was', 'VERB', 'VBD', 'ROOT'), ('slow', 'ADJ', 'JJ', 'acomp'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('but', 'CCONJ', 'CC', 'cc'), ('the', 'DET', 'DT', 'det'), ('people', 'NOUN', 'NNS', 'nsubj'), ('were', 'VERB', 'VBD', 'conj'), ('friendly', 'ADJ', 'JJ', 'acomp'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n",
      "At sentence: 3000\n",
      "(['if', 'you', 'like', 'the', 'food', 'and', 'the', 'value', 'you', 'get', 'from', 'some', 'of', 'chinatown', 'restaurants', ' <COMMA> ', 'this', 'is', 'not', 'the', 'place', 'for', 'you', ' <PERIOD> '], [5, 5, 3, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], [('if', 'ADP', 'IN', 'mark'), ('you', 'PRON', 'PRP', 'nsubj'), ('like', 'VERB', 'VBP', 'advcl'), ('the', 'DET', 'DT', 'det'), ('food', 'NOUN', 'NN', 'dobj'), ('and', 'CCONJ', 'CC', 'cc'), ('the', 'DET', 'DT', 'det'), ('value', 'NOUN', 'NN', 'conj'), ('you', 'PRON', 'PRP', 'nsubj'), ('get', 'VERB', 'VBP', 'relcl'), ('from', 'ADP', 'IN', 'prep'), ('some', 'DET', 'DT', 'pobj'), ('of', 'ADP', 'IN', 'prep'), ('chinatown', 'ADJ', 'JJ', 'amod'), ('restaurants', 'NOUN', 'NNS', 'pobj'), (' <COMMA> ', 'PUNCT', ',', 'punct'), ('this', 'DET', 'DT', 'nsubj'), ('is', 'VERB', 'VBZ', 'ROOT'), ('not', 'ADV', 'RB', 'neg'), ('the', 'DET', 'DT', 'det'), ('place', 'NOUN', 'NN', 'attr'), ('for', 'ADP', 'IN', 'prep'), ('you', 'PRON', 'PRP', 'pobj'), (' <PERIOD> ', 'PUNCT', '.', 'punct')])\n"
     ]
    }
   ],
   "source": [
    "d2 = Domain('Rest','./Final_data/Domains/Rest/', './Final_data/Domains/Rest/','./Final_data/Semeval_14_ver1/Combined_restaurant.csv',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./Final_data/Domains/rest.pkl', 'wb') as output:\n",
    "    pickle.dump(d1, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Auxiliary functions (subsampling, sense, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Subsampling-> Remove words with a probability proportional to  high frequency given the function\n",
    "\n",
    "\n",
    "\n",
    "### Generate target context pairs\n",
    "def get_context(words, index, window_size):\n",
    "    '''Given a window size and current index of target, return the context words'''\n",
    "    r = np.random.randint(1, window_size+1)\n",
    "    start = index - r if(index - r )>0 else 0 \n",
    "    stop = index + r\n",
    "    context_words = list(set(words[start:index]+words[index+1, stop+1]))\n",
    "    return context_words\n",
    "\n",
    "\n",
    "def generate_training_batch(tokenized_corpus, batch_size, window_size = 5):\n",
    "    '''This runs over the entire dataset once'''\n",
    "    num_batches = np.ceil(float(len(corpus)))/batch_size #This is assuming that each target->all contexts are taken as a single element\n",
    "    \n",
    "    num_in_last_batch = len(corpus)%batch_size \n",
    "    tr_x = []\n",
    "    tr_y = []\n",
    "            \n",
    "    for batch_num in range(num_batches-1): #do the last batch with \n",
    "    #1) For each word, we obtain the context words with a random variable ranging from 1 to desired window size\n",
    "        tr_x = []\n",
    "        tr_y = []\n",
    "        batched_corpus = tokenized_corpus[batch_num*batch_size:(batch_num+1)*batch_size] \n",
    "        for target in batched_corpus :\n",
    "            tr_x = [target]\n",
    "            tr_y = get_context()\n",
    "            \n",
    "    #Do same operations for last batch\n",
    "    \n",
    "   # if(num_in_last_batch>0):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ContextPredictionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(ContextPredictionEmbedding,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim, 300)\n",
    "        self.linear2 = nn.Linear(300, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        input_embedding = self.embeddings(inputs).view((1,-1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./Final_data/laptop_lower_additional_training_list.pickle') as f:\n",
    "    training_data_laptop = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./Final_data/laptop_lower_vocab.pickle') as f:\n",
    "    laptop_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = ContextPredictionEmbedding(len(vocab), 300, 2)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0]):\n",
    "    for target, context in training_batch:\n",
    "        \n",
    "        #1) Convert target var to embedding and wrap as a variable\n",
    "        target_id = [vocab[target]]\n",
    "        context_id = [vocab[context]]\n",
    "        target_var = autograd.Variable(torch.LongTensor(target_id))\n",
    "        context_var = autograd.Variable(torch.LongTensor(context))\n",
    "        \n",
    "        #2) reset gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #3) run forward pass\n",
    "        log_probs = model(target_var)\n",
    "        \n",
    "        #4) compute loss and update parameters\n",
    "        loss = loss_function(log_probs, context_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+= loss.data\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = \"Hi Mr. K, what is up?\"\n",
    "text = nlp(unicode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos_tagged_text = [(str(token), str(token.pos_), str(token.tag_), str(token.dep_)) for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'INTJ', 'UH', 'intj'),\n",
       " ('Mr.', 'PROPN', 'NNP', 'compound'),\n",
       " ('K', 'PROPN', 'NNP', 'npadvmod'),\n",
       " (',', 'PUNCT', ',', 'punct'),\n",
       " ('what', 'NOUN', 'WP', 'nsubj'),\n",
       " ('is', 'VERB', 'VBZ', 'ROOT'),\n",
       " ('up', 'ADV', 'RB', 'advmod'),\n",
       " ('?', 'PUNCT', '.', 'punct')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tagged_text, tokenized_text = process_text_pos_and_punctuation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Mr.', 'K', ' <COMMA> ', 'what', 'is', 'up', ' <QUESTION_MARK> ']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'INTJ', 'UH', 'intj'),\n",
       " ('Mr.', 'PROPN', 'NNP', 'compound'),\n",
       " ('K', 'PROPN', 'NNP', 'npadvmod'),\n",
       " (' <COMMA> ', 'PUNCT', ',', 'punct'),\n",
       " ('what', 'NOUN', 'WP', 'nsubj'),\n",
       " ('is', 'VERB', 'VBZ', 'ROOT'),\n",
       " ('up', 'ADV', 'RB', 'advmod'),\n",
       " (' <QUESTION_MARK> ', 'PUNCT', '.', 'punct')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
