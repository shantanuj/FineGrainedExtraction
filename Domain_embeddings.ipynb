{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Why do we need pretrained embeddings?\n",
    "\n",
    "General embeddings\n",
    "The purpose of embeddings is to be able to represent words with information we deem useful for our task. This is vaguely the definition of a feature for a model. The current trending mechanism to represent word information in deep learning is by being able to encode the usage of a word through its neighbours/context. These context specific features can be derived through tuned matrix transformations, ie, a neural network. Thus, from a neural network point of view, for each word we try and obtain a representation/vector that when transformed to the vocab space (softmax layer) results in high activations for mostly co-occuring words. \n",
    "\n",
    "These embeddings can be used in multiple ways for a deep learning text specific task. They can be tuned further based on the task (this results in a lot more parameters), or set as fixed. Give rep to a model and ask it to based on this, capture other info. Good detailed overview  with other techniques: https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795) \n",
    "\n",
    "2nd extension: Word senses\n",
    "Consider the sentences:\n",
    "S1: \"The stock price took a hit during recession\"\n",
    "S2: \"He hit the ball for a six\" \n",
    "The word hit has a different meaning/contribution in the sentence. This varying usage of a word is termed as a word exhibiting different senses, and something the vanilla word embedding will not capture.\n",
    "A simple method to capture different senses is to associate the POS tag with the word when computing the embedding. Hence, hit|NOUN and hit|VERB will have two different embeddings. \n",
    "\n",
    "3rd extension: Domain extension-Domain linkers, etc\n",
    "Now consider the scenario for our problem wherein we train a deep learning model to detect aspect and opinion terms with its input features as our pretrained word embeddings. The resultant trained model, based on its architecture, learns the transformation matrices (which may be used to derive additional variables for computation- ex. attention) to transform the data into a 'latent/middle ground' space.\n",
    "Let's break down the computation and training process of a BiLSTM-CRF model:\n",
    "1. Prepare the sequence of inputs- and corresponding embeddings.\n",
    "2. Run the computation steps to obtain the intermediate features for each word- composite of transformations of two hidden bilstm states (https://arxiv.org/abs/1511.00215) \n",
    "3. Compute the log likelihoods based on crf feature weights to output sequence labels.\n",
    "\n",
    "I suppose that the network tries to do the following:\n",
    "Given \n",
    "\n",
    "The question now is if we had two domains where labels are available for the first one, but a limited number of labels are available for the second one (we can take the case of being able to ask which sentence really needs a label-which would help our model the most, etc)\n",
    "\n",
    "One simple approach would be to say that since the data is tuned generally- we should have similar feature representations for similar words across domains. This can be viewed as sharing a common latent space- which can either be done by finding similar word contributions amongst words in sentences of different domains- basically words that perform similar roles should have similar embeddings. \n",
    "An easier approach is to say that words that are linked by the same word across domains should have the same embedding- since they're the same feature. \n",
    "\n",
    "-->Another problem in domain adaptation for general sentiment analysisis (not just extraction) is that words can connotate different sentiments. easy-> good for a test, perhaps bad for describing a footballer\n",
    "difficult-> good for describing defence, bad for describing a situation \n",
    "(https://nlp.stanford.edu/projects/socialsent/) \n",
    "\n",
    "--> This is again why a reasoning structure is needed-> soft when used in football can tell about a soft shot(-), soft tackle(-), soft touch(-), feather like control(+). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0bb0241530>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer and vocab one hot encoded inputs\n",
    "#Embedding layer and its resultant transformation into vocab size<- another parameter\n",
    "#Noisy inputs, etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions (subsampling, sense, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Subsampling-> Remove words with a probability proportional to  high frequency given the function\n",
    "\n",
    "\n",
    "\n",
    "### Generate target context pairs\n",
    "def get_target(words, index, window_size):\n",
    "    '''Given a window size and current index of target, return the context words'''\n",
    "    r = np.random.randint(1, window_size+1)\n",
    "    start = index - r if(index - r )>0 else 0 \n",
    "    stop = index + r\n",
    "    target_words = list(set(words[start:index]+words[index+1, stop+1]))\n",
    "    return target_words\n",
    "\n",
    "\n",
    "def generate_training_batch(tokenized_corpus, batch_size, window_size = 5):\n",
    "    '''This runs over the entire dataset once'''\n",
    "    num_batches = np.ceil(float(len(corpus))/batch_size #This is assuming that each target->all contexts are taken as a single element\n",
    "    \n",
    "    num_in_last_batch = len(corpus)%batch_size \n",
    "    \n",
    "    for batch_num in range(num_batches-1): #do the last batch with \n",
    "    #1) For each word, we obtain the context words with a random variable ranging from 1 to desired window size\n",
    "        target, context = [], []\n",
    "        for target in tokenized_corpus[batch_num*batch_size:batch_num*batch_size]\n",
    "    #Do same operations for last batch\n",
    "    if(num_in_last_batch>0):\n",
    "        \n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextPredictionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(ContextPredictionEmbedding,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim, 300)\n",
    "        self.linear2 = nn.Linear(300, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        input_embedding = self.embeddings(inputs).view((1,-1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Final_data/laptop_lower_additional_training_list.pickle') as f:\n",
    "    training_data_laptop = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Final_data/laptop_lower_vocab.pickle') as f:\n",
    "    laptop_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = ContextPredictionEmbedding(len(vocab), 300, 2)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0]):\n",
    "    for target, context in training_batch:\n",
    "        \n",
    "        #1) Convert target var to embedding and wrap as a variable\n",
    "        target_id = [vocab[target]]\n",
    "        context_id = [vocab[context]]\n",
    "        target_var = autograd.Variable(torch.LongTensor(target_id))\n",
    "        context_var = autograd.Variable(torch.LongTensor(context))\n",
    "        \n",
    "        #2) reset gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #3) run forward pass\n",
    "        log_probs = model(target_var)\n",
    "        \n",
    "        #4) compute loss and update parameters\n",
    "        loss = loss_function(log_probs, context_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+= loss.data\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
