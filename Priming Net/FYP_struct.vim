1) Problem description and breakdown into domain adaption and absa
2) PART 1: Domain adaption-> Overview of current approaches, two views on structural learning and common feature learning. 
3) PART 1: Baseline model used- BILSTM CRF-> Explanation of how they work, and brief implementation details. 
4) PART 1: Use of embeddings and relevance in deep learning approaches. Introducition to sense and word2vec details. 
5) PART 1: Differnetial embeddings.-> Visualization of embeddings. 
6) Tabulation of results, need for similar structure in embeddings.  
6) PART 1: Detection of Domain linkers, independent and dependent words. Need for word contribution; brief overview of bipartite graph and spectral graph application. 


7) PART 2: Motivated by detection of domain word types-> 'word contribution'-> it's definition and relevance in problem of ABSA and Domain types.
8) Preliminary experiment-> Impact of changes in sentence structure (syntactic and semantic) due to word removal. Use of Seq2Seq autoencoders.
9) Our method: Induce archetypal words and word properties. Word properties and training methodology. Word properties are the parameters trained through minimization of KL divergence. 
10) Performance in ABSA labelling task, future use in domain adaption. 
11) Future work in hierarichal reasoning structures; incomplete reasoning blocks; concept activation/excitement through priming replica; Future references/books


