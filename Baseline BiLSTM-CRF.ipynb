{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f17d40e9510>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import pandas as pd\n",
    "import pickle, csv\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from pprint import pprint\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bilstm +CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''Auxiliary functions'''\n",
    "\n",
    "#Load pretrained embeddings function \n",
    "def load_embeddings(path_to_embeddings, word2id, embedding_dim = 25):\n",
    "    '''\n",
    "    Input: This only takes in w2v format. So convert all other embedding types/vectors to w2v format: \n",
    "    Output: Torch variable with embeddings only belonging to and indexed by word2id\n",
    "    \n",
    "    For glove:\n",
    "    Use python -m gensim.scripts.glove2word2vec -i <GloVe vector file> -o <Word2vec vector file>\n",
    "    as per https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "    \n",
    "    \n",
    "    W2V format is:\n",
    "    <Num vectors> <dimensionality>\n",
    "    <word1> <vector rep>\n",
    "    <word2> <vector rep>\n",
    "    ....and so on\n",
    "    '''\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(path_to_embeddings, binary = False)\n",
    "    \n",
    "    \n",
    "    corpus_embeddings = np.zeros((len(word2id), embedding_dim))\n",
    "    #Should words not found in embedding file be random or zeros? \n",
    "    #Currently 0 and then they are trained anyway\n",
    "    '''Should we use a try/catch block here so that words that are not in vocab force an exception?\n",
    "    Or should we choose every word in vocab and see if it is in embedding vocab-> worst case |V|*|embedding_vocab|\n",
    "    Or vice versa, same worst case but early stopping \n",
    "    Or use key retrieval so all words in embedding vocab are checked if they belong to dictionary key\n",
    "    '''\n",
    "    for word in word2id.keys():\n",
    "        if word in word_vectors.vocab:\n",
    "            corpus_embeddings[word2id[word]] = np.array(word_vectors[word])\n",
    "        \n",
    "    return torch.from_numpy(corpus_embeddings).float()\n",
    "\n",
    "\n",
    "'''Training the simple encoder-decoder'''\n",
    "def form_indexed_sequence_variable(sequence, word_to_id, end_token):\n",
    "    #Input sequence is a list of word tokens\n",
    "    #Output sequence is a list of id tokens based on provided word to id mapping\n",
    "    input_seq = [word_to_id[token] for token in sequence] + [end_token]\n",
    "    input_seq_var = autograd.Variable(torch.LongTensor(input_seq)).view(-1,1) #make a 2 rank tensor\n",
    "    if(use_cuda):\n",
    "        return input_seq_var.cuda()\n",
    "    else:\n",
    "        return input_seq_var\n",
    "    \n",
    "def generate_training_sequence(training_dataset, word_to_id, end_token):\n",
    "    '''Generator function to yield training example \n",
    "    Not very useful in this case but for large datasets it can save memory if we pass the dataset by reference.\n",
    "    Use hpy (guppy for py2) to explore memory usage in the heap\n",
    "    '''\n",
    "    random.shuffle(training_dataset) \n",
    "    for training_example in training_dataset:\n",
    "        yield(form_indexed_sequence_variable(training_example[0], word_to_id, end_token), form_indexed_sequence_variable(training_example[1], word_to_id, end_token))\n",
    "        \n",
    "    \n",
    "def to_scalar(var):\n",
    "    # returns a python float\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "'''Final model-> Have to make batchwise'''\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, with_pretrained_embeddings = False, pretrained_embedding_matrix = None, freeze_embeddings = False):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        if(not with_pretrained_embeddings):\n",
    "            self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            assert vocab_size == pretrained_embedding_matrix.size()[0]\n",
    "            assert embedding_dim == pretrained_embedding_matrix.size()[1]\n",
    "            self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "            self.word_embeds.weight = nn.Parameter(pretrained_embedding_matrix)\n",
    "            if(freeze_embeddings):\n",
    "                self.word_embeds.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j. \n",
    "        '''Imp:\n",
    "        Transition probs are log likelihoods (log(p(X))) therefore range is -infinite (log(0)) to 0 (log(1))\n",
    "        '''\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[start_tag], :] = -10000 #\n",
    "        self.transitions.data[:, tag_to_ix[end_tag]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\n",
    "                autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[start_tag]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = autograd.Variable(init_alphas)\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward variables at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[end_tag]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = autograd.Variable(torch.Tensor([0]))\n",
    "        tags = torch.cat([torch.LongTensor([self.tag_to_ix[start_tag]]), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[end_tag], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_to_ix[start_tag]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = autograd.Variable(init_vvars)\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id])\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to end_tag\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[end_tag]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[start_tag]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        '''Cost function'''\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load relevant data for training\n",
    "def get_dataset_info(file_name = \"laptop\", training_data_additional = False, get_inverse = False, lower = True):\n",
    "    \n",
    "    if(lower):\n",
    "        opt_string = \"lower\" #hardcoded\n",
    "    else:\n",
    "        opt_string = \"final\"\n",
    "        \n",
    "        \n",
    "    if(training_data_additional):\n",
    "        training_data = pickle.load(open(\"Final_data/{}_{}_additional_training_list.pickle\".format(file_name, opt_string)))\n",
    "    else:\n",
    "        training_data = pickle.load(open(\"Final_data/{}_{}_normal_training_list.pickle\".format(file_name, opt_string)))\n",
    "        \n",
    "    tag2id = pickle.load(open(\"Final_data/{}_{}_tag2id.pickle\".format(file_name, opt_string)))\n",
    "    \n",
    "    vocab2id = pickle.load(open(\"Final_data/{}_{}_vocab.pickle\".format(file_name, opt_string)))\n",
    "    \n",
    "    if(\"<START>\" in vocab2id.keys()): #assuming <START> and <END> always co-occur\n",
    "        start_token = vocab2id[\"<START\"]\n",
    "        end_token = vocab2id[\"<END>\"]\n",
    "    else:\n",
    "        start_token = vocab2id[\"<START>\"] = len(vocab2id)\n",
    "        end_token = vocab2id[\"<END>\"] = len(vocab2id)\n",
    "    \n",
    "    if(not get_inverse):\n",
    "        return training_data, tag2id, vocab2id, start_token, end_token\n",
    "    else:\n",
    "        id2vocab = {id_:token for token, id_ in vocab2id.items()}\n",
    "        id2tag = {id_:tag for tag, id_ in tag2id.items()}\n",
    "        \n",
    "        return training_data, tag2id, vocab2id, start_token, end_token, id2vocab, id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_data, tag2id, laptop_vocab2id, start_token, end_token = get_dataset_info(\"laptop\", True)\n",
    "vocab_len = len(laptop_vocab2id)\n",
    "num_total_examples = len(laptop_data)\n",
    "\n",
    "\n",
    "training_first_index = 0\n",
    "training_last_index = int(num_total_examples*0.7)\n",
    "testing_first_index = training_last_index + 1\n",
    "\n",
    "dataset_aoext = map(lambda x: (x[0],x[1]) , laptop_data) \n",
    "training_data_aoext = dataset_aoext[: training_last_index+1]\n",
    "testing_data_aoext = dataset_aoext[testing_first_index:]\n",
    "\n",
    "id2tag = {id_: tag for tag, id_ in tag2id.items()}\n",
    "assert len(training_data_aoext) + len(testing_data_aoext)  == len(dataset_aoext) #Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initializing Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Hyperparameters'''\n",
    "\n",
    "start_tag = \"<START>\"\n",
    "end_tag = \"<END>\"\n",
    "\n",
    "embed_dim = 50\n",
    "hidden_dim = 10\n",
    "pretrained_embedding_matrix = load_embeddings(\"./Final_data/embeddings/glove/glove50dimw2v_format.txt\", laptop_vocab2id, 50)\n",
    "\n",
    "model = BiLSTM_CRF(vocab_len, tag2id, embed_dim, hidden_dim, True, pretrained_embedding_matrix, True)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss and predictions: ', (Variable containing:\n",
      " 29.1685\n",
      "[torch.FloatTensor of size 1]\n",
      ", [3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))\n",
      "Correct sequence: \n",
      "\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 1\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 5\n",
      " 3\n",
      " 1\n",
      " 2\n",
      "[torch.LongTensor of size 18]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check predictions before training\n",
    "precheck_sent = prepare_sequence(training_data_aoext[0][0], laptop_vocab2id)\n",
    "precheck_tags = torch.LongTensor(training_data_aoext[0][1])\n",
    "print(\"Loss and predictions: \", model(precheck_sent))\n",
    "print(\"Correct sequence: \")\n",
    "pprint(precheck_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_sequence(training_data, token_to_id):\n",
    "    '''Generator function to yield training example \n",
    "    Not very useful in this case but for large datasets it can save memory if we pass the dataset by reference.\n",
    "    Use hpy (guppy for py2) to explore memory usage in the heap\n",
    "    '''\n",
    "    random.shuffle(training_data)\n",
    "    for training_example in training_data:\n",
    "        yield (prepare_sequence(training_example[0], token_to_id), torch.LongTensor(training_example[1]) )\n",
    "        \n",
    "def generate_testing_sequence(testing_data, token_to_id, testing_size = 500):\n",
    "    random.shuffle(testing_data)\n",
    "    for testing_example in testing_data[:testing_size]:\n",
    "        yield(prepare_sequence(testing_example[0], token_to_id), torch.LongTensor(testing_example[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training'''\n",
    "def train(model, n_epochs, optimizer, token_to_id, training_data, testing_data, testing_size = 100):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"At epoch: {}\".format(epoch))\n",
    "        t1 = time.time()\n",
    "     \n",
    "    \n",
    "        training_gen = generate_training_sequence(training_data, token_to_id)\n",
    "        loss = 0\n",
    "        iters = 0\n",
    "    \n",
    "        for sentence_in, targets in training_gen:\n",
    "            model.zero_grad() #r1) remove all accumulated gradients from previous batch\n",
    "        \n",
    "          \n",
    "        \n",
    "            #3) Run forward pass \n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "            loss += neg_log_likelihood\n",
    "        \n",
    "            if(iters%500==0):\n",
    "                tm = time.time()\n",
    "                print(\"At iteration {}, time taken: {}\".format(iters, tm-t1))\n",
    "            iters+=1\n",
    "            \n",
    "            #4) Compute gradients, run backprop based on optimizer\n",
    "            neg_log_likelihood.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Training Loss for epoch: {}\".format(loss/iters))\n",
    "        t2 = time.time()\n",
    "        print(\"time for epoch: {}\".format(t2-t1))\n",
    "        \n",
    "        \n",
    "        te_loss = 0 \n",
    "        te_iters = 0\n",
    "        testing_gen = generate_testing_sequence(testing_data, token_to_id, testing_size)\n",
    "\n",
    "        for sentence_in, targets in testing_gen:\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "            te_loss += neg_log_likelihood\n",
    "            \n",
    "        print(\"Testing Loss for epoch: {}\".format(te_loss/iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 0\n",
      "At iteration 0, time taken: 0.0226540565491\n",
      "At iteration 500, time taken: 22.6349010468\n",
      "At iteration 1000, time taken: 40.6797330379\n",
      "At iteration 1500, time taken: 56.1470930576\n",
      "At iteration 2000, time taken: 71.1635129452\n",
      "Training Loss for epoch: Variable containing:\n",
      " 2.6766\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 74.8348219395\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1778\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 1\n",
      "At iteration 0, time taken: 1.95178198814\n",
      "At iteration 500, time taken: 17.6402630806\n",
      "At iteration 1000, time taken: 32.1657540798\n",
      "At iteration 1500, time taken: 47.6366760731\n",
      "At iteration 2000, time taken: 62.4609210491\n",
      "Training Loss for epoch: Variable containing:\n",
      " 2.4356\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 66.4103529453\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1406\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 2\n",
      "At iteration 0, time taken: 1.96679496765\n",
      "At iteration 500, time taken: 18.7126209736\n",
      "At iteration 1000, time taken: 36.7455370426\n",
      "At iteration 1500, time taken: 63.225135088\n",
      "At iteration 2000, time taken: 83.2392668724\n",
      "Training Loss for epoch: Variable containing:\n",
      " 2.2620\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 89.2536230087\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1364\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 3\n",
      "At iteration 0, time taken: 2.25591492653\n",
      "At iteration 500, time taken: 19.7994229794\n",
      "At iteration 1000, time taken: 37.50090909\n",
      "At iteration 1500, time taken: 58.1212890148\n",
      "At iteration 2000, time taken: 76.1223199368\n",
      "Training Loss for epoch: Variable containing:\n",
      " 2.1047\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 80.4235348701\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1714\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 4\n",
      "At iteration 0, time taken: 2.15599298477\n",
      "At iteration 500, time taken: 20.9485089779\n",
      "At iteration 1000, time taken: 38.8602080345\n",
      "At iteration 1500, time taken: 58.2765319347\n",
      "At iteration 2000, time taken: 75.5128090382\n",
      "Training Loss for epoch: Variable containing:\n",
      " 1.9924\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 80.0124390125\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1472\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 5\n",
      "At iteration 0, time taken: 2.3962328434\n",
      "At iteration 500, time taken: 20.22220397\n",
      "At iteration 1000, time taken: 38.0935418606\n",
      "At iteration 1500, time taken: 58.5672698021\n",
      "At iteration 2000, time taken: 80.647867918\n",
      "Training Loss for epoch: Variable containing:\n",
      " 1.8719\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 88.0554869175\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1627\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 6\n",
      "At iteration 0, time taken: 2.18711900711\n",
      "At iteration 500, time taken: 23.2666170597\n",
      "At iteration 1000, time taken: 48.8769159317\n",
      "At iteration 1500, time taken: 76.7741560936\n",
      "At iteration 2000, time taken: 95.8284339905\n",
      "Training Loss for epoch: Variable containing:\n",
      " 1.7898\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 100.835817099\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1468\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 7\n",
      "At iteration 0, time taken: 2.34621214867\n",
      "At iteration 500, time taken: 18.24340415\n",
      "At iteration 1000, time taken: 33.1689901352\n",
      "At iteration 1500, time taken: 48.6582510471\n",
      "At iteration 2000, time taken: 63.9103469849\n",
      "Training Loss for epoch: Variable containing:\n",
      " 1.7127\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 67.4861550331\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1371\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 8\n",
      "At iteration 0, time taken: 2.6157438755\n",
      "At iteration 500, time taken: 20.7805678844\n",
      "At iteration 1000, time taken: 42.0768768787\n",
      "At iteration 1500, time taken: 59.3430738449\n",
      "At iteration 2000, time taken: 76.8901188374\n",
      "Training Loss for epoch: Variable containing:\n",
      " 1.6389\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 81.1086409092\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1496\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 9\n",
      "At iteration 0, time taken: 2.33831810951\n",
      "At iteration 500, time taken: 19.0954620838\n",
      "At iteration 1000, time taken: 36.0020470619\n",
      "At iteration 1500, time taken: 52.046364069\n",
      "At iteration 2000, time taken: 69.5901360512\n",
      "Training Loss for epoch: Variable containing:\n",
      " 1.5712\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "time for epoch: 74.2642359734\n",
      "Testing Loss for epoch: Variable containing:\n",
      " 0.1414\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "At epoch: 10\n",
      "At iteration 0, time taken: 2.29526400566\n",
      "At iteration 500, time taken: 18.670404911\n",
      "At iteration 1000, time taken: 35.8101258278\n"
     ]
    }
   ],
   "source": [
    "train(model, 40, optimizer, laptop_vocab2id, training_data_aoext, testing_data_aoext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Trained_models/ABSA_models/glove50dims/Bilstm_CRF_state.pth\")\n",
    "#torch.save(model, \"Trained_models/ABSA_models/glove50dims/Bilstm_CRF.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
