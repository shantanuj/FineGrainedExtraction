{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is decomposed in multiple aspects each tackled in multiple ways.\n",
    "\n",
    "Below tries to arrange it systematically. \n",
    "Current primary questions/ideas:\n",
    "1. Approach 1: Contribution of word detection by stimulating missing words. \n",
    "    1. See approach 1 notebook. \n",
    "    2. Ideas- syntactic tree differences, representation differences, Tree RNN reps (more subproblems, decisions, experiments)\n",
    "\n",
    "2. Interesting question: <b>Why are some nouns more important than other nouns  (replace nouns with tags)?</b>\n",
    "\n",
    "3. Domain adaption through \n",
    "    1. Domain linkers, domain independent and domain dependent words based on SFG?\n",
    "    2. Domain local and sense embeddings, syntactic trees, word contribution,\n",
    "\n",
    "4. Semantic priming: How do we stimulate bias for words (<b>semantic primed networks</b>, etc)\n",
    "\n",
    "5. Examples of where models cannot adapt. Interaction in words? \n",
    "\n",
    "\n",
    "\n",
    "Current scripts/models:\n",
    "1. Baseline BIlstm+CRF\n",
    "2. seq2seq Autoencoder \n",
    "3. Freq counts of tags and parser tree changes based on missing words\n",
    "4. Dataset pickled, formation, etc\n",
    "\n",
    "Doing work:\n",
    "1. Missing word candidate selection\n",
    "2. Hash fitting for missing word\n",
    "3. Detection of domain independent, linker and dependent words. Generation of sense and domain embeddings \n",
    "4. Foundations for semantic priming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Index/Overview\n",
    "1. Problem \n",
    "    1. Description, statement\n",
    "    2. Importance of problem\n",
    "    3. Breakdown of problem and components\n",
    "    4. Subproblems and motivation of using representation learning (why rule based systems don't work, etc)\n",
    "    \n",
    "2. List of subproblems:\n",
    "    1. Detect contribution of words towards a sentiment\n",
    "        1. COmmon approaches: n-grams, etc\n",
    "        2. <b>Contribution = Interaction, influence, any more variables?<b>\n",
    "        \n",
    "    2. Why do models not adapt against domains. Elements that make models fail across domains for ABSA \n",
    "        1. Specific cases for language models: Ngrams, CRF \n",
    "        2. <b> Importance of input features and learned latent features</b>\n",
    "        3. <b> Examples of above where model cannot adapt </b> \n",
    "        2. Deep learning \n",
    "        3. CRF, prob based\n",
    "        4. Why do certain models perform better than others\n",
    "    \n",
    "    3. <b>Elements of a domain learner:</b>\n",
    "        1. Domain linkers\n",
    "        2. Domain variable breakdown \n",
    "        3. Generalizing over words across domains: Embeddings\n",
    "    \n",
    "    3. How to adapt a current rep learning model for domains\n",
    "        1. Architecture elements: Input features, Architecture to transition and learn latent features, final decision\n",
    "        2. Input features: Spectral learning; denoising architectures\n",
    "        3. Joint feature space through clever technique\n",
    "        4. Learning word contributions through difference in normal autoencoder and denoised encoder-decoders. \n",
    "        4. <b> If I were to remove a word, by how much does the normal representation drop or the sentiment prediction prob drop. Note, sentiment prediction is not equal to sentence representation</b>\n",
    "    \n",
    "    4. How to learn info about domain that is essential for sentiment\n",
    "       1. Identification of new variables.\n",
    "       2. Define variables that are essential for domain. Ex: lexicons, user-audience,<b> expert reviewers vs twitter reviewers </b> \n",
    "       3. This is obviously precoded, but can we dare to <b>learn essential domain variables through another model?? </b>\n",
    "    \n",
    "3. Subproblems and motivation for solving each subproblem\n",
    "    1. Performance of proposed solutions\n",
    "    2. Combination of all solutions\n",
    "    3. Future steps\n",
    "        \n",
    "4. Further investigation from Psychological perspective\n",
    "    1. Semantic priming and role of psychology in sentiment. How to capture tone and primes?\n",
    "    2. Word pose: Something similar to page rank?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-cf17692c7aec>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-cf17692c7aec>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def find_approximate_occurance_based_on_structure(): #computes similarity between two sentences based on word structure and word contribution\n",
    "    '''Example:\n",
    "    s1: It is very overpriced and not very tasty\n",
    "    s2: This lacks the features and is very expensive.\n",
    "    s3: This lacks the features and is very expensive for a model A laptop.\n",
    "    s4: It is very overpriced and not very tasty for a Michelin star restaraunt. \n",
    "    overpriced= expensive\n",
    "    tasty = feaatures\n",
    "    very = very\n",
    "    not = lack\n",
    "    Michelin star = model A \n",
    "    \n",
    "    Domain dependent words--> get_domain_dependent_words()\n",
    "    Domain independent words --> get_independent_words()\n",
    "    \n",
    "    'very expensive' is together/combined word \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_approximate_occurance_based_on_words():\n",
    "    '''Example:\n",
    "    \n",
    "    '''\n",
    "    None\n",
    "    \n",
    "def find_inter_domain_similarity():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Problem description, applicability, and breakdown\n",
    "\n",
    "## 1.1) Problem description: \n",
    "Implement a learning machine that can parse text to identify a user's opinion terms and specific aspect terms. \n",
    "Also, ensure that it works on multiple domains, ie, the learning machine should be a general language parser, not fixed to understanding in a particular domain. \n",
    "\n",
    "Approach this as a sequence labelling problem, wherein, given each sentence S with tokens t1..tn, the machine outputs \n",
    "\n",
    "## 1.2) Why is the problem worth doing\n",
    "This problem firstly finds great industrial use, and secondly works on exploring the development of future intelligent machines. Use cases in finance, consumer understanding, user profiling etc. Use case in general machines that can learn how to pick up important terms and get info about mentioned unknown topics. Again, user profiling, parsing text-> what are the important components, and mecanisms to store these in memory inspired semantic networks.  MEMORY inspired semantic networks. \n",
    "\n",
    "## 1.3) Exploring the dataset; Breakdown of problem and targeted components. \n",
    "\n",
    "\n",
    "## 1.4) Subproblems and motivations for usage of models/machine learning\n",
    "\n",
    "\n",
    "# Section Preliminaries and introduction\n",
    "### Problem description and problem breakdown\n",
    "\n",
    "\n",
    "\n",
    "## Subproblems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 approaches:\n",
    "\n",
    "1. Baseline simple method: BiLSTM + CRF\n",
    "2. To find out word contribution, by training a model based on missing words. So, if a word is removed, get some representation. Embed the ones with aspects missing to a similar space. Meaning if I have a sentence: I love their ice-cream so much. Then remove ice-cream, get a representation of the sentence, and a representation of the normal (with ice-cream sentence). Using these representations, map as similar (1) in a Siamese net. Now take all other (except opinion terms) and use same model to get representation. But the final model outputs the representations as dissimilar.\n",
    "\n",
    "    G(F(I love their so much), F(I love their ice cream so much)) = 1\n",
    "    G(F(love their ice cream so much), F(I love their ice cream so much)) = 0\n",
    "    G(F(I love their ice cream), F(I love their ice cream so much)) = 2\n",
    "    G(F(I love ice cream so much), F(Sentence)) = \n",
    "\n",
    "\n",
    "\n",
    "        Another approach could be if we remove the word then from all the mined words, if we replace the term with it, does the sentence still make sense (is it legible)-> then it is an aspect word. \n",
    "\n",
    "        So given s= I love their ice cream so much, if we remove ice-cream then replace it with say 50 previous aspect words like -> football, passing, nachos, ambience, does it still make sense? A model M is trained to identify whether a sentence still makes sense. \n",
    "\n",
    "3. Word environment: Nurse induces doctor, uniform, injury, etc etc -> semantic network, but how do we implement primiming or induce word priming.\n",
    "\n",
    "4. Domain difference, adaptation, need for common features?, how to enforce common features: How to detecte domain independent, linkers, and dependent words. Get common embeddings? \n",
    "\n",
    "5. Domain adaptaion methods-> covered in approach 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is an aspect and what is an opinion term?\n",
    "\n",
    "S1: 'Plus it is small and reasonably light so I can take it with me to and from work. -> small, reasonably light\n",
    "\n",
    "S2:'If internet connectivity is important I would recommend going with a dell net book for 50 bucks more, or buy a USB wireless card.' -> recommend, and internet connectivity;USB wireless card\n",
    "\n",
    "S3: \"Rao is a good restaurant, but it's nothing special.\" -> good, nothing special\n",
    "\n",
    "S4: iLife is easily compatible with Microsoft Office so you can send and receive files from a PC.--> easily compatible-->  iLife;Microsoft Office \n",
    "\n",
    "S5: \"people are rude bit again it's new york!\" -> rude; people\n",
    "\n",
    "S6: \"The speed is incredible and I am more than satisfied.\" --> incredible, 'more than satisfied', 'speed'\n",
    "\n",
    "\n",
    "\n",
    "#### Aspect terms: Serve to indicate related properties, entities of a given entity. \n",
    "1. As can be seen, an ASPECT would be a NOUN\n",
    "2. <b>Is it possible to have an aspect that is not a Noun? </b> What else 'good rules' can we use to the cover all aspect and opinion words. \n",
    "3. The structure of the sentence, how the aspect/opinion occurs also matters. We call this the syntactic structure. \n",
    "4. As shown in the script below, it is possible to have different types of aspects based on context. They can also occur as ADJs modifying Nouns. However, this requires covering a large number of rules, or inducing of rules. Instead, we would like to learn different patterns by capturing word information, context, syntax, etc. \n",
    "<b>4. Can we measure the disturbance in the syntactic structure and sub-representation of a sentence by removing possible aspect words ?</b> \n",
    "\n",
    "5. What is the final definition and salient propery of an aspect term??: An incomplete definition would be that it is a word that seeks to indicate an entity of property of something. Multiple terms could be combined to form an aspect phrase, and this leads to inconsistency in rules. Encoding such rules is not a good approach since it would be dataset specific, and require intensive effort by human coder.  \n",
    "\n",
    "6. Looking at syntax and type of structure linked to an aspect/opinion word, and making a co-occurance is too generic\n",
    "\n",
    "#### Opinion terms: Give a description of the aspect.\n",
    "\n",
    "1. PROBLEMS: Opinions can be long range, and have different intensities; however, the opinion dataset does not reflect this. Example: S6 opinion is just satisfied; S3 opinion is special (should be nothing special)\n",
    "2. <b>Is it possible to have an opinion that is not an ADJ?</b> Yes, examples in 'opinion expressions' \n",
    "\n",
    "\n",
    "#### What is domain adaptation, and what are domain differences? \n",
    "1. To do once we've got a good model.\n",
    "2. How to detect domain similar sentences, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Script to get frequencies of POS tags for opinion and aspect words'''\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.data import load\n",
    "#tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "examples_of_aspect_as_ADJ = []\n",
    "examples_of_opinion_as_NOUN = []\n",
    "\n",
    "\n",
    "def get_freq_pos_opinion_and_aspect(df, limit =-1):\n",
    "    #bopinion_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #baspect_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #iopinion_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    #iaspect_tags = {tag:0 for tag in tagdict.keys()}\n",
    "    bopinion_tags = {} \n",
    "    baspect_tags = {}\n",
    "    iopinion_tags = {} \n",
    "    iaspect_tags = {}\n",
    "    '''\n",
    "    if(limit==-1):\n",
    "        limit = len(df)\n",
    "        print_it = False\n",
    "    else:\n",
    "        limit = limit #for debugging\n",
    "        '''\n",
    "    for sentence, opinion, aspect in zip(df.Sentence, df.Opinions, df.Aspects):\n",
    "        '''\n",
    "        if(limit<=0):\n",
    "            break\n",
    "        limit-=1\n",
    "        '''\n",
    "        '''NLTK\n",
    "        #text = word_tokenize(sentence) #<-NLTK\n",
    "        #pos_tagged_text = pos_tag(text) #<-NLTK\n",
    "        '''\n",
    "        text = nlp(unicode(sentence))\n",
    "        pos_tagged_text = [(str(token), str(token.pos_)) for token in text]\n",
    "        \n",
    "        \n",
    "        for seqs in opinion.split(';'): #find all opinion terms-> can be done together with aspect terms but cleaner this way\n",
    "            for i, opinion_term in enumerate(seqs.split()):\n",
    "                for token_pos_tuple in pos_tagged_text:\n",
    "                    token, pos_tag = token_pos_tuple[0], token_pos_tuple[1]\n",
    "                    if(opinion_term == token):\n",
    "                        if(i>1):#IO\n",
    "                            if(pos_tag not in iopinion_tags):\n",
    "                                iopinion_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                iopinion_tags[pos_tag] += 1\n",
    "                        else:#BO\n",
    "                            if(pos_tag not in bopinion_tags):\n",
    "                                bopinion_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                bopinion_tags[pos_tag] += 1\n",
    "                        '''    \n",
    "                        if(limit> 0 and print_it):\n",
    "                            print(sentence, token_pos_tuple)\n",
    "                        '''\n",
    "                        if(pos_tag == \"NOUN\"):\n",
    "                            examples_of_opinion_as_NOUN.append((sentence, token_pos_tuple))\n",
    "                        \n",
    "        for seqs in aspect.split(';'): #find all aspect terms\n",
    "            for i, aspect_term in enumerate(seqs.split()):\n",
    "                for token_pos_tuple in pos_tagged_text:\n",
    "                    token, pos_tag = token_pos_tuple[0], token_pos_tuple[1]\n",
    "                    if(aspect_term == token):\n",
    "                        if(i>1):#IA\n",
    "                            if(pos_tag not in iaspect_tags):\n",
    "                                iaspect_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                iaspect_tags[pos_tag] += 1\n",
    "                        else:#BA\n",
    "                            if(pos_tag not in baspect_tags):\n",
    "                                baspect_tags[pos_tag] = 1\n",
    "                            else:\n",
    "                                baspect_tags[pos_tag] += 1\n",
    "                       \n",
    "                        if(pos_tag == 'ADJ'):\n",
    "                            examples_of_aspect_as_ADJ.append((sentence, token_pos_tuple))\n",
    "    return bopinion_tags, iopinion_tags, baspect_tags, iaspect_tags\n",
    "\n",
    "laptop_freq_dicts = get_freq_pos_opinion_and_aspect(laptop_data[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My only disappointment with the 13\" model is that they\\'re the only ones in the MacBook Pro line up without an i5 or i7 processor and dual graphics cards, however the trade-off is that you achieve a longer battery life (of about 2 more hours).',\n",
       "  ('disappointment', 'NOUN')),\n",
       " (\"Pairing it with an iPhone is a pure pleasure - talk about painless syncing - used to take me forever - now it's a snap.\",\n",
       "  ('pleasure', 'NOUN')),\n",
       " ('I also got the added bonus of a 30\" HD Monitor, which really helps to extend my screen and keep my eyes fresh!',\n",
       "  ('bonus', 'NOUN')),\n",
       " ('What a waste.', ('waste', 'NOUN')),\n",
       " ('but now i have realized its a problem with this brand.',\n",
       "  ('problem', 'NOUN')),\n",
       " ('There also seemed to be a problem with the hard disc as certain times windows loads but claims to not be able to find any drivers or files.',\n",
       "  ('problem', 'NOUN')),\n",
       " ('Speaking of the browser, it too has problems.', ('problems', 'NOUN')),\n",
       " ('A month or so ago, the freaking motherboard just died.',\n",
       "  ('freaking', 'NOUN')),\n",
       " ('We upgraded the memory to four gigabytes in order to take advantage of the performace increase in speed.',\n",
       "  ('increase', 'NOUN')),\n",
       " ('I had always used PCs and been constantly frustrated by the crashing and the poorly designed operating systems that were never very intuitive.',\n",
       "  ('crashing', 'NOUN')),\n",
       " ('And if you do a lot of writing, editing is a problem since there is no  forward delete  key.',\n",
       "  ('problem', 'NOUN')),\n",
       " ('Its ease of use and the top service from Apple- be it their phone assistance or bellying up to the genius bar- cannot be beat.',\n",
       "  ('ease', 'NOUN')),\n",
       " ('Its ease of use and the top service from Apple- be it their phone assistance or bellying up to the genius bar- cannot be beat.',\n",
       "  ('genius', 'NOUN')),\n",
       " ('But we had paid for bluetooth, and there was none.', ('none', 'NOUN')),\n",
       " (\"Dell's customer disservice is an insult to it's customers who pay good money for shoddy products.\",\n",
       "  ('insult', 'NOUN')),\n",
       " (\"I can render AVCHD movies with little effort, which was a problem for most pc's unless you had a quad core I7.\",\n",
       "  ('problem', 'NOUN')),\n",
       " ('Yes, a Mac is much more money than the average laptop out there, but there is no comparison in style, speed and just cool factor.',\n",
       "  ('comparison', 'NOUN')),\n",
       " ('It absolutely is more expensive than most PC laptops, but the ease of use, security, and minimal problems that have arisen make it well worth the pricetag.',\n",
       "  ('ease', 'NOUN')),\n",
       " ('lots of preloaded software.', ('lots', 'NOUN')),\n",
       " ('One drawback, I wish the keys were backlit.', ('drawback', 'NOUN')),\n",
       " ('Acer was no help and Garmin could not determine the problem(after spending about 2 hours with me), so I returned it and purchased a Toshiba R700 that seems even nicer and I was able to load all of my software with no problem.',\n",
       "  ('problem', 'NOUN')),\n",
       " ('they improved nothing else such as Resolution, appearance, cooling system, graphics card, etc.',\n",
       "  ('nothing', 'NOUN')),\n",
       " ('The board has a bad connector with the power supply and shortly after warrenty expires the power supply will start having issues.',\n",
       "  ('issues', 'NOUN')),\n",
       " ('Being virus-resistant is a huge plus.', ('plus', 'NOUN')),\n",
       " ('The screen is very large and crystal clear with amazing colors and resolution.',\n",
       "  ('crystal', 'NOUN')),\n",
       " (\"Additional caveat: the base installation comes with some Toshiba-specific software that may or may not be to a user's liking.\",\n",
       "  ('liking', 'NOUN')),\n",
       " (\"The service tech said he had tried to duplicate the damage and wasn't able to recreate it therefore it had to be their defect.\",\n",
       "  ('defect', 'NOUN')),\n",
       " ('The processor a AMD Semprom at 2.1 ghz is a bummer it does not have the power for HD or heavy computing.',\n",
       "  ('bummer', 'NOUN')),\n",
       " (\"They sent out the box right away for me to send in my computer, they paid postage and whatnot, but when I got my computer back it still wasn't running right, and now my CD drive wasn't reading anything!\",\n",
       "  ('anything', 'NOUN')),\n",
       " ('Yeah, of course smarty pants \"fix it now\")Software - Compared to the early 2011 edition I did see inbuilt applications crashing and it prompted me to send the report to Apple (which I promptly did).',\n",
       "  ('smarty', 'NOUN')),\n",
       " (\"I think part of the problem with this computer is Vista, yet I know Vista isn't the entire issue because my latest purchase was my Acer and it also has Vista (I should have waited the few months to get the next operating system).\",\n",
       "  ('problem', 'NOUN')),\n",
       " ('it is hard to fix and makes it a hassle to own one.', ('hassle', 'NOUN')),\n",
       " ('The OS is also very user friendly, even for those that switch from a PC, with a little practice you can take full advantage of this OS!',\n",
       "  ('user', 'NOUN')),\n",
       " ('the programs are esay to use and are quick to process this computer works like a charm.',\n",
       "  ('charm', 'NOUN')),\n",
       " ('Tech support tells me the latter problem is a power supply problem and have offered to fix it if it happens again.',\n",
       "  ('problem', 'NOUN')),\n",
       " ('Tech support tells me the latter problem is a power supply problem and have offered to fix it if it happens again.',\n",
       "  ('problem', 'NOUN')),\n",
       " ('Windows XP SP2 caused many problems on the computer, so I had to remove it.',\n",
       "  ('problems', 'NOUN')),\n",
       " ('The salesman talked us into this computer away from another we were looking at and we have had nothing but problems with software problems and just not happy with it.',\n",
       "  ('problems', 'NOUN')),\n",
       " ('The salesman talked us into this computer away from another we were looking at and we have had nothing but problems with software problems and just not happy with it.',\n",
       "  ('problems', 'NOUN')),\n",
       " ('Once open, the leading edge is razor sharp.', ('razor', 'NOUN')),\n",
       " ('It runs very quiet too which is a plus.', ('plus', 'NOUN')),\n",
       " ('but it  has a major design flaw.', ('flaw', 'NOUN'))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_freq_dicts\n",
    "examples_of_opinion_as_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['My only other complaint is that it gets really hot.',\n",
       "  'I could not even put my entire music collection on this garabage.',\n",
       "  'Back then my entire family was Devoted to the Sony name.',\n",
       "  'Plus it is small and reasonably light so I can take it with me to and from work.',\n",
       "  'I also experience the same with my MacBook Air.'],\n",
       " [\"Just don't do it.\",\n",
       "  'So some of the reviews here are accurate about the crowd and noise.',\n",
       "  'Service was very friendly.',\n",
       "  \"Don't judge this place prima facie, you have to try it to believe it, a home away from home for the literate heart.\",\n",
       "  'I love this cozy around the way Rest.'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_pd_column = lambda df,column:df.column \n",
    "lap_sentences = selected_lap_examples.Sentence.tolist()\n",
    "rest_sentences = selected_rest_examples.Sentence.tolist()\n",
    "zip(lap_sentencesrest_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To do:\\nMake baseline Bi-directional LSTM and top-CRF model with info about POS tags \\n\\nInputs: POS_tags; Dependency relation wrt next word; Word vector (context info); Info about influence of a word \\n\\nQ: To get a better representation what can we do to the architecture itself (rather than feeding in more features)\\nQ: How do we obtain the influence of a word in a sentence (like on a color scale); attention\\nQ: How do we obtain the interdependeny of words -> dependency structure OR POSE OF WORDS\\nQ: Define interdependency with an example\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To do:\n",
    "Make baseline Bi-directional LSTM and top-CRF model with info about POS tags \n",
    "\n",
    "Inputs: POS_tags; Dependency relation wrt next word; Word vector (context info); Info about influence of a word \n",
    "\n",
    "Q: To get a better representation what can we do to the architecture itself (rather than feeding in more features)\n",
    "Q: How do we obtain the influence of a word in a sentence (like on a color scale); attention\n",
    "Q: How do we obtain the interdependeny of words -> dependency structure OR POSE OF WORDS\n",
    "Q: Define interdependency with an example\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
